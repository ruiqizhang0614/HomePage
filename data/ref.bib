@article{minimax,
  author    = {Yaqi Duan and
               Mengdi Wang},
  title     = {Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation},
  journal   = {CoRR},
  volume    = {abs/2002.09516},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.09516},
  archivePrefix = {arXiv},
  eprint    = {2002.09516},
  timestamp = {Sat, 23 Jan 2021 01:14:29 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-09516.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{bootstrap,
      title={Bootstrapping Statistical Inference for Off-Policy Evaluation}, 
      author={Botao Hao and Xiang Ji and Yaqi Duan and Hao Lu and Csaba Szepesvári and Mengdi Wang},
      year={2021},
      eprint={2102.03607},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@book{RL,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@inproceedings{papini2018stochastic,
  title={Stochastic variance-reduced policy gradient},
  author={Papini, Matteo and Binaghi, Damiano and Canonaco, Giuseppe and Pirotta, Matteo and Restelli, Marcello},
  booktitle={International conference on machine learning},
  pages={4026--4035},
  year={2018},
  organization={PMLR}
}

@inproceedings{xu2020improved,
  title={An improved convergence analysis of stochastic variance-reduced policy gradient},
  author={Xu, Pan and Gao, Felicia and Gu, Quanquan},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={541--551},
  year={2020},
  organization={PMLR}
}

@article{xu2019sample,
  title={Sample efficient policy gradient methods with recursive variance reduction},
  author={Xu, Pan and Gao, Felicia and Gu, Quanquan},
  journal={arXiv preprint arXiv:1909.08610},
  year={2019}
}

@inproceedings{shen2019hessian,
  title={Hessian aided policy gradient},
  author={Shen, Zebang and Ribeiro, Alejandro and Hassani, Hamed and Qian, Hui and Mi, Chao},
  booktitle={International conference on machine learning},
  pages={5729--5738},
  year={2019},
  organization={PMLR}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{agarwal2021theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={98},
  pages={1--76},
  year={2021}
}

@inproceedings{thomas2015high,
  title={High confidence policy improvement},
  author={Thomas, Philip and Theocharous, Georgios and Ghavamzadeh, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={2380--2388},
  year={2015},
  organization={PMLR}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}


@InProceedings{pmlr-v139-hao21b,
  title = 	 {Bootstrapping Fitted Q-Evaluation for Off-Policy Inference},
  author =       {Hao, Botao and Ji, Xiang and Duan, Yaqi and Lu, Hao and Szepesvari, Csaba and Wang, Mengdi},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4074--4084},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hao21b/hao21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/hao21b.html},
  abstract = 	 {Bootstrapping provides a flexible and effective approach for assessing the quality of batch reinforcement learning, yet its theoretical properties are poorly understood. In this paper, we study the use of bootstrapping in off-policy evaluation (OPE), and in particular, we focus on the fitted Q-evaluation (FQE) that is known to be minimax-optimal in the tabular and linear-model cases. We propose a bootstrapping FQE method for inferring the distribution of the policy evaluation error and show that this method is asymptotically efficient and distributionally consistent for off-policy statistical inference. To overcome the computation limit of bootstrapping, we further adapt a subsampling procedure that improves the runtime by an order of magnitude. We numerically evaluate the bootrapping method in classical RL environments for confidence interval estimation, estimating the variance of off-policy evaluator, and estimating the correlation between multiple off-policy evaluators.}
}

@inproceedings{thomas2016data,
  title={Data-efficient off-policy policy evaluation for reinforcement learning},
  author={Thomas, Philip and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={2139--2148},
  year={2016},
  organization={PMLR}
}

@article{degris2012off,
  title={Off-policy actor-critic},
  author={Degris, Thomas and White, Martha and Sutton, Richard S},
  journal={arXiv preprint arXiv:1205.4839},
  year={2012}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={PMLR}
}

@article{wang2016sample,
  title={Sample efficient actor-critic with experience replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}

@article{xu2021doubly,
  title={Doubly Robust Off-Policy Actor-Critic: Convergence and Optimality},
  author={Xu, Tengyu and Yang, Zhuoran and Wang, Zhaoran and Liang, Yingbin},
  journal={arXiv preprint arXiv:2102.11866},
  year={2021}
}

@article{khodadadian2021finite,
  title={Finite-Sample Analysis of Off-Policy Natural Actor-Critic Algorithm},
  author={Khodadadian, Sajad and Chen, Zaiwei and Maguluri, Siva Theja},
  journal={arXiv preprint arXiv:2102.09318},
  year={2021}
}

@article{wang2019neural,
  title={Neural policy gradient methods: Global optimality and rates of convergence},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1909.01150},
  year={2019}
}

@article{khodadadian2021finite2,
  title={Finite Sample Analysis of Two-Time-Scale Natural Actor-Critic Algorithm},
  author={Khodadadian, Sajad and Doan, Thinh T and Maguluri, Siva Theja and Romberg, Justin},
  journal={arXiv preprint arXiv:2101.10506},
  year={2021}
}

@article{qiu2021finite,
  title={On Finite-Time Convergence of Actor-Critic Algorithm},
  author={Qiu, Shuang and Yang, Zhuoran and Ye, Jieping and Wang, Zhaoran},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={2},
  number={2},
  pages={652--664},
  year={2021},
  publisher={IEEE}
}

@article{kumar2019sample2,
  title={On the sample complexity of actor-critic method for reinforcement learning with function approximation},
  author={Kumar, Harshat and Koppel, Alec and Ribeiro, Alejandro},
  journal={arXiv preprint arXiv:1910.08412},
  year={2019}
}

@article{xiong2020non,
  title={Non-asymptotic convergence of adam-type reinforcement learning algorithms under Markovian sampling},
  author={Xiong, Huaqing and Xu, Tengyu and Liang, Yingbin and Zhang, Wei},
  journal={arXiv preprint arXiv:2002.06286},
  year={2020}
}

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  pages={1008--1014},
  year={2000}
}

@article{peters2008natural,
  title={Natural actor-critic},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1180--1190},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{sutton2009fast,
  title={Fast gradient-descent methods for temporal-difference learning with linear function approximation},
  author={Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={993--1000},
  year={2009}
}

@inproceedings{kallus2020statistically,
  title={Statistically efficient off-policy policy gradients},
  author={Kallus, Nathan and Uehara, Masatoshi},
  booktitle={International Conference on Machine Learning},
  pages={5089--5100},
  year={2020},
  organization={PMLR}
}

@INPROCEEDINGS{spoken,
  author={Jurčíček, Filip},
  booktitle={2012 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={Reinforcement learning for spoken dialogue systems using off-policy natural gradient method}, 
  year={2012},
  volume={},
  number={},
  pages={7-12},
  doi={10.1109/SLT.2012.6424161}}
  
@article{tokdar2010importance,
  title={Importance sampling: a review},
  author={Tokdar, Surya T and Kass, Robert E},
  journal={Wiley Interdisciplinary Reviews: Computational Statistics},
  volume={2},
  number={1},
  pages={54--60},
  year={2010},
  publisher={Wiley Online Library}
}

@inproceedings{hanna2018towards,
  title={Towards a data efficient off-policy policy gradient},
  author={Hanna, Josiah P and Stone, Peter},
  booktitle={2018 AAAI Spring Symposium Series},
  year={2018}
}

@inproceedings{tosatto2020nonparametric,
  title={A Nonparametric Off-Policy Policy Gradient},
  author={Tosatto, Samuele and Carvalho, Joao and Abdulsamad, Hany and Peters, Jan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={167--177},
  year={2020},
  organization={PMLR}
}

@article{gu2016q,
  title={Q-prop: Sample-efficient policy gradient with an off-policy critic},
  author={Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.02247},
  year={2016}
}

@inproceedings{zhang2020provably,
  title={Provably convergent two-timescale off-policy actor-critic with function approximation},
  author={Zhang, Shangtong and Liu, Bo and Yao, Hengshuai and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={11204--11213},
  year={2020},
  organization={PMLR}
}

@article{maei2018convergent,
  title={Convergent actor-critic algorithms under off-policy training and function approximation},
  author={Maei, Hamid Reza},
  journal={arXiv preprint arXiv:1802.07842},
  year={2018}
}

@article{xu2020improving,
  title={Improving sample complexity bounds for (natural) actor-critic algorithms},
  author={Xu, Tengyu and Wang, Zhe and Liang, Yingbin},
  journal={arXiv preprint arXiv:2004.12956},
  year={2020}
}

@article{khodadadian2021finite,
  title={Finite Sample Analysis of Two-Time-Scale Natural Actor-Critic Algorithm},
  author={Khodadadian, Sajad and Doan, Thinh T and Maguluri, Siva Theja and Romberg, Justin},
  journal={arXiv preprint arXiv:2101.10506},
  year={2021}
}

@article{shelton2013policy,
  title={Policy improvement for POMDPs using normalized importance sampling},
  author={Shelton, Christian R},
  journal={arXiv preprint arXiv:1301.2310},
  year={2013}
}

@article{meuleau2001exploration,
  title={Exploration in gradient-based reinforcement learning},
  author={Meuleau, Nicolas and Peshkin, Leonid and Kim, Kee-Eung},
  year={2001}
}

@inproceedings{10.5555/645529.658134, author = {Precup, Doina and Sutton, Richard S. and Singh, Satinder P.}, title = {Eligibility Traces for Off-Policy Policy Evaluation}, year = {2000}, isbn = {1558607072}, publisher = {Morgan Kaufmann Publishers Inc.}, address = {San Francisco, CA, USA}, booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning}, pages = {759–766}, numpages = {8}, series = {ICML '00} }

@article{xie2019towards,
  title={Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling},
  author={Xie, Tengyang and Ma, Yifei and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:1906.03393},
  year={2019}
}

@article{zhang2021convergence,
  title={On the convergence and sample efficiency of variance-reduced policy gradient method},
  author={Zhang, Junyu and Ni, Chengzhuo and Yu, Zheng and Szepesvari, Csaba and Wang, Mengdi},
  journal={arXiv preprint arXiv:2102.08607},
  year={2021}
}

@article{liu2019off,
  title={Off-policy policy gradient with state distribution correction},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={arXiv preprint arXiv:1904.08473},
  year={2019}
}

@inproceedings{papini2018stochastic,
  title={Stochastic variance-reduced policy gradient},
  author={Papini, Matteo and Binaghi, Damiano and Canonaco, Giuseppe and Pirotta, Matteo and Restelli, Marcello},
  booktitle={International conference on machine learning},
  pages={4026--4035},
  year={2018},
  organization={PMLR}
}

@article{yang2019global,
  title={On the global convergence of actor-critic: A case for linear quadratic regulator with ergodic cost},
  author={Yang, Zhuoran and Chen, Yongxin and Hong, Mingyi and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1907.06246},
  year={2019}
}

@article{zanette2021provable,
  title={Provable benefits of actor-critic methods for offline reinforcement learning},
  author={Zanette, Andrea and Wainwright, Martin J and Brunskill, Emma},
  journal={arXiv preprint arXiv:2108.08812},
  year={2021}
}