\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{amsthm}
\usepackage{todonotes}
\allowdisplaybreaks[4]
\usepackage{geometry}
%\usepackage{ntheorem}
\geometry{left = 2.0cm, right = 2.0cm, top = 2.0cm, bottom = 3.0cm}
\usepackage{multicol}
\usepackage{algorithm}  
\usepackage{algorithmicx}  
\usepackage{algpseudocode}
\def\integral#1#2{\int_{#2}#1\textrm{d}#2}
\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{biblatex}  
\addbibresource{ref.bib}
\newtheorem{lemma}{Lemma}[section]
%\newtheorem*{proof}{Proof}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{condition}{Condition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\usepackage{appendix}
\usepackage{bm}
\usepackage{authblk}
\def\mw#1{\textcolor{red}{#1}}

% for table
%\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{array}

\numberwithin{equation}{section}
%\def\done#1{\todo[color=green!40]{#1}}

\begin{document}

\title{Efficient Off-Policy Policy Gradient Estimation via Value Function Approximation}
\author[1]{Chengzhuo Ni}
\author[2]{Ruiqi Zhang}
\author[1]{Xiang(Jack) Ji}
\author[1]{Mengdi Wang}
\affil[1]{Department of Electrical and Computer Engineering, Princeton University}
\affil[2]{School of Mathematical Science, Peking University}
\date{}
\maketitle

\begin{abstract}
    Policy gradient estimation poses a challenge when we are not allowed to simulate the target policy but only have access to off-policy data generated by some unknown behavior policy. Conventional methods for off-policy PG estimation often suffer from bias or exponentially large variance. In this paper, we propose a double Fitted PG evaluation (FPG) algorithm, for arbitrary policy parametrization, that leverages linear value function approximation. Without any assumption about full data coverage, we provide a tight finite-sample policy gradient error bound that is determined by the mismatch in feature space. Further, we established that the PG estimation error is asymptotic normal with a precise covariance characterization. Lastly, we show that the PG estimates are statistically optimal by establishing a matching Cramer-Rao lower bound.
\end{abstract}

\section{Introduction}
Policy gradient plays a key role in policy-based reinforcement learning methods (RL). We focus on the estimation of policy gradient in off-policy reinforcement learning. In the off-policy setting, we are given episodic state-transition experiences that were generated by some possibly unknown behavior policy. Our goal is to estimate the policy gradient of a new target policy $\pi_\theta$, i.e., $\nabla_\theta v^{\pi_{\theta}}$, based on the off-policy data only. We are not allowed to simulate the new target policy or interact with the environment in anyway. To handle the distribution shift due to off-policy data, one conventional approach is importance sampling. However, this approach is sample-expensive and unstable, as the importance sampling weight could grow exponentially and causing uncontrollably high variances.

In this work, we aim to avoid the high variance of importance sampling and leverage the idea of function approximation commonly used in value-based RL methods. In particular, we ask the questions: 
\begin{center}
    \it{Can we use value function approximation to improve the accuracy of off-policy policy gradient estimates? What are the theoretical limits?}
\end{center}
The key idea of this work is to 
leverage function approximation and the popular fitted Q evaluation for evaluating the PG of a new target policy from offline data. We propose a Fitted Policy Gradient (FPG) estimation algorithm, which conducts iterative regression to estimate $Q$ functions and $\nabla_{\theta} Q$ functions jointly. The FPG algorithm is able to provide accurate estimates without full coverage data, and it also does not require knowledge of the behavior policy. 
In fact, when the function approximator is linear, we show that FPG is equivalent to a model-based plugin estimator.

We show that the FPG gives an $\epsilon$-close PG estimator using a sample size 
$N = O\left( {\frac{C H^5}{\epsilon^2}} \right)$,
where $N$ is the number of off-policy transition samples, and $C$ measures the distribution shift from behavior policy to target policy. Notably, this distribution shift $C$ can be bounded by a form of relative condition number or a restricted chi-square divergence, measuring the mismatch only in feature space. The sample efficiency of FPG estimate does not depend on feature dimension or state space size. The sample efficiency bound can take small values even if the data only partially covers the state space.
Further, we establish variance-aware error bounds and asymptotic normality for the FPG estimator. We also provide a matching information-theoretic Cramer-Rao lower bound. See Table 1 for a summary of theoretical results for off-policy PG estimation. 

In addition, one may use the FPG estimate to directly solve the off-policy policy optimization problem. 
%If the policy optimization landscape satisfies the gradient dominance property, the algorithm can find an $\epsilon$-stationary policy using sample size $N = \tilde O\left({\frac{\hbox{dim}(\Theta)^2 }{\epsilon}} \right)$ \mw{not right. this is the general case}.
In general, the algorithm can find an $\epsilon$-stationary policy using sample size $N = \tilde O\left(\left(\frac{\hbox{dim}(\Theta) }{\epsilon}\right)^2 \right)$. If the policy optimization landscape has the nice gradient dominance property, the sample complexity can further improve to $N = \tilde O\left({\frac{\left(\hbox{dim}(\Theta)\right)^2}{\epsilon}} \right)$ for finding an $\epsilon$-optimal policy. 

 Lastly we evaluate our approach empirically. We observe that the FPG demontrates superior sample efficiency compared to importance sampling. It is also tolerant to distribution shift. To the authors' best knowledges, FPG is a first off-policy PG estimation algorithm that is both computationally efficient and sample efficient. Statistical bounds appear to be the first set of statistical-optimal results for off-policy PG estimation.

\section{Problem Formulation}

In this paper, we study the off-policy estimation of policy gradient from fixed empirical batch data collected from a Markov Decision Process (MDP). 

\subsection{Markov Decision Process}
An instance of MDP is defined by the tuple $(\mathcal{S}, \mathcal{A}, p, r, \xi, H)$ where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $H\in\mathbb{N}_+$ is the horizon, $p_h: \mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}_+, h\in[H]$ is the transition probability, $r_h:\mathcal{S}\times\mathcal{A}\rightarrow[0,1], h\in[H]$ is the reward function and $\xi:\mathcal{S}\rightarrow\mathbb{R}_+$ is the initial state distribution. Given an MDP, a policy $\pi:\mathcal{S}\times\mathcal{A}\times[H]\rightarrow\mathbb{R}_+$ is a distribution over the action space conditioned on an arbitrary given state $s$ and the current step $h$. At each time step $h$, based on the current state $s_h$, the MDP requires the agent to input an action $a_h$ that could be selected from any given policy $\pi$. Conditioned on $(s_h,a_h)$, the agent observes a reward $r_h(s_h, a_h)$ and the next state $s_{h+1}$ sampled according to $s_{h+1}\sim p_h(\cdot\vert s_h, a_h)$. A policy $\pi$ is measured by the Q function $Q^\pi_h$ and the value $v^\pi$, defined as $Q^{\pi}_h(s, a)= \mathbb{E}^\pi\left[\left.\sum_{h^{\prime}=h}^Hr_{h^\prime}(s_{h^{\prime}}, a_{h^{\prime}})\right\vert s_h=s,\ a_h=a\right],\forall h\in[H],\ s\in\mathcal{S}, \ a\in\mathcal{A}$ and $v^{\pi}= \mathbb{E}^\pi\left[\left.\sum_{h=1}^Hr_h(s_h, a_h)\right\vert s_1\sim\xi\right]=\mathbb{E}^{\pi}\left[Q^{\pi}_1(s, a)\vert s\sim\xi\right]$, where $\mathbb{E}^\pi$ denotes taking expectation over the sample path following policy $\pi$. The optimal policy of the MDP is denoted as $\pi^* = \arg\max_{\pi}v^\pi$. 

\subsection{Off-Policy Policy Gradient Estimation}

%reformulates the expression of the policy gradient in a computationally tractable form that could be efficiently computed with typical sampling methods:


Direct policy optimization methods are popular for RL because they are often effective and computationally inexpensive. Among direct policy-based methods, a most elementary one is
the Policy Gradient (PG) method. Its idea is to
represent policies through some policy parameterization and then move the parameters of a policy in the direction of the gradient of the objective function.

Suppose that the policy admits a parametrization $\pi_{\theta}$, where $\theta\in\Theta$ is a policy parameter. Policy gradient (PG for short) is defined as the gradient of policy value $v_{\theta}$ with respect to the policy parameter $\theta$:
$$\nabla_{\theta} v_{\theta}= \nabla_{\theta} \mathbb{E}^{\pi_{\theta}}\left[\left.\sum_{h=1}^Hr_h(s_h, a_h)\right\vert s_1\sim\xi\right].$$
With policy gradients, one may directly search in the policy parameter space $\Theta$ using gradient ascent iterations, giving rise to the class of PG algorithms. 
Directly differentiating the value function is very difficult, especially when we do not have access to the transition probability of the MDP. 
%However, evaluating $\nable v_{\theta}$ directly is often computationally intractable, especially when the transition probabilities are unknown. 
%Policy gradient method aims to find the optimal policy by using gradient ascent or its variants in the policy parameter space $\Theta$ based on the gradient of the value function $\nabla_\theta v_\theta$, or policy gradient (PG) in short.
The policy gradient theorem \cite{sutton2000policy} gives a convenient formula for calculating PG using Monte Carlo sampling:
\begin{align}
    \label{pg_thm}
    \begin{aligned}
    \nabla_\theta v_\theta &= \mathbb{E}^{\pi_\theta}\left[\left.\sum_{h=1}^H \left(\sum_{h^\prime=h}^H r_{h^\prime}\right)\nabla_\theta\log\pi_{\theta, h}\left(a_h\vert s_h\right)\right\vert s_1\sim\xi\right]
    \end{aligned}
\end{align}
In the on-policy RL setting, one can interact with the environment and run policy $\pi_{\theta}$, in order to estimate the PG by averaging over sample trajectories  \cite{degris2012off, kakade2001natural, peters2008natural,sutton2000policy, williams1992simple}. 

We focus on the more challenging off-policy RL setting, where we cannot interact with the environment and we are not allowed to test the target policy $\pi_{\theta}$. Instead, we have access to off-line logged data, $\mathcal{D}=\{(s^{(k)}_h,a^{(k)}_h,s^{(k)}_{h+1},r^{(k)}_h)\}_{h\in[H],k\in[K]}$, generated from a different behavior policy.
Suppose the offline data consists of $K$ \textit{i.i.d.} trajectories, each of length $H$, generated by a behavior policy $\bar{\pi}$. %We denote by $N=KH$ the total number of sample transitions in $\mathcal{D}$.

The objective of off-policy PG estimation is to construct the best estimator $\widehat{\nabla_{\theta} v_{\theta}}$ based on the off-policy data $\mathcal{D}$. Computationally, we hope to design a practical algorithm to compute the PG estimator with low runtime. Theoretically, we hope to establish tight estimation error bounds and information-theoretic lower bounds.


%However, PG estimation has mostly been done by on-policy simulation, without making full use of large-scale batch data. This is partially due to the lack of theoretical and algorithmic tools to utilize batch data with function approximation. 

\paragraph{Notations}
Let $\pi_\theta$ be a policy parameterized by $\theta\in\Theta\subseteq\mathbb{R}^m$, where $\Theta$ is compact and $m=\textrm{dim}(\theta)$. Let $\theta^*=\arg\max_{\theta\in\Theta}v_\theta$. Denote for short that $Q^\theta_h:=Q^{\pi_\theta}_h, v_\theta := v^{\pi_\theta}$. 
Define the transition operator $\mathcal{P}_\theta$ by
\begin{align*}
	\left(\mathcal{P}_{\theta, h} f\right)(s,a)=\mathbb{E}^{\pi_\theta}\left[f(s_{h+1}, a_{h+1})\vert s_h=s, a_h=a\right],\quad\forall f:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R},h\in[H],s\in\mathcal{S},a\in\mathcal{A}.
\end{align*}
where we denote $[N]$ as the set of integer $1,2,...,N$. Given some function class $\mathcal{F}$, for any vector function $u:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{m}$, we say $u\in\mathcal{F}$ if and only if $u_j\in\mathcal{F},\ \forall j\in[m]$. For any matrix $E\in\mathbb{R}^{d_1\times d_2}$ (which includes scalars and vectors as special cases), we define its Jacobian as $\nabla_\theta E_\theta = (\nabla_\theta^1 E_\theta, \nabla_\theta^2 E_\theta, \ldots, \nabla_\theta^m E_\theta)\in\mathbb{R}^{d_1\times md_2}$, where $\nabla_\theta^j$ represents the partial derivative w.r.t. the $j$th entry, i.e., $\nabla_\theta^j := \frac{\partial}{\partial\theta_j}$. %Let $\bar\mu$ be the occupancy measure of state-action-state transition (aka data generating distribution) of $\mathcal{D}$.

\begin{table*}[htb!]
    \centering
	\caption{\textbf{Comparison of Off-Policy PG Estimation Methods.}}  
	\label{table_off_policy_PG_methods} 
    \begin{tabular}{m{3.5cm}<{\centering} m{2cm}<{\centering} m{1.5cm}<{\centering} m{1.5cm}<{\centering} m{3cm}<{\centering} m{3.8cm}<{\centering}}
    \toprule
    Algorithm & Variance & Require Full Coverage Data? & Require Known Behavior Policy? & Required Estimators & Notes\\
    \hline
       REINFORCE \cite{kakade2001natural,shelton2013policy}  & $2^{\Theta(H)} \Theta(1/K)$  & Yes & Yes & None & High Variance\\
    \hline
        GPOMDP \cite{kakade2001natural,shelton2013policy} & $2^{\Theta(H)} \Theta(1/K)$ & Yes & Yes & $\widehat{Q}_h^\theta$ & High Variance\\
        \hline
        EOOPG \cite{kallus2020statistically} & $\Theta(H^4/K)$  & Yes & Yes & $\widehat{\mu}_h^\theta,\widehat{\nabla_\theta\mu_h^\theta},\widehat{Q}_h^\theta,\widehat{\nabla_\theta Q_h^\theta}$ & Requires $\mu^{\pi}/\bar\mu$ be uniformly bounded and estimated at $K^{-1/2}$ rate\\
        \hline
        \textbf{FPG} (This Paper) & $\Theta(H^4/K)$ & No & No & None & Function approximation of $Q,\nabla_{\theta} Q$\\
        \bottomrule
    \end{tabular}
\end{table*}

\section{Related Work}

%The value of the policy gradient is usually estimated in a Monte Carlo style by rolling a bunch of trajectories using $\pi_\theta$, calculating the RHS of \eqref{pg_thm} and taking the empirical mean. Classical PG methods including REINFORCE and GPOMDP \cite{williams1992simple, sutton2000policy} are all based on this idea or its modifications \cite{degris2012off} \cite{kakade2001natural} \cite{peters2008natural}.


%\paragraph{Off-policy estimation}
%Because online data collecting can be costly, dangerous or impractical, offline learning has attracted more and more attention. 

When it comes to off-policy estimation, one demanding challenge is the distribution shift between the possibly unknown behavior policy and target policy \cite{agarwal2021theory}. 
%In this case, Importance Sampling \cite{papini2018stochastic, hanna2018towards} still serves as an immediate solution to estimate the PG from the data that may not necessarily be generated from the current policy. Suppose the data contains $K$ path samples, the typical formulation of 
The basic Importance Sampling (IS) estimator for off-policy PG is
\begin{align*}
    \widehat{\nabla_\theta v_\theta^{IS}}:=\frac{1}{K}\sum_{k=1}^K w_k\sum_{h=1}^H \left(\sum_{h^\prime =h}^H r_{h^\prime}^{(k)}\right)\nabla_\theta\log\pi_{\theta, h}\left(\left. a_h^{(k)}\right\vert s_h^{(k)}\right)
\end{align*}
where $w_k=\prod_{h=1}^H \frac{\pi_{\theta, h}\left(\left.a_h^{(k)}\right\vert s_h^{(k)}\right)}{\bar \pi_h\left(\left.a_h^{(k)}\right\vert s_h^{(k)}\right)}$ is the IS weight. %The IS formula has been applied early in the area of spoken dialogue system\cite{spoken}. 
Classical PG methods including REINFORCE and GPOMDP \cite{sutton2000policy, williams1992simple} are all based on this idea or its modifications \cite{degris2012off, kakade2001natural, peters2008natural}.
A severe drawback of the IS method is the estimator comes with a huge variance as large as $2^{\Theta(H)}$, resulting in a main obstacle for both theoretical analysis and empirical usage. Further, it requires prior knowledge of $\bar{\pi}$ to compute the IS weights.
\cite{kallus2020statistically} proposed a meta-algorithm called (EOOPG) that uses priorly available nuisance estimators for PG estimation. They show that if the state-action density ratio function $\mu^{\pi}/\bar\mu$ can be estimated with error rate $K^{-1/2}$, the EOOPG would be asymptotically efficient with a limit variance $\Theta(H^4/K).$ However, they require that high-dimensional probability densities be precisely estimated at $K^{-1/2}$ rate, which is often a harder problem. Several other methods for off-policy PG, such as Nonparametric OPPG \cite{tosatto2020nonparametric} and Q-Prop \cite{gu2016q}, have been empirically proven to be sample efficient. However, theoretical understanding for off-policy PG remains limited. We summarize known variance bounds for off-policy PG estimation in Table \ref{table_off_policy_PG_methods}.

Off-policy PG estimation is closely related to policy optimization. For example, even in on-policy optimization, one may try to use past data to come up with more accurate estimates of new PG. %We will survey related results in the appendix.
Several works \cite{papini2018stochastic, xu2020improved, xu2019sample} combines IS with variance reduction technique, but their theory is still based on the assumption that the variance of the IS estimator is bounded at some controllable level instead of grow exponentially \cite{jiang2016doubly, degris2012off, kallus2020statistically}. %\cite{spoken} optimizes policy using off-policy PG estimation similar to a variance-reduced variant of REINFORCE, and shows empirically it achieves good performance on spoken dialogue system.
\cite{tosatto2020nonparametric} provides a non-parametric OPPG method with some error analysis. \cite{liu2019off, gu2016q} combine off policy PG estimation with actor-critic/policy gradient schemes.

Another closely related topic is the Offline Policy Evaluation (OPE), i.e., to estimate the target policy's value given offline data generated by some behavior policy $\bar{\pi}$. Various methods, from importance sampling to doubly robust estimators have been proposed \cite{tokdar2010importance,10.5555/645529.658134, jiang2016doubly, thomas2016data}. A marginalized importance sampling \cite{xie2019towards} and a fitted Q evaluation approach provably achieve minimax-optimal error bound with matching information-theoretic lower bounds.  %marginalized importance sampling (MIS); in the linear function approximation setting, } achieves minimax optimal error bound with Fitted Q-Evaluation (FQE). 
We defer more discussions of these related areas to the appendix.

\begin{comment}
\paragraph{On-policy Policy Gradient Methods} In order to achieve the optimal policy, policy gradient methods update parameters along the direction of estimated gradient. Vanilla Policy Gradient(PG) method REINFORCE and its variant GPOMDP using Natural Policy Gradient(NPG) methods were proposed and developed by \cite{williams1992simple, sutton2000policy, kakade2001natural}. Early policy gradient methods suffered from its huge variance, hence a lot of variance-reduced modifications of REINFORCE and GPOMDP were proposed. 
\cite{papini2018stochastic} introduced stochastic gradient technique into gradient estimation and proposed Stochastic Variance-Reduced Policy Gradient(SVRPG) algorithm to effectively reduce variance and achieved $O(1/\varepsilon^2)$ sample complexity to reach an $\varepsilon$ stationary policy, i.e, $\left\|\mathbb{E} [\nabla_{\theta} v_{\theta}]\right\|_2^2 \leq \varepsilon.$ This complexity upper bound was soon improved to $O(1/\varepsilon^{\frac{5}{3}})$ by \cite{xu2020improved}, and further to $O(1/\varepsilon^{\frac{3}{2}})$ by \cite{xu2019sample}. Another variant, Hessian Aid Policy Gradient(HAPG) in \cite{shen2019hessian}, replaces calculating gradient correction in stochastic gradient estimation with constructing an unbiased estimate of policy Hessian and also needs $O(1/\varepsilon^{\frac{3}{2}})$ sampled trajectories to reach $\varepsilon$ stationary policy. For a long time, analysis of convergent behavior has been limited within local optimality. Recently, \cite{agarwal2021theory} prove the convergence to the global optimal solution for the first time. They prove that in tabular case, several PG methods converge with $O(1/\varepsilon^2)$ sample complexity, while NPG method with softmax paramiterization converges with $O(1/\varepsilon)$ samples to global optimum. 
\end{comment}

\section{Assumptions}
Consider using a function approximator to directly represent $Q$ functions and its gradient maps. Suppose there is a function class $\mathcal{F}$ that is sufficiently expressive. In this paper, we focus on situations where $Q^{\theta}$ and $\nabla_{\theta}Q^{\theta}$ can be represented within this function class $\mathcal{F}$. 
%In particular, our analysis will center around the case of a linear $\mathcal{F}$.
%We will first present our method in terms of some general function class, and later confine $\mathcal{F}$ to the class of linear functions of state-action features when presenting our theory. 

%We assume that the Q function and its gradient map can be represented by $\mathcal{F}$. 
%In particular, 
\begin{assumption}
\label{fclass}
For any $f\in\mathcal{F}$ and $h\in[H]$, we have $\mathcal{P}_{\theta, h} f\in\mathcal{F}$, and we suppose $r_h\in\mathcal{F}, \forall h\in[H]$. It follows that $Q^{\theta}_h\in\mathcal{F}, \forall h\in[H],\theta\in\Theta$.
\end{assumption}
The above assumption is very common in the literature. It requires $\mathcal{F}$ be closed under the transition operator $\mathcal{P}_\theta$, so that the function approximation incurs zero Bellman error. 
\begin{assumption}
\label{gclass}
$\nabla_\theta Q^\theta_h\in\mathcal{F},\forall h\in[H], \theta\in\Theta$. 
\end{assumption}
We will focus on the important case where $\mathcal{F}$ is a linear function class.
Let $\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^d$ be a state-action feature map. Let $\mathcal{F}$ be the class of linear functions given by $\mathcal{F}=\{\phi(\cdot,\cdot)^\top w\vert w\in\mathbb{R}^d\}$. Then for any policy $\pi_\theta$ and $h\in[H]$, Assumption \ref{fclass} implies there exist $w_r\in\mathbb{R}^d$ and $w_h^\theta\in\mathbb{R}^d$ such that
\begin{align*}
    r_h(s,a)=\phi(s,a)^\top w_{r,h},\quad Q_h^\theta(s,a)=\phi(s,a)^\top w_h^\theta. 
\end{align*}
Furthermore, we show that Assumption \ref{fclass} alone is sufficient to ensure the expressiveness of $\mathcal{F}$ for PG estimation in case of the linear function class. 
\begin{proposition}
\label{lin_rep}
If $\mathcal{F}=\{\phi(\cdot,\cdot)^\top w\vert w\in\mathbb{R}^d\}$, Assumption \ref{fclass} implies Assumption \ref{gclass}. In particular, we have $w_h^\theta$ is differentiable w.r.t. $\theta$ and
\begin{align*}
    \nabla_\theta Q_h^\theta(s, a) = \phi(s, a)^\top \nabla_\theta w_h^\theta,\quad \forall h\in[H]. 
\end{align*}
\end{proposition}
In other word, as long as one can use linear function approximation for policy evaluation, the same feature map automatically allows linear function approximation of every $ \nabla_\theta Q_h^\theta$.

\section{Approach}
In this section, we focus on the estimation of the policy gradient $\nabla_\theta v_\theta$ from the batch data $\mathcal{D}$. %For convenience, we denote $(s_n, a_n, s_n^\prime, r_n)$ as the $n$th sample from $\mathcal{D}$, where $n\in[N]$. 

\subsection{Policy Gradient Bellman Equation}
Notice that by Bellman's equation, we have
\begin{align*}
    Q_h^\theta(s,a)&=r_h(s,a)+\int_{\mathcal{S}\times\mathcal{A}}p_h(s^\prime\vert s,a)\pi_{\theta, h+1}(a^\prime\vert s^\prime)Q_{h+1}^\theta(s^\prime, a^\prime)\mathrm{d}s^\prime\mathrm{d}a^\prime.
\end{align*}
Differentiating both sides w.r.t. $\theta$, we get %and applying the chain rule, we get
\begin{align*}
    \nabla_\theta Q_h^\theta(s,a)=&\int_{\mathcal{S}\times\mathcal{A}}p_h(s^\prime\vert s,a)\left(\nabla_\theta\pi_{\theta,h+1}(a^\prime\vert s^\prime)\right) Q_{h+1}^\theta(s^\prime,a^\prime)\mathrm{d}s^\prime\mathrm{d}a^\prime+\int_{\mathcal{S}\times\mathcal{A}}p_h(s^\prime\vert s,a)\pi_{\theta,h+1}(a^\prime\vert s^\prime)\nabla_\theta Q_{h+1}^\theta(s^\prime,a^\prime)\mathrm{d}s^\prime\mathrm{d}a^\prime\\
    =&\mathbb{E}^{\pi_\theta}\left[\left.\left(\nabla_\theta\log\pi_{\theta,h+1}(a_{h+1}\vert s_{h+1})\right)Q_{h+1}^\theta(s_{h+1},a_{h+1})+\nabla_\theta Q_{h+1}^\theta(s_{h+1},a_{h+1})\right\vert s_h=s,a_h=a\right].
\end{align*}
Here we use the convention that the gradient of $\nabla_\theta Q_h^\theta$ or $\nabla_\theta \pi_{\theta,h}$ is a function from $\mathcal{S}\times\mathcal{A}$ to a row vector in $\mathbb{R}^{1\times m}$. Thus, we get the {\it Policy Gradient Bellman equation}, given by 
\begin{align}
    \label{bel}
    Q_h^\theta &= r_h + \mathcal{P}_{\theta,h} Q_{h+1}^\theta\\
    \label{bel_g}
    \nabla_\theta Q_h^\theta &= \mathcal{P}_{\theta,h}\left(\left(\nabla_\theta\log\Pi_{\theta,h+1}\right)Q_{h+1}^\theta+\nabla_\theta Q^\theta_{h+1}\right),
\end{align}
where we define the operator $\nabla_\theta\log \Pi_{\theta,h}$ by
\begin{align*}
	\left(\left(\nabla_\theta\log\Pi_{\theta,h}\right) f\right)(s, a)= \left(\nabla_\theta\log\pi_{\theta,h}(a\vert s)\right)f(s, a). 
\end{align*}
Once we get the estimations of $Q_1^\theta$ and $\nabla_\theta Q_1^\theta$, we can calculate the policy gradient $\nabla_\theta v_\theta$ using the formula: 
\begin{align*}
     \nabla_\theta v_\theta=\int_{\mathcal{S}\times\mathcal{A}}\xi(s)\pi_{\theta,1}(a\vert s)\left(\nabla_\theta Q_1^\theta(s,a)+\left(\nabla_\theta\log\pi_{\theta,1}(a\vert s)\right) Q_1^\theta(s,a)\right)\mathrm{d}s\mathrm{d}a.
\end{align*}

\subsection{Fitted PG Estimation}
In a similar spirit to Fitted Q-Evaluation, we develop our PG estimator based on the gradient Bellman equations \eqref{bel} and \eqref{bel_g}.  We derive our estimator by applying regression recursively: Let $\widehat{Q}_{H+1}^{\theta,\textrm{FPG}}=\widehat{\nabla_\theta^j Q_{H+1}^{\theta,\textrm{FPG}}}=0, \forall j\in[m]$. For $h=H,H-1,\ldots, 1$ and $j\in[m]$, let
{\small
\begin{align}
    \label{Q_rec}
    &\widehat{Q}_h^{\theta,\textrm{FPG}}=\mathop{\arg\min}_{f\in\mathcal{F}}\left(\sum_{k=1}^K\left(f\left(s^{(k)}_{h},a^{(k)}_{h}\right)-r^{(k)}_h-\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a^\prime\left\vert s^{(k)}_{h+1}\right.\right)\widehat{Q}_{h+1}^{\theta,\textrm{FPG}}\left(s^{(k)}_{h+1}, a^\prime\right)\mathrm{d}a^\prime\right)^2+\lambda\rho(f)\right)\\
    &\widehat{\nabla_\theta^j Q_h^{\theta,\textrm{FPG}}}\nonumber\\
    =&\mathop{\arg\min}_{f\in\mathcal{F}}\Bigg(\sum_{k=1}^K\left(f\left(s^{(k)}_h,a^{(k)}_h\right)-\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a^\prime\left\vert s^{(k)}_{h+1}\right.\right)\left(\left(\nabla_\theta^j\log\pi_{\theta, h+1}\left(a^\prime\left\vert s^{(k)}_{h+1}\right.\right)\right)\widehat{Q}_{h+1}^{\theta,\textrm{FPG}}\left(s^{(k)}_{h+1},a^\prime\right)+\widehat{\nabla_\theta^j Q_{h+1}^{\theta,\textrm{FPG}}}\left(s^{(k)}_{h+1}, a^\prime\right)\right)\mathrm{d}a^\prime\right)^2\nonumber\\
    \label{gQ_rec}
    &+\lambda\rho(f)\Bigg)
\end{align}}
After the computation of $\widehat{Q}_h^{\theta,\textrm{FPG}}, \widehat{\nabla_\theta Q_h^{\theta,\textrm{FPG}}}$, the policy gradient can be estimated straightforwardly. 
\begin{comment}
\begin{align*}
    &\widehat{\nabla_\theta v_\theta^{\textrm{FPG}}}=\int_{\mathcal{S}\times\mathcal{A}} \xi(s)\pi_\theta(a\vert s)\\
    \cdot&\left(\widehat{\nabla_\theta Q_1^{\theta,\textrm{FPG}}}(s,a)+\nabla_\theta\log\pi_\theta(a\vert s)\cdot \widehat{Q_1^{\theta,\textrm{FPG}}}(s,a)\right)\mathrm{d}s\mathrm{d}a.
\end{align*}
\end{comment}
The full algorithm is summarized in Algorithm \ref{alg1}.
\begin{algorithm}[htb!]
\caption{Fitted PG Algorithm}
\label{alg1}
	\begin{algorithmic}[1] 
		\Require Dataset $\mathcal{D}=\{(s^{(k)}_h,a^{(k)}_h,s^{(k)}_{h+1},r^{(k)}_h)\}_{h\in[H],k\in[K]}$, target policy $\pi_\theta$, initial state distribution $\xi$.
		\State \textbf{Initialize } $\widehat{Q_{H+1}^{\theta,\textrm{FPG}}}=0$ and $\widehat{\nabla_\theta^j Q_{H+1}^{\theta,\textrm{FPG}}}=0,\ \forall j\in[m]$.
		\For{$h=H,H-1,\ldots,1$}   
		\State Calculate $\widehat{Q}_h^{\theta,\textrm{FPG}},\widehat{\nabla_\theta Q_h^{\theta,\textrm{FPG}}}$ by solving \eqref{Q_rec} and \eqref{gQ_rec}. 
		\EndFor
		\State \textbf{Return} 
		\begin{align*}
		    \widehat{\nabla_\theta v_\theta^{\textrm{FPG}}}&=\int_{\mathcal{S}\times\mathcal{A}} \xi(s)\pi_{\theta,1}(a\vert s)\left(\widehat{\nabla_\theta Q_1^{\theta,\textrm{FPG}}}(s,a)+\widehat{Q}_1^{\theta,\textrm{FPG}}(s,a)\nabla_\theta\log\pi_{\theta,1}(a\vert s)\right)\mathrm{d}s\mathrm{d}a.
		\end{align*}
	\end{algorithmic}
\end{algorithm}

\subsection{Equivalence to a Model-based Plug-in Estimator}

Next we show that the FPG estimator is equivalent to a model-based plugin estimator. 
Define the model-based reward estimate $\widehat{r}$ and transition operator estimate $\widehat{\mathcal{P}}_\theta$ as
\begin{align*}
    \widehat{r}_h&:=\mathop{\arg\min}_{f^\prime\in\mathcal{F}}\left(\sum_{k=1}^K\left(f^\prime\left(s^{(k)}_h,a^{(k)}_h\right)-r^{(k)}_h\right)^2+\lambda\rho(f^\prime)\right),\\
    \widehat{\mathcal{P}}_{\theta,h} f&:=\mathop{\arg\min}_{f^\prime\in\mathcal{F}}\Bigg(\sum_{k=1}^K\left(f^\prime\left(s_h^{(k)},a_h^{(k)}\right)-\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a^\prime\left\vert s_h^{(k)}\right.\right)f\left(s_{h+1}^{(k)}, a^\prime\right)\mathrm{d}a^\prime\right)^2+\lambda\rho(f^\prime)\Bigg),\quad \forall f:\mathcal{S}\times\mathcal{A}\rightarrow \mathbb{R}, h\in[H].
\end{align*}
Plugging $\widehat{\mathcal{P}}_\theta$ and $\widehat{r}$ into \eqref{bel} and \eqref{bel_g}, we may calculate the policy gradient associated with the estimated model. By using the PG Bellman equation, we have $\widehat{Q}_{H+1}^{\theta,\textrm{MB}}=\widehat{\nabla_\theta^j Q_{H+1}^{\theta,\textrm{MB}}}=0, j\in[m]$, for $h=H,H-1,\ldots, 1$, 
\begin{align*}
    \widehat{Q}_h^{\theta,\textrm{MB}}&=\widehat{r}_h+\widehat{\mathcal{P}}_{\theta,h}\widehat{Q}_{h+1}^{\theta,\textrm{MB}},\\
    \widehat{\nabla_\theta^j Q_h^{\theta,\textrm{MB}}}&= \widehat{\mathcal{P}}_{\theta,h}\left(\left(\nabla_\theta^j\log\Pi_{\theta,h+1}\right)\widehat{Q}_{h+1}^{\theta,\textrm{MB}}+\widehat{\nabla_\theta^j Q^{\theta,\textrm{MB}}_{h+1}}\right), j\in[m].
\end{align*}
Then the model-based gradient estimator is 
\begin{align*}
    &\widehat{\nabla_\theta v_\theta^{\textrm{MB}}}=\int_{\mathcal{S}\times\mathcal{A}} \xi(s)\pi_{\theta,1}(a\vert s)\left(\widehat{\nabla_\theta Q_1^{\theta,\textrm{MB}}}(s,a)+\widehat{Q}_1^{\theta,\textrm{MB}}(s,a)\nabla_\theta\log\pi_{\theta,1}(a\vert s)\right)\mathrm{d}s\mathrm{d}a.
\end{align*}
Note that the model-based plug-in approach makes intuitive sense, but is intractable to implement.

Remarkably, we show that the model-based plug-in estimator $\widehat{\nabla_\theta v_\theta^{\textrm{MB}}}$ is essentially equivalent to the fitted PG estimator, when $\mathcal{F}$ is the class of linear functions.

\begin{proposition}
\label{equiv_mb}
When $\mathcal{F} = \{\phi(\cdot,\cdot)^\top w\vert w\in\mathbb{R}^d\}$ and the regulator $\rho$ is chosen to be $\rho(\phi^\top w) = \Vert w\Vert^2$, we have 
\begin{itemize}
    \item $\widehat{Q}_h^{\theta,\mathrm{FPG}} = \widehat{Q}_h^{\theta,\mathrm{MB}}, \widehat{\nabla_\theta Q_h^{\theta,\mathrm{FPG}}}=\widehat{\nabla_\theta Q_h^{\theta,\mathrm{MB}}}, \quad\forall h\in[H]$;
    \item $\widehat{\nabla_\theta v_\theta^{\mathrm{FPG}}}=\widehat{\nabla_\theta v_\theta^{\mathrm{MB}}}$.
\end{itemize}
\end{proposition}

In the remainder, we focus on linear $\mathcal{F}$ and let $\rho(\phi^\top w) = \Vert w\Vert^2$. We will omit the superscript $\textrm{FPG}$ and $\textrm{MB}$ , and simply denote $\widehat{Q}_h^\theta, \widehat{\nabla_\theta Q_h^\theta}, \widehat{\nabla_\theta v_\theta}$ as our estimators. 

\subsection{FPG with Linear Function Approximation}
Define the empirical covariance matrix
\begin{align*}
    \widehat{\Sigma}_h=\frac{1}{K}\left(\lambda I_d+\sum_{k=1}^K\phi\left(s_h^{(k)}, a_h^{(k)}\right)\phi\left(s_h^{(k)}, a_h^{(k)}\right)^\top\right),\quad h\in[H]. 
\end{align*}
where $I_d\in\mathbb{R}^{d\times d}$ is the identity matrix. In the case of linear function class, one could write down the expression of $\widehat{r}$ and $\widehat{\mathcal{P}}_\theta$ explicitly: for any (possibly vector-valued) function $f$ on $\mathcal{S}\times\mathcal{A}$,
\begin{align}
    \label{wr}
    \widehat{r}_h(\cdot,\cdot)&=\phi(\cdot,\cdot)^\top\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)r_h^{(k)}=:\phi(\cdot,\cdot)^\top\widehat{w}_{r,h},\\
    \left(\widehat{\mathcal{P}}_{\theta,h} f\right)(\cdot,\cdot)&=\phi(\cdot,\cdot)^\top\widehat{\Sigma}^{-1}_h\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)f\left(s_{h+1}^{(k)}, a^\prime\right)\mathrm{d}a^\prime.\nonumber
\end{align}
For $f(\cdot,\cdot)=\phi(\cdot,\cdot)^\top w\in\mathcal{F}$, the above become concise closed forms: 
\begin{align*}
    \left(\widehat{\mathcal{P}}_{\theta,h} f\right)(\cdot,\cdot)=\phi(\cdot,\cdot)^\top\widehat{M}_{\theta,h} w,\quad \left(\widehat{\mathcal{P}}_{\theta,h}\left(\nabla_\theta\log\Pi_{\theta,h+1}\right)f\right)(\cdot,\cdot)=\phi(\cdot,\cdot)^\top\widehat{\nabla_\theta M_{\theta,h}}\left(I_m \otimes w\right).
\end{align*}
where the notation $\otimes$ is used to denote the Kronecker product between two matrices, $\widehat{M_{\theta,h}}\in\mathbb{R}^{d\times d}, \widehat{\nabla_\theta M_{\theta,h}}\in\mathbb{R}^{d\times md}$ are defined by
\begin{align}
    \label{M}
	\widehat{M_{\theta,h}}&:=\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)}, a_h^{(k)}\right)\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\phi\left(s_{h+1}^{(k)}, a^\prime\right)^\top\mathrm{d}a^\prime,\\
	\label{gM}
	\widehat{\nabla_\theta M_{\theta,h}}&:=\nabla_\theta\widehat{M}_{\theta,h}=\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\int_{\mathcal{A}}\phi\left(s_{h+1}^{(k)}, a^\prime\right)^\top\left(\nabla_\theta\pi_{\theta,h+1}\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\otimes I_d\right)\mathrm{d}a^\prime
\end{align}
In this way, one can easily compute $\widehat{Q}_h^\theta$ and $\widehat{\nabla_\theta Q_h^\theta}$ in a matrix recursive form, which we illustrate in Algorithm \ref{alg2}.
\begin{algorithm}[htb!]
\caption{Fitted PG Esimation with Linear Function Approximator}
\label{alg2}
	\begin{algorithmic}[1] 
		\Require Dataset $\mathcal{D}=\{(s_h^{(k)},a_h^{(k)},s^{(k)}_{h+1},r_h^{(k)})\}_{h\in[H],k\in[K]}$, target policy $\pi_\theta$, initial state distribution $\xi$.
		\State Calculate $\widehat{w}_{r,h}, \widehat{M}_{\theta,h}, \widehat{\nabla_\theta M_{\theta,h}},\ h\in[H]$ according to \eqref{wr}, \eqref{M}, \eqref{gM}
		\State Let $\widehat{w}_{H+1}^\theta=\bm{0}_d$ and $\widehat{W}_{H+1}^\theta=\bm{0}_{d\times m}.$
		\For{$h=H,H-1,\ldots,1$}   
		\State Set $\widehat{w}_h^{\theta}=\widehat{w}_{r,h}+\widehat{M}_{\theta,h}\widehat{w}_{h+1}^\theta, \quad\widehat{W}^\theta_h=\widehat{\nabla_\theta M_{\theta,h}}(I_m\otimes\widehat{w}_{h+1}^\theta)+\widehat{M_{\theta,h}}\widehat{W}^\theta_{h+1}.$
		\EndFor
		\State Return
$
\widehat{\nabla_\theta v_\theta}=\int_{\mathcal{S}\times\mathcal{A}}\xi(s)\pi_{\theta,1}(a\vert s)\phi(s,a)^\top\left(\widehat{W}_1^\theta+\widehat{w}_1^\theta\nabla_\theta\log\pi_{\theta,1}(a\vert s)\right)\mathrm{d}s \mathrm{d}a.
$
	\end{algorithmic}
\end{algorithm}
\paragraph{Runtime Complexity} Algorithm \ref{alg2} is computationally very efficient. Suppose that caculating integral against action distribution takes time $O(1)$. In Algorithm 2, the calculation of $\widehat{w}_{r,h}, \widehat{M}_{\theta,h}$ and $\widehat{\nabla_\theta M_{\theta,h}}$ require at most $O(KHmd^2)$ numeric operations. The recursive function fitting steps at line 3-5 require at most $O(Hmd^2)$ numeric operations. Thus the total runtime is only $O(KHmd^2)$. 

\section{Main Results}
In this section we study the statistical properties of the FPG estimator. 
Define the population covariance matrix as $\Sigma_h:=\mathbb{E}\left[\phi\left(s^{(1)}_h,a^{(1)}_h\right)\phi\left(s^{(1)}_h,a^{(1)}_h\right)^\top\right],\ h\in[H]$,
where $\mathbb{E}$ represents the expectation over the data generating distribution by behavior policy.
\begin{assumption}[Boundedness Conditions]\label{Boundedness_Conditions}
	Assume for any $h\in[H]$, $\Sigma_h$ is invertible. There exist absolute constants $C_1, G$ such that for any $h\in[H]$ and $(s,a)\in \mathcal{S}\times\mathcal{A},j\in[m]$, we have
	\begin{align*}
		\phi(s,a)^\top\Sigma^{-1}_h\phi(s,a)\leq C_1 d,\quad\left\vert\nabla_\theta^j\log\pi_{\theta,h}(a\vert s)\right\vert\leq G.
	\end{align*}
\end{assumption}
Define $\nu^\theta_h:=\mathbb{E}^{\pi_\theta}\left[\phi(s_h,a_h)\vert s_1\sim\xi\right]$ and $\Sigma_{\theta,h}:=\mathbb{E}^{\pi_\theta}\left[\left.\phi(s_h,a_h)\phi(s_h,a_h)^\top\right\vert s_1\sim\xi\right]$. 

\subsection{Finite-Sample Variance-Aware Error Bound}
Let us first consider finite-sample analysis of our estimator. We present a variance-aware error bound. Define
\begin{align*}
\phi_{\theta,h}(s)=\int_{\mathcal{A}}\pi_{\theta,h}(a^\prime\vert s)\phi(s,a^\prime)\mathrm{d}a^\prime,\quad\varepsilon_{h,k}^\theta=Q_h^\theta\left(s_h^{(k)},a_h^{(k)}\right)-r_h^{(k)}-\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)Q_{h+1}^\theta\left(s_{h+1}^{(k)},a^\prime\right)\mathrm{d}a^\prime,     
\end{align*}
and 
\begin{align*}
   &\Lambda_\theta=\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}_h\nu_h^\theta\right)\right)^\top\nabla_\theta\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}_h\nu_h^\theta\right)\right].
\end{align*}

\begin{theorem}[Finite Sample Guarantee] 
\label{thm2_var}
For any $t\in\mathbb{R}^m$, when $K\geq 36\kappa_1(4+\kappa_2+\kappa_3)^2C_1dH^2\log\frac{8dmH}{\delta}$ and $\lambda\leq\log\frac{8dmH}{\delta}C_1d\min_{h\in[H]}\sigma_{\min}(\Sigma_h)$, with probability $1-\delta$, we have,
\begin{align*}
    &\vert\langle t, \widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\rangle \vert\leq  \sqrt{\frac{2t^\top\Lambda_\theta t}{K}\cdot \log\frac{8}{\delta}}+\frac{C_\theta\Vert t\Vert\log\frac{72mdH}{\delta}}{K},
\end{align*}
where $C_\theta=240C_1dm^{0.5}H^3\kappa_1\left(5+\kappa_2+\kappa_3)(\max_{j\in[m]}\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+HG\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)$ and
\begin{align*}
    \kappa_1&=\max_{h\in[H]}\frac{\sigma_{\max}\left(\Sigma_h^{-\frac{1}{2}}\Sigma_{\theta,h}\Sigma_h^{-\frac{1}{2}}\right)}{\sigma_{\min}\left(\Sigma_{h+1}^{-\frac{1}{2}}\Sigma_{\theta,h+1}\Sigma_{h+1}^{-\frac{1}{2}}\right)\wedge 1},\quad\kappa_2=\max_{h\in[H]}\left\Vert\Sigma_{h+1}^{-\frac{1}{2}}\mathbb{E}\left[\phi_{\theta,h+1}\left(s_{h+1}^{(1)}\right)\phi_{\theta,h+1}\left(s_{h+1}^{(1)}\right)^\top\right]\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert^{\frac{1}{2}},\\
    \kappa_3&=\frac{1}{G}\max_{j\in[m],h\in[H]}\left\Vert\Sigma_{h+1}^{-\frac{1}{2}}\mathbb{E}\left[\left(\nabla_\theta^j \phi_{\theta,h+1}\left(s_{h+1}^{(1)}\right)\right)\left(\nabla_\theta^j \phi_{\theta,h+1}\left(s_{h+1}^{(1)}\right)\right)^\top\right]\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert^{\frac{1}{2}}.
\end{align*}
\end{theorem}
Theorem \ref{thm2_var} shows that the finite-sample FPG error is largely determined by $\sqrt{\frac{t^T \Lambda_{\theta}t}{K}}.$ Here $\Lambda_{\theta}$ gives a precise characterization of the error's covariance. 


\subsection{Worst-Case Error Bound and Distribution Shift}
Next we derive a worst-case error bound that depends only on the distribution shift but not on reward/variance properties. The following theorem provides a worst-case guarantee under arbitrary choice of the reward function. 
\begin{theorem}[Finite Sample Guarantee - Reward Free]
\label{thm2}
Let the conditions in Theorem \ref{thm2_var} hold, with probability $1-\delta$, we have for any $r$,
\begin{align*}
&\left\vert\widehat{\nabla_\theta^j v_\theta}-\nabla_\theta^j v_\theta\right\vert\leq 4b_\theta\sqrt{\frac{\min\{C_1d,H\}\log\frac{8m}{\delta}}{K}}+\frac{C_\theta\log\frac{72mdH}{\delta}}{K}, \quad\forall j\in[m],
\end{align*}
where $b_\theta=H^2G\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nu_h^\theta\right\Vert+H\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nabla_\theta^j\nu_h^\theta\right\Vert$ and $C_\theta$ is the same as that in Theorem \ref{thm2_var}. If we in addition have $\phi(s^\prime,a^\prime)^\top\Sigma_{h}^{-1}\phi(s,a)\geq 0,\forall (s,a),(s^\prime,a^\prime)\in\mathcal{S}\times\mathcal{A},h\in[H]$, we have
\begin{align*}
&\left\vert\widehat{\nabla_\theta^j v_\theta}-\nabla_\theta^j v_\theta\right\vert\leq 4H^2G\sqrt{\frac{\min\{C_1d,H\}\log\frac{8m}{\delta}}{K}}\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nu_h^\theta\right\Vert+\frac{2C_\theta\log\frac{72mdH}{\delta}}{K}, \quad\forall j\in[m].
\end{align*}
%Furthermore, when the MDP is tabular with one-hot features, we have
%\begin{align*}
%&\vert\widehat{\nabla_\theta^j v_\theta}-\nabla_\theta^j v_\theta\vert\leq 4H^2G \sqrt{\sup_{s\in\mathcal{S}, a\in\mathcal{A}, h\in[H]} \frac{d^{\pi_\theta}_h(s,a)}{d^{\bar\pi}_h(s, a)}\frac{\log\frac{8m}{\delta}}{K}}+\frac{C\log\frac{72mdH}{\delta}}{K}, \quad\forall j\in[m],
%\end{align*}
\end{theorem}
The complete proofs of Theorem \ref{thm2_var} and Theorem \ref{thm2} are deferred to Appendix \ref{pfthm2_var} and \ref{pfthm2}. To further simplify the expression in Theorem \ref{thm2}, we define a variant of $\chi^2$-divergence restricted to the family $\mathcal{F}$: for any two groups of probability distributions $\{p_{1,h}\}_{h=1}^H, \{p_{2,h}\}_{h=1}^H$, define
\begin{align*}
    \chi^2_{\mathcal{F}}(\{p_{1,h}\}_{h=1}^H,\{p_{2,h}\}_{h=1}^H):=\max_{h\in[H]}\sup_{f\in\mathcal{F}}\frac{\mathbb{E}_{p_{1,h}}\left[f(x)\right]^2}{\mathbb{E}_{p_{2,h}}\left[f(x)^2\right]}-1 = \max_{h\in[H]}\nu_{p_{1,h}}^\top\Sigma^{-1}_{p_{2,h}}\nu_{p_{1,h}}-1,
\end{align*}
where $\nu_{p}=\mathbb{E}_p[\phi(s,a)],\ \Sigma_p=\mathbb{E}_p[\phi(s,a)\phi(s,a)^\top]$. Let $\bar{\mu}_h$ be the expected occupancy measure of observation $\left(s_h^{(1)},a_h^{(1)}\right)$. Let $\mu_{\theta,h}$ be the expected occupancy distribution of $(s_h,a_h)$ under policy $\pi_\theta$. When we have $\phi(s^\prime,a^\prime)^\top\Sigma_{h}^{-1}\phi(s,a)\geq 0,\ \forall (s,a),(s^\prime,a^\prime)\in\mathcal{S}\times\mathcal{A},h\in[H]$, the result of Theorem \ref{thm2} implies 
\begin{align*}
&\vert\widehat{\nabla_\theta^j v_\theta}-\nabla_\theta^j v_\theta\vert\leq 4H^2G\sqrt{\frac{\min\{C_1d,H\}\log\frac{8m}{\delta}}{K}}\sqrt{1+\chi_{\mathcal{F}}^2(\{\mu_{\theta,h}\}_{h=1}^H,\{\bar{\mu}_h\}_{h=1}^H)}+\frac{2C_\theta\log\frac{72mdH}{\delta}}{K},\quad\forall j\in[m],
\end{align*}
The result of Theorem \ref{thm2} matches the asymptotic bound provided in \cite{kallus2020statistically} but requires less stringent conditions. 

\subsection{Asymptotic Normality and Cramer-Rao Lower Bound}
Next we show that FPG is an asymptotically normal and efficient estimator. 
\begin{theorem}[Asymptotic Normality]
\label{thm1}
The FPG estimator given by Algorithm \ref{alg2} is asymptotically normal:
\begin{align*}
    \sqrt{K}\left(\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\right)\stackrel{d}{\rightarrow}\mathcal{N}(0,\Lambda_\theta).
\end{align*}
\end{theorem}
The proof of Theorem \ref{thm1} is deferred to Appendix \ref{pfthm1}. An obvious corollary of Theorem \ref{thm1} is that for any vector $t \in \mathbb{R}^m,$ 
\begin{equation*}
    \sqrt{K}\left\langle t,  \widehat{\nabla_{\theta} v_{\theta}} -\nabla_{\theta} v_{\theta} \right\rangle \stackrel{d}{\rightarrow} \mathcal{N}\left(0, t^{\top} \Lambda_{\theta} t\right).
\end{equation*}
%In the expression, gradient is a row vector and $t$ is a column vector. 

An asymptotically efficient estimator has the minimal variance among all the unbiased estimators. The following theorem states the Cramer Rao bound for FPG estimation. 
\begin{theorem}
\label{thm4}
Let Assumption \ref{fclass} hold. For any vector $t\in\mathbb{R}^m$, the variance of any unbiased estimator for $t^{\top}\nabla_{\theta}v_{\theta}  \in \mathbb{R}$ is lower bounded by $\frac{1}{\sqrt{K}}t^{\top} \Lambda_{\theta} t.$
\end{theorem}
The proof of Theorem \ref{thm4} is deferred to Appendix \ref{pfthm4}. Comparing Theorem 6.4 with Theorems 6.1-6.3, our upper bounds tightly match with the Cramer Rao lower bound. These results jointly prove the FQE estimator is statistically optimal.

\subsection{FPG for Policy Optimization}

Lastly we consider the use of FPG for off-policy policy optimization. Suppose that we can reliably estimate the PG for all policies, obtaining $\widehat{\nabla_{\theta}v_{\theta}}$ for all $\theta\in\Theta$. Then we can simply set $\widehat{\nabla_{\theta}v_{\theta}} = 0$, identify all the stationary solutions, and pick the best one. For MDP with Lipschitz continuous policy gradients, we show that a policy with $\widehat{\nabla_{\theta}v_{\theta}} = 0$ would be nearly stationary/optimal.  %small estimated policy gradient is near optimal. 
\begin{assumption}\label{Lipschitz_cont}
Suppose the parameter space $\Theta$ is bounded and the policy gradient is $L$-Lipschitz continuous, i.e., 
\begin{align*}
    \Vert\nabla_{\theta_1}v_{\theta_1}-\nabla_{\theta_2}v_{\theta_2}\Vert\leq L\Vert\theta_1-\theta_2\Vert,\quad\forall\theta_1,\theta_2\in\Theta.
\end{align*}
and $\chi^2_{\mathcal{F}}$ is $L^\prime$-Lipschitz continuous, i.e.,
\begin{align*}
    \left\vert\chi_{\mathcal{F}}^2\left(\{\mu_{\theta_1,h}\}_{h=1}^H,\{\bar{\mu}_h\}_{h=1}^H\right)-\chi_{\mathcal{F}}^2\left(\{\mu_{\theta_2,h}\}_{h=1}^H,\{\bar{\mu}_h\}_{h=1}^H\right)\right\vert\leq L^\prime\Vert\theta_1-\theta_2\Vert,\quad\forall\theta_1,\theta_2\in\Theta.
\end{align*}
\end{assumption}
\begin{proposition}
\label{union_bd}
Suppose assumption \ref{Lipschitz_cont} and the condition of Theorem \ref{thm2} hold. When $K$ is sufficiently large, we have with probability at least $1-\delta$, 
\begin{align*}
    \Vert\nabla_\theta v_\theta-\widehat{\nabla_\theta v_\theta}\Vert\leq 64H^2Gm\sqrt{\min\{C_1d,H\}}\sqrt{1+\chi^2_{\mathcal{F}}(\{\mu_{\theta,h}\}_{h=1}^H,\{\bar{\mu}_h\}_{h=1}^H)}\sqrt{\frac{\log\frac{24DKLL^\prime}{\delta HG}}{K}},\quad\forall\theta\in\Theta.
\end{align*}
where $D$ is the diameter of $\Theta$. 

In addition, if the Polyak-Åojasiewicz condition holds, i.e., there exists a constant $c > 0$ such that for any $\theta \in \Theta$, 
$
    \frac{1}{2}\Vert\nabla_\theta v_\theta\Vert^2 \geq c(v_{\theta^*}-v_\theta). 
$
Then for any $\hat{\theta}$ such that $\widehat{\nabla_\theta v_{\hat{\theta}}}=0$, we have
$v_{\theta^*}-v_{\hat{\theta}}\leq \tilde{\mathcal{O}}\left(\frac{m^2H^4\min\{C_1d,H\} G^2}{K}\right)$. 
\end{proposition}
In general, Proposition \ref{union_bd} implies a $O(1/\epsilon^2)$ sample complexity for finding $\epsilon$-stationary policies. This off-policy sample efficiency is remarkly better than the best know $O(1/\epsilon^3)$ on-policy sample efficiency obtained by variance-reduced PG algorithm, as long as distribution shift is uniformly bounded. This improvement is due to that FPG makes full usage of data to evaluate PG at every $\theta$. 

\paragraph{\textcolor{red}{The case of on-policy optimization}}

Suppose that 
$$\chi_{\mathcal{F}}^2(\theta_1,\theta_2) \leq L\|\theta_1-\theta_2\|,$$ and $$\|\theta-\theta^*\|\leq C \|\nabla_\theta v_\theta\|.$$
Here $L$ quantifies the sensitivity of distribution shift wrt to change in $\theta$.
Can we achieve an $\epsilon$-optimal policy using sample complexity $\frac{\textrm{poly}(L)\log D}{\epsilon^2}$?

We may consider two phases: phase 1 is when $|x-x^*| \in[1, D]$ and in phase 2 we will start with a significantly smaller search space $|x-x^*|\leq 1$.
Diameter of the search space controls the chi-square, thus smaller diameter leads to higher accuracy in subsequent optimization steps. The algorithm works by applying off-policy optimization and Prop 6.1 iteratively, while the search space shrinks.

Phase 1: We begin with the full space with diameter $D$ and collect data using any policy. The gradient estimation error would be bounded by $\sqrt{1+D}\leq (1+D)/\sqrt{K}$. One may reduce the diameter by half using $O(1)$ samples. Repeat the process and cut diameter by half each step. The total number of steps (and sample complexity) to reduce diameter to $O(1)$ would be $\log D$. 
\paragraph{Complexity of Phase 1} Let $D_1=D$ and denote $D_{t+1} = \tilde{C}\sqrt{\frac{1+D_t}{K_t}}$. When we choose
\begin{align*}
    K_t = 4\tilde{C}^2\frac{1+D_t}{D_t^2},
\end{align*}
we have
\begin{align*}
    D_{t+1}\leq\tilde{C}\sqrt{\frac{1+D_t}{K_t}}=\frac{D_t}{2}.
\end{align*}
We repeat the iteration until some $t_0$ where we achieve $D_{t_0+1}\leq 2$. In this case, the total sample complexity would become
\begin{align*}
    \sum_{t=1}^{t_0}K_t = 4\tilde{C}^2\sum_{t=1}^{t_0}\frac{1+D_t}{D_t^2}\leq 8\tilde{C}^2\sum_{t=1}^{t_0}\frac{1}{D_t}\leq 8\tilde{C}^2(1+\frac{1}{2}+\frac{1}{2^2}+\ldots)\leq 16\tilde{C}^2.
\end{align*}


Phase 2: Apply prop. 6.1 to the remainder problem, where chi-square is now bounded by constant. The sample complexity of the second phase would no longer depend on $D$.
\paragraph{Sample Complexity of Phase 2} In this case, to get an $\varepsilon$-stationary policy, we have
\begin{align*}
    K\sim O(\frac{1}{\varepsilon^2})
\end{align*}

%Assuming the value function $v_\theta$ satisfies the Polyak-Åojasiewicz condition, i.e., there exists a constant $c_{PÅ} > 0$ such that for any $\theta \in \Theta$, we have
%\begin{align*}
%\frac{1}{2}\Vert \nabla_\theta v_\theta\Vert^2 \geq c_{PÅ}(v_{\theta^*} - v_\theta). 
%\end{align*}
%Lemma \ref{union_bd} directly leads to the following Corollary: 
%\begin{corollary}
%When $N$ is sufficiently large, for any $\hat{\theta}$ that makes $\Vert\widehat{\nabla_\theta v_{\hat{\theta}}}\Vert\leq\eta^\prime$, we have
%\begin{align*}
%    v_{\theta^*}-v_{\hat{\theta}}\leq&\frac{1}{2c_{PÅ}}\left(2\eta\sqrt{m\log\frac{DL}{\eta}}+\eta^\prime\right)^2\\
%    =&\tilde{\mathcal{O}}\left(\frac{m^2H^6G^2}{N}+\left(\eta^\prime\right)^2\right).
%\end{align*}
%\end{corollary}

\begin{comment}
\section{Distributional Consistency of Bootstrap FQE Estimator}
Recall the original dataset $\mathcal{D}$ consists of $K$ episodes. We propose to bootstrap FPG by episodes: Draw sample episodes $\mathcal{D}_1^*, \ldots, \mathcal{D}_K^*$ independently with replacement from $\mathcal{D}$. Then we run FPG on the new bootstrapped set $\mathcal{D}^* = \{\mathcal{D}_1^*,\ldots, \mathcal{D}_K^*\}$  let the output $\widehat{\nabla_\theta v_\theta^*}$ be the bootstrapping FPG estimator. By repeating the above process, we may obtain multiple samples of $\widehat{\nabla_\theta v_\theta^*}$, and may use these samples to further conduct off-policy inference. In this section, we show that the bootstrapping FPG method is distributionally consistent. More precisely, we prove that the bootstrap distribution distribution of $\sqrt{N}\left(\widehat{\nabla_\theta v_\theta^*}-\widehat{\nabla_\theta v_\theta}\right)$, conditioned on the data $\mathcal{D}$, asymptotically imitates the true error distribution of $\sqrt{N}\left(\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\right)$. Consequently, we may use the method to construct confidence regions with asymptotically correct and tight coverage. 
\begin{theorem}
\label{dis_cons}
Conditioned on $\mathcal{D}$, we have
\begin{align*}
    \sqrt{N}\left(\widehat{\nabla_\theta v_\theta^*}-\widehat{\nabla_\theta v_\theta}\right)\stackrel{d}{\rightarrow}\mathcal{N}(0,\Lambda_\theta)\ \mathrm{as}\ N\rightarrow\infty.
\end{align*} 
\end{theorem}
The complete proof of Theorem \ref{dis_cons} is deferred to Appendix \ref{pfdis_cons}. 
\end{comment}

\rm
\section{Experiments}
In this section, we empirically evaluate the performance of FPG using the OpenAI gym FrozenLake environment with softmax tabular policy parameterization and $H=100$. 
%(Section 6 \cite{zhang2021convergence}). 
We pick the target policy to be a fixed near-optimal policy, and test using dataset generated from different behavior policies. For comparison, we compute the true gradient using the policy gradient theorem and on-policy Monte Carlo simulation.
%trained from REINFORCE. %and in the CliffWalking environment with softmax ReLU policy parameterization. The experiments in CliffWalking are presented in Appendix \ref{exp_appendix}.

\subsection{FPG's data efficiency}
\label{sec:exp-K}
We test the performance of FPG against the size of off-policy data. Choosing the behavior policy to be the $0.1$ $\epsilon$-greedy modification of the target policy, we generate datasets with varying sizes.  We evaluate the FPG's estimation error with two metrics: the cosine of the angle between the true policy gradient and the FPG estimator, and the relative estimation error in $\ell_2$-norm. The closer the cosine is to $1$ and the smaller the relative norm error is, the better the estimated policy gradient is.  
%In the experiments of this section, we average the results over $20$ trials and compare with the popular importance sampling gradient estimator (\textcolor{blue}{add a label to the expression in Section 3 to ref?}) for benchmarking.
\textbf{Figure \ref{fig:FrozenLake_1}} shows that FPG gives good estimate even when the data is rather small. The FPG estimate improves and converges to true gradient asymptotically, with rather moderate variance. 
In comparison, importance sampling (IS) converges much more slowly and incurs substantially larger variance. 
%'s policy gradient estimate is almost irrelevant to the true policy gradient when $K$ is small, and its estimates at $K=500$ are still worse than FPG's estimates at $K=25$. 

\def\grad{\nabla}
\def\hat{\widehat}
\begin{figure}[!t]
 \centering
 \includegraphics[width=0.3\linewidth]{figure/fig1.2.pdf}
  \includegraphics[width=0.3\linewidth]{figure/fig1.2_norm.pdf}
\caption{\textbf{Sample efficiency of FPG on off-policy data.} The off-policy PG estimation accuracy is evaluated using two metrics: $\cos\angle (\hat{\grad_\theta v_{\theta}}, \grad_\theta v_{\theta})$ and the relative error norm $\frac{\| \hat{\grad_\theta v_{\theta}} -  \grad_\theta v_{\theta}\| }{\|\grad_\theta v_{\theta}\|}$. }
\label{fig:FrozenLake_1}
\end{figure}

\subsection{The effect of distributional mismatch} 
\label{sec:exp-mismatch}
Next we investigate the effect of distribution shift on off-policy PG estimation. 
We consider $5$ choices of behavior policies: the target policy, the $0.1$, $0.3$, $0.5$ and $0.7$ $\epsilon$-greedy policies of the target policy. We generate a dataset containing $200$ episodes with each of these behavior policies, run FPG and IS, and evaluate their estimation errors.

\textbf{Figure \ref{fig:FrozenLake_2}} shows that larger distribution mismatch leads to larger estimation error in both methods. However, when compared to IS, FPG is significantly more robust to off-policy distribution shift. The accuracy of FPG only degrades slightly with larger distribution mismatch, while IS suffers from exponentially blowing-up error and stops generating reasonable estimates. 

We would like to point out that as mismatch becomes large, the behavior policy is very different from the target policy, so the importance sampling ratio is more likely to become very small. In this case, FPG would give a near zero gradient estimate, but relative norm error is still $1$, despite such void estimate.


\begin{figure}[!t]
 \centering
  \includegraphics[width=0.3\linewidth]{figure/fig1.1.pdf}
  \includegraphics[width=0.3\linewidth]{figure/fig1.1_norm.pdf}
\caption{\textbf{Tolerance to off-policy distribution shift.} The distributional mismatch is measured by  $\hbox{cond}(\bar{\Sigma}^{1/2}\Sigma^{-1}\bar{\Sigma}^{1/2})$, where $\Sigma$ is the data covariance and $\bar{\Sigma}$ is the target policy's occupancy measure.}
\label{fig:FrozenLake_2}
\end{figure}
%\todo{try using top eigenvalue, not condition number} top eigenvalue is even larger

\subsection{FPG for policy optimization}
\label{sec:exp-optimization}

We further test FPG's applicability to policy optimization. In particular, we test FPG as a gradient estimation module in policy gradient optimization methods. We conduct an experiment using FPG in on-policy REINFORCE and compare it with the vanilla REINFORCE and SVRPG \cite{papini2018stochastic}. All methods are configured to sample $100$ on-policy episodes per iteration. When implementing the FPG-REINFORCE, we take advantage of FPG's off-policy capability and use data from the recent $5$ iterations to improve the gradient estimation accuracy. \textbf{Figure \ref{fig:FrozenLake_3}} shows that such design indeed allows FPG-REINFORCE to converge faster than the other two methods.

\begin{figure}[!t]
 \centering
 \includegraphics[width=0.3\linewidth]{figure/fig1.4.pdf}
\caption{\textbf{FPG for policy optimization.} FPG is used as a module for improving PG estimates in REINFORCE, compared with other baselines.}
\label{fig:FrozenLake_3}
\end{figure}

\subsection{Bootstrap inference for FPG}
Finally we test bootstrap inference to construct confidence regions of FPG estimates by subsampling episodes and estimating the bootstrapped probability distribution. 
%We evaluate the performance of bootstrapping FPG with confidence region estimation. %Specifically, we run Algorithm \ref{alg:BFPG} with FQE and IS, where $B=100$. 
We plot contours of bootstrapped confidence regions via quantile KDE. 
%and subtracting it from $\widehat{\nabla_\theta v_\theta}(\mathcal{D})$. %We compute the mean and covariance of the output to fit a multivariate normal distribution on it, and find the set of points on which the evaluation of the CDF of this normal distribution is at least $1 - \delta$. For every point $v$ within this set, we find $\widehat{\nabla_\theta v_\theta}(\mathcal{D}) - v$, and the resulting set is the confidence region estimated by bootstrap FPG.
\textbf{Figure \ref{fig:FrozenLake_4}} visualizes the bootstrapped confidence regions in 2D, compared with the confidence region for IS and the groundtruth confidence set. We observe that the contours of bootstrapping FPG are much smaller and more accurate than the one by bootstrapping IS. As the data size $K$ increases, the bootstrapped confidence regions become more concentrated. Across all experiments, FQE consistently enjoys substantially smaller confidence regions, confirming our theory.

%\begin{small}

\begin{comment}
\begin{algorithm}[t]
\caption{Bootstrapping FPG}
\label{alg:BFPG}
\begin{algorithmic}[1]%\label{alg:BFPG}
    \Require Dataset $\mathcal{D}$, target policy $\pi$, number of bootstrap samples $B$.
    \State Compute FPG estimator $\widehat{\nabla_\theta v_\theta}(\mathcal{D})$ (Algorithm \ref{alg1}).
    \For{$b=1,\ldots, B$}
    \State Sample $\mathcal{D}$ with replacement by episode $K$ times for $\mathcal{D}_{K}^{(b)*}$.
    \State Compute  $\widehat{\nabla_\theta v_\theta}(\mathcal{D}_{K}^{(b)*})$.
    \State Compute $\varepsilon^{(b)} = \widehat{\nabla_\theta v_\theta}(\mathcal{D}_{K}^{(b)*})-\widehat{\nabla_\theta v_\theta}(\mathcal{D})$.
    \EndFor  
    \State Return $\mathcal{E} := \{\varepsilon^{(1)},\ldots, \varepsilon^{(B)}\}$.
    %\State Return $\{\widehat{\nabla_\theta v_\theta}(\mathcal{D}_{K}^{(1)*}),\ldots, \widehat{\nabla_\theta v_\theta}(\mathcal{D}_{K}^{(B)*})\}$.
\end{algorithmic}
\end{algorithm}
\end{comment}
%\end{small}

\begin{figure}[!t]
 \centering
  \includegraphics[width=0.3\linewidth]{figure/fig1.6_arxiv.pdf}
  \includegraphics[width=0.3\linewidth]{figure/fig1.6_2_arxiv.pdf}
\caption{\textbf{Bootstrapped distribution of PG estimation errors.} The red dot marks the true gradient. } %Left $K=200$; Right $K=500$.
\label{fig:FrozenLake_4}
\end{figure}

\printbibliography

\newpage
\appendix
\section{Technical Lemmas}
Let $U_h^\theta =\mathcal{P}_{\theta,h}\left(\nabla_\theta\log\Pi_{\theta,h+1}\right) Q_{h+1}^\theta,\ \widehat{U}_h^\theta=\widehat{\mathcal{P}}_{\theta,h} \left(\nabla_\theta\log\Pi_{\theta,h+1}\right)Q_{h+1}^\theta,\ \tilde{U}_h^\theta=\widehat{\mathcal{P}}_{\theta,h}\left(\nabla_\theta\log\Pi_{\theta,h+1}\right)\widehat{Q}_{h+1}^\theta$, 
\begin{lemma}
\label{Q_decomp_base}
We have
\begin{align*}
    Q_h^\theta&=\sum_{h^\prime=h}^H\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\mathcal{P}_{\theta, h^{\prime\prime}}\right)r_{h^\prime},\quad\widehat{Q_h^\theta}=\sum_{h^\prime=h}^H\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\widehat{\mathcal{P}}_{\theta, h^{\prime\prime}}\right)\widehat{r}_{h^\prime},\\
    \nabla_\theta Q_h^\theta&=\sum_{h'=h}^H\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\mathcal{P}_{\theta, h^{\prime\prime}}\right) U_{h^\prime}^{\theta},\quad\widehat{\nabla_\theta Q_h^\theta}=\sum_{h^\prime=h}^H\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\widehat{\mathcal{P}}_{\theta, h^{\prime\prime}}\right)\tilde{U}_{h^\prime}^\theta.
\end{align*}
\end{lemma}
\begin{proof}
By Bellman's equation, we have $Q_h^\theta=r_h+\mathcal{P}_{\theta,h} Q_{h+1}^\theta$. Therefore, by induction and use the fact that $Q_{H+1}^\theta=0$, we have proved the first equation. By the policy gradient Bellman's equation, we have
\begin{align*}
    \nabla_\theta Q_h^\theta(s,a)&=\mathbb{E}^{\pi_\theta}\left[\left(\nabla_\theta\log\pi_{\theta, h+1}(a^\prime\vert s^\prime)\right)Q_{h+1}^\theta(s^\prime,a^\prime)\vert s,a,h\right]+\mathbb{E}^{\pi_\theta}\left[\nabla_\theta Q_{h+1}^\theta(s^\prime, a^\prime)\vert s,a,h\right],
\end{align*}
i.e., $\nabla_\theta Q_h^\theta=U_h^\theta+\mathcal{P}_{\theta,h}\nabla_\theta Q_{h+1}^\theta$. By induction, we have proved the third equation. The expressions of $\widehat{Q}_h^\theta$ and $\widehat{\nabla_\theta Q_h^\theta}$ can be derived directly from their definitions and induction. 
\end{proof}
The decomposition leads to the following boundedness result: 
\begin{lemma}
\label{upbd}
We have $\vert Q_h^\theta(s,a)\vert\leq H-h+1,\ \Vert\nabla_\theta Q_h^\theta(s,a)\Vert_\infty\leq G(H-h)^2,\ \forall s\in\mathcal{S},a\in\mathcal{A},h\in[H].$
\end{lemma}
Now we consider the decomposition of $Q_h^\theta-\widehat{Q}_h^\theta$: 
\begin{lemma}
\label{Q_decomp}
We have 
\begin{align*}
    Q_h^\theta-\widehat{Q}_h^\theta=\sum_{h^\prime=h}^H\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(Q_{h^\prime}^\theta-\widehat{r}_{h^\prime}- \widehat{\mathcal{P}}_{\theta,h^\prime}Q_{h^\prime+1}^\theta\right),\quad\forall h\in[H].
\end{align*}
\end{lemma}
\begin{proof}
Simply note that 
\begin{align*}
    Q_h^\theta-\widehat{Q}_h^\theta&=\sum_{h^\prime=h}^{H}\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\mathcal{P}_{\theta,h^{\prime\prime}}\right)r_{h^\prime}-\sum_{h^\prime=h}^H\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\widehat{\mathcal{P}}_{\theta, h^{\prime\prime}}\right)\widehat{r}_{h^\prime}\\
    &=\sum_{h^\prime=h}^H\left(\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\mathcal{P}_{\theta,h^{\prime\prime}}\right)-\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\widehat{\mathcal{P}}_{\theta, h^{\prime\prime}}\right)\right)r_{h^\prime}+\sum_{h^\prime=h}^H\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(r_{h^\prime}-\widehat{r}_{h^\prime}\right)\\
    &=\sum_{h^\prime=h}^H\sum_{h^{\prime\prime}=h}^{h^\prime-1}\left(\prod_{h^{\prime\prime\prime}=h}^{h^{\prime\prime}-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime\prime}}\right)\left(\mathcal{P}_{\theta, h^{\prime\prime}}-\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(\prod_{h^{\prime\prime\prime}=h^{\prime\prime}+1}^{h^\prime-1}\mathcal{P}_{\theta,h^{\prime\prime\prime}}\right)r_{h^\prime}+\sum_{h^\prime=h}^H\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(r_{h^\prime}-\widehat{r}_{h^\prime}\right)\\
    &=\sum_{h^{\prime\prime}=h}^{H}\left(\prod_{h^{\prime\prime\prime}=h}^{h^{\prime\prime}-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime\prime}}\right)\left(\mathcal{P}_{\theta, h^{\prime\prime}}-\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\sum_{h^\prime=h^{\prime\prime}+1}^H\left(\prod_{h^{\prime\prime\prime}=h^{\prime\prime}+1}^{h^\prime-1}\mathcal{P}_{\theta,h^{\prime\prime\prime}}\right)r_{h^\prime}+\sum_{h^\prime=h}^H\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(r_{h^\prime}-\widehat{r}_{h^\prime}\right)\\
    &=\sum_{h^{\prime}=h}^{H}\left(\prod_{h^{\prime\prime}=h}^{h^{\prime}-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(\mathcal{P}_{\theta, h^{\prime}}-\widehat{\mathcal{P}}_{\theta,h^{\prime}}\right)Q^\theta_{h^{\prime}+1}+\sum_{h^\prime=h}^H\left(\prod_{h^{\prime\prime}=h}^{h^\prime-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(r_{h^\prime}-\widehat{r}_{h^\prime}\right)\\
    &=\sum_{h^\prime=h}^H\left(\prod_{h^{\prime\prime}=h}^{h^{\prime}-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(Q_{h^\prime}^\theta-\widehat{r}_{h^\prime}-\widehat{\mathcal{P}}_{\theta,h^\prime}Q_{h^\prime+1}^\theta\right), 
\end{align*}
which is the desired result. 
\end{proof}
The following lemma provides an upper bound of matrix production, which will be used when bounding the higher order terms of the finite sample bound. 
\begin{lemma}
\label{decomp}
For any series of matrices $A_1,A_2,\ldots,A_n$ and $\Delta A_1,\Delta A_2,\ldots\Delta A_n$, we have
\begin{align*}
    \left\Vert\prod_{i=1}^n(A_i+\Delta A_i)-\prod_{i=1}^n A_i\right\Vert\leq \prod_{i=1}^n\left(\Vert A_i\Vert+\Vert\Delta A_i \Vert\right)-\prod_{i=1}^n\Vert A_i\Vert.
\end{align*}
\end{lemma}
\begin{proof}
We have
\begin{align*}
    \left\Vert\prod_{i=1}^n(A_i+\Delta A_i)-\prod_{i=1}^n A_i\right\Vert&= \left\Vert\sum_{\delta\in\{0,1\}^n\setminus\{(1,1,\ldots,1)\}}\prod_{i=1}^n A_i^{\delta_i}(\Delta A_i)^{1-\delta_i}\right\Vert\leq\sum_{\delta\in \{0,1\}^n\setminus\{(1,1,\ldots,1)\}}\prod_{i=1}^n\Vert A_i\Vert^{\delta_i}\Vert\Delta A_i\Vert^{1-\delta_i}\\
    &=\prod_{i=1}^n\left(\Vert A_i\Vert+\Vert\Delta A_i\Vert\right)-\prod_{i=1}^n\Vert A_i\Vert.
\end{align*}
\end{proof}
When $\mathcal{F}$ is the class of the linear functions, there exists matrix $M_{\theta,h}$ such that the transition probability satisfies
\begin{align*}
    \mathbb{E}^{\pi_\theta}\left[\phi(s^\prime,a^\prime)^\top\vert s,a,h\right] = \phi(s,a)^\top M_{\theta,h}. 
\end{align*}
The following lemma gives an upper bound on the 2-norm of $M_{\theta,h}$ and its derivatives. 
\begin{lemma}
\label{ineq}
We have $\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}M_{\theta,h}\Sigma_{\theta,h+1}^{-\frac{1}{2}}\right\Vert \leq 1$ and $\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\left(\nabla_\theta^j M_{\theta,h}\right)\Sigma_{\theta,h+1}^{-\frac{1}{2}}\right\Vert\leq G,\ \forall j\in[m],h\in[H]$.
\end{lemma}
\begin{proof}
Note that for any $f:\mathcal{S}\times\mathcal{A}\rightarrow \mathbb{R},\ f(s,a):=\mu^\top\phi(s, a)$, we have
\begin{align*}
    \mathbb{E}^{\pi_\theta}\left[f^2(s_{h+1},a_{h+1})\vert s_1\sim\xi\right]&=\mathbb{E}^{\pi_\theta}\left[\mathbb{E}^{\pi_\theta}\left[f^2(s_{h+1},a_{h+1})\vert s_h,a_h\right]\vert s_1\sim\xi\right]\\
    &\geq\mathbb{E}^{\pi_\theta}[\mathbb{E}^{\pi_\theta}[f(s_{h+1},a_{h+1})\vert s_h,a_h]^2\vert  s_1\sim\xi]
\end{align*}
The LHS satisfies
\begin{align*}
    \mathbb{E}^{\pi_\theta}[f^2(s_{h+1},a_{h+1})\vert s_1\sim \xi]&=\mu^\top\Sigma_{\theta, h+1}\mu
\end{align*}
and the RHS satisfies
\begin{align*}
    \mathbb{E}^{\pi_\theta}[\mathbb{E}^{\pi_\theta}[f(s_{h+1},a_{h+1})\vert s_h,a_h]^2\vert s_1\sim\xi]=\mathbb{E}^{\pi_\theta}[\mu^\top M_{\theta,h}^\top\phi(s_h,a_h)\phi(s_h,a_h)^\top M_{\theta,h}\mu\vert s_1\sim\xi]=\mu^\top M_{\theta,h}^\top\Sigma_{\theta,h} M_{\theta,h}\mu
\end{align*}
Therefore, we have $\mu^\top\Sigma_{\theta,h+1}\mu\geq\mu^\top M_{\theta,h}^\top\Sigma_{\theta,h} M_{\theta,h} \mu,\ \forall\mu$, which implies $\Vert\Sigma_{\theta,h}^{\frac{1}{2}}M_{\theta,h} \Sigma_{\theta,h+1}^{-\frac{1}{2}}\Vert\leq 1$. Similarly, let $g:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R},\ g(s, a):=\left(\nabla_\theta^j\log\pi_{\theta, h+1}(s, a)\right)\mu^\top\phi(s,a)$, we have
\begin{align*}
    \mathbb{E}^{\pi_\theta}[g^2(s_{h+1},a_{h+1})\vert s_1\sim\xi]=&\mathbb{E}^{\pi_\theta}[\mathbb{E}^{\pi_\theta}[g^2(s_{h+1}, a_{h+1})\vert s_h,a_h]\vert s_1\sim\xi]\\
    \geq&\mathbb{E}^{\pi_\theta}[\mathbb{E}^{\pi_\theta}[g(s_{h+1},a_{h+1})\vert s_h,a_h]^2\vert s_1\sim\xi]
\end{align*}
The LHS satisfies
\begin{align*}
    \mathbb{E}^{\pi_\theta}[g^2(s_{h+1},a_{h+1})\vert s_1\sim\xi]&=\mu^\top\mathbb{E}^{\pi_\theta}\left[\left(\nabla_\theta^j\log\pi_{\theta,h+1}(a\vert s)\right)^2\phi(s_{h+1}, a_{h+1})\phi(s_{h+1},a_{h+1})^\top\vert s_1\sim\xi\right]\mu\\
    &\leq G^2\mu^\top\mathbb{E}^{\pi_\theta}\left[\phi(s_{h+1},a_{h+1})\phi(s_{h+1},a_{h+1})^\top\vert s_1\sim\xi\right]\mu=G^2\mu^\top\Sigma_{\theta,h+1}\mu
\end{align*}
and the RHS satisfies 
\begin{align*}
    &\mathbb{E}^{\pi_\theta}[\mathbb{E}^{\pi_\theta}[g(s_{h+1},a_{h+1})\vert s_h, a_h]^2\vert s_1\sim\xi]\\
    =&\mathbb{E}^{\pi_\theta}[\mu^\top\left(\nabla_\theta^j M_{\theta,h}\right)^\top \phi(s_h,a_h)\phi(s_h,a_h)^\top\left(\nabla_\theta^j M_{\theta,h}\right)\mu\vert s_1\sim\xi]\\
    =&\mu^\top\left(\nabla_\theta^j M_{\theta,h}\right)^\top\Sigma_{\theta,h}\left(\nabla_\theta^j M_{\theta,h}\right)\mu.
\end{align*}
Therefore, we get $G^2\mu^\top\Sigma_{\theta,h+1}\mu\geq\mu^\top\left(\nabla_\theta^j M_{\theta,h}\right)^\top\Sigma_{\theta,h}\left(\nabla_\theta^j M_{\theta,h}\right)\mu,\ \forall\mu$, which implies $\left\Vert \Sigma_{\theta,h}^{\frac{1}{2}}\left(\nabla_\theta^j M_\theta\right)\Sigma_{\theta,h+1}^{-\frac{1}{2}}\right\Vert \leq G$.
\end{proof}

\begin{lemma}
\label{dsig1}
We have with probability at least $1-\delta$,
\begin{align*}
\left\Vert\Sigma_h^{-\frac{1}{2}}\left(\frac{1}{K}\sum_{k=1}^K\phi(s_h^{(k)}, a_h^{(k)})\phi(s_h^{(k)},a_h^{(k)})^\top\right)\Sigma^{-\frac{1}{2}}_h-I_d\right\Vert\leq \sqrt{\frac{2C_1d\log\frac{2dH}{\delta}}{K}} + \frac{2C_1d\log\frac{2dH}{\delta}}{3K}.
\end{align*}
\end{lemma}
\begin{proof}
Define
\begin{align*}
	X^{(k)}_h=\Sigma^{-\frac{1}{2}}_h\phi\left(s^{(k)}_h,a^{(k)}_h\right) \phi\left(s^{(k)}_h,a^{(k)}_h\right)^\top\Sigma_h^{-\frac{1}{2}}\in\mathbb{R}^{d\times d}.
\end{align*}
It's easy to see that $X^{(1)}_h,X^{(2)}_h,\ldots,X^{(K)}_h$ are independent and $\mathbb{E}\left[X^{(k)}_h\right]=I_d$. In the remaining part of the proof, we will apply the matrix Bernstein's inequality to analyze the concentration of $\frac{1}{K}\sum_{k=1}^K X^{(k)}_h$. We first consider the matrix-valued variance $\textrm{Var}\left(X^{(k)}_h\right)=\mathbb{E}\left[\left(X^{(k)}_h-I_d\right)^2\right]=\mathbb{E}\left[\left(X^{(k)}_h\right)^2\right]-I_d$. For any vector $\mu\in\mathbb{R}^d$,
\begin{align*} 
	\mu^\top\mathbb{E}\left[\left(X_h^{(k)}\right)^2\right]\mu=&\mathbb{E}\left[\left\Vert X_h^{(k)}\mu\right\Vert^2\right]=\mathbb{E}\left[\left\Vert\Sigma^{-\frac{1}{2}}_h\phi\left(s^{(k)}_h,a^{(k)}_h\right)\phi\left(s^{(k)}_h,a^{(k)}_h\right)^\top\Sigma_h^{-\frac{1}{2}}\mu\right\Vert^2\right]\\
	\leq&\mathbb{E}\left[\left\Vert\Sigma_h^{-\frac{1}{2}}\phi\left(s^{(k)}_h,a^{(k)}_h\right)\right\Vert^2\left\Vert\phi\left(s^{(k)}_h,a^{(k)}_h\right)^\top\Sigma_h^{-\frac{1}{2}}\mu\right\Vert^2\right]\leq C_1d\mathbb{E}\left[\left\Vert\phi\left(s^{(k)}_h,a^{(k)}_h\right)^\top\Sigma_h^{-\frac{1}{2}}\mu\right\Vert^2\right]\\
    =&C_1d\mu^\top\mathbb{E}\left[X_h^{(k)}\right]\mu = C_1d \Vert\mu\Vert^2,
\end{align*}
where we used the identity $\left\Vert\phi\left(s^{(k)}_h,a^{(k)}_h\right)^\top\Sigma_h^{-\frac{1}{2}}\mu\right\Vert^2=\mu^\top X_h^{(k)}\mu$ and $\mathbb{E}\left[X_h^{(k)}\right]=I_d$. We have
\begin{align*}
    \textrm{Var}(X_h^{(k)})\preceq\mathbb{E}\left[\left(X_h^{(k)}\right)^2\right]\preceq C_1dI_d. 
\end{align*}
Additionally,
\begin{align*}
	-I_d\preceq X_h^{(k)}-I_d=\Sigma_h^{-\frac{1}{2}}\phi\left(s^{(k)}_h,a^{(k)}_h\right)\phi\left(s^{(k)}_h,a^{(k)}_h\right)^\top\Sigma_h^{-\frac{1}{2}}-I_d \preceq C_1dI_d-I_d. 
\end{align*}
Therefore, $\Vert X_h^{(k)}-I_d\Vert\leq C_1d$. Since $X^{(1)}_h,X^{(2)}_h,\ldots,X^{(K)}_h$ are \textit{i.i.d.}, by the matrix-form Bernstein inequality, we have
\begin{align*}
	\mathbb{P}\left(\left\Vert\sum_{k=1}^K X_h^{(k)}-I_d\right\Vert\geq\varepsilon\right)\leq 2d\cdot\exp\left(-\frac{\varepsilon^2/2}{C_1dK+C_1d \varepsilon/3}\right),\quad \forall \varepsilon>0. 
\end{align*}
With probability at least $1 - \delta$,
\begin{align*}
    \left\Vert\frac{1}{K}\sum_{k=1}^K\left(X_h^{(k)}-I_d\right)\right\Vert\leq \sqrt{\frac{2C_1d\log\frac{2d}{\delta}}{K}}+\frac{2C_1d\log\frac{2d}{\delta}}{3K}, 
\end{align*}
Taking a union bound over $h\in[H]$, we derive the desired result.	
\end{proof}
Let $\Delta\Sigma_h^{-1}=\widehat{\Sigma}_h^{-1}-\Sigma_h$, 
\begin{lemma}
\label{dsig2}
If $\left\Vert\Sigma_h^{-\frac{1}{2}}\widehat{\Sigma}_h\Sigma_h^{-\frac{1}{2}}-I_d\right\Vert\leq\frac{1}{2}$, then 
$\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert\leq 2\left\Vert\Sigma_h^{-\frac{1}{2}}\widehat{\Sigma}_h\Sigma_h^{-\frac{1}{2}}-I_d\right\Vert$. 
\end{lemma}
\begin{proof}
Note that
\begin{align}
    \label{2_1}
    \left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert=\left\Vert\Sigma_h^{\frac{1}{2}}\left(\widehat{\Sigma}_h^{-1}-\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert\leq\left\Vert\Sigma_h^{\frac{1}{2}}\widehat{\Sigma}_h^{-1}\Sigma_h^{\frac{1}{2}}\right\Vert\left\Vert\Sigma_h^{-\frac{1}{2}}\widehat{\Sigma}_h\Sigma_h^{-\frac{1}{2}}-I_d\right\Vert.  
\end{align}
Because we have $\left\Vert\Sigma_h^{-\frac{1}{2}}\widehat{\Sigma}_h\Sigma_h^{-\frac{1}{2}}-I_d\right\Vert\leq\frac{1}{2}$, we get $\sigma_{\textrm{min}}\left(\Sigma_h^{-\frac{1}{2}}\widehat{\Sigma}_h\Sigma_h^{-\frac{1}{2}}\right)\geq\frac{1}{2}$, which implies $\left\Vert\Sigma_h^{\frac{1}{2}}\widehat{\Sigma}_h^{-1}\Sigma_h^{\frac{1}{2}}\right\Vert\leq 2$. Combining this result with \eqref{2_1} finishes the proof. 
\end{proof}
Let $\Delta Y_{\theta,h}=\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)^\top-\Sigma_h M_{\theta,h}$, 
\begin{lemma}
\label{dy}
With probability at least $1-\delta$, the following inequalities hold simultaneously:
\begin{align}
\label{3_1}
\left\Vert\Sigma_h^{-\frac{1}{2}}\left(\Delta Y_{\theta,h}\right)\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert&\leq\left(\kappa_2\vee 1\right)\sqrt{\frac{2C_1d\log\frac{4dH}{\delta}}{K}} + \frac{4C_1d\log\frac{4dH}{\delta}}{3K},\\
\label{3_2}
\left\Vert\Sigma_h^{-\frac{1}{2}}\left(\nabla_\theta^j\left(\Delta Y_{\theta,h}\right)\right)\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert&\leq\left(\kappa_3\vee 1\right)G\sqrt{\frac{2C_1d\log\frac{4mdH}{\delta}}{K}}+\frac{4C_1dG\log\frac{4mdH}{\delta}}{3K},\quad\forall j\in[m],
\end{align}
where $\kappa_2, \kappa_3$ are defined in Theorem \ref{thm2_var}. 
\end{lemma}
\begin{proof}
Take
\begin{align*}
	Y_{\theta,h}^{(k)}:=\Sigma_h^{-\frac{1}{2}}\phi\left(s^{(k)}_h,a^{(k)}_h\right)\phi_{\theta,h+1}\left(s^{(k)}_{h+1}\right)^\top\Sigma_{h+1}^{-\frac{1}{2}},\quad\forall k\in[K]. 
\end{align*}
Then, $\Sigma_h^{-\frac{1}{2}}\left(\Delta Y_{\theta,h}\right)\Sigma_{h+1}^{-\frac{1}{2}} =\frac{1}{K}\sum_{k=1}^K\left(Y_{\theta,h}^{(k)}-\Sigma_h^{\frac{1}{2}}M_{\theta,h} \Sigma^{-\frac{1}{2}}_{h+1}\right)$. Note that
\begin{align}
    \label{SMS0} 
    \begin{aligned} 
        \mathbb{E}\left[Y_{\theta,h}^{(k)}\right]=&\mathbb{E}\left[\Sigma_h^{-\frac{1}{2}}\phi\left(s^{(k)}_h,a^{(k)}_h\right)\phi_{\theta,h+1}\left(s^{(k)}_{h+1}\right)^\top\Sigma_{h+1}^{-\frac{1}{2}}\right]\\
        =&\mathbb{E}\left[\Sigma_h^{-\frac{1}{2}} \phi\left(s^{(k)}_h,a^{(k)}_h\right)\mathbb{E}^{\pi_\theta}\left[\phi\left(s^\prime, a^\prime\right)^\top \vert s^{(k)}_h,a^{(k)}_h, h\right]\Sigma_{h+1}^{-\frac{1}{2}}\right]\\
        =&\mathbb{E}\left[\Sigma_h^{-\frac{1}{2}}\phi\left(s^{(k)}_h,a^{(k)}_h\right)\phi\left(s^{(k)}_h,a^{(k)}_h\right)^\top M_{\theta,h}\Sigma_{h+1}^{-\frac{1}{2}}\right]=\Sigma_h^{\frac{1}{2}}M_{\theta,h}\Sigma_{h+1}^{-\frac{1}{2}}, 
    \end{aligned}
\end{align}
To this end, $\Sigma_h^{-\frac{1}{2}}\left(\Delta Y_{\theta,h}\right)\Sigma_{h+1}^{-\frac{1}{2}}=\frac{1}{K}\sum_{k=1}^K\left( Y_{\theta,h}^{(k)}-\mathbb{E}\left[Y_{\theta,h}^{(k)}\right]\right)$. Since the trajectories are \textrm{i.i.d.}, we use the matrix-form Bernstein inequality to estimate $\left\Vert\Sigma_h^{-\frac{1}{2}}(\Delta Y_{\theta,h})\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert$. For any $\mu\in \mathbb{R}^d$, we have
\begin{align*}
    \mu^\top\mathbb{E}\left[Y_{\theta,h}^{(k)}\left(Y_{\theta,h}^{(k)}\right)^\top\right]\mu=&\mathbb{E}\left[\left\Vert\left(Y_{\theta,h}^{(k)}\right)^\top\mu\right\Vert^2\right]=\mathbb{E}\left[\left\Vert\Sigma_{h+1}^{-\frac{1}{2}}\phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma_h^{-\frac{1}{2}}\mu\right\Vert^2\right]\\
    \leq&\mathbb{E}\left[\left\Vert\Sigma_{h+1}^{-\frac{1}{2}} \phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)\right\Vert^2\left\Vert\phi\left(s_h^{(k)}, a_h^{(k)}\right)^\top\Sigma_h^{-\frac{1}{2}}\mu\right\Vert^2\right]. 
\end{align*}		
Parallel to the proof of Lemma \ref{dsig1}, it holds that $\left\Vert \Sigma_{h+1}^{-\frac{1}{2}}\phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)\right\Vert^2\leq C_1d$. Therefore,
\begin{align*}
    \mu^\top\mathbb{E}\left[Y_{\theta,h}^{(k)}\left(Y_{\theta,h}^{(k)}\right)^\top\right]\mu&\leq \mathbb{E}\left[C_1d\left\Vert\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma_h^{-\frac{1}{2}}\mu\right\Vert^2\right]\\
    &=C_1d\mu^\top\Sigma_h^{-\frac{1}{2}}\mathbb{E}\left[\phi\left(s_h^{(k)},a_h^{(k)}\right)\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\right]\Sigma_h^{-\frac{1}{2}}\mu\\
    &=C_1d\Vert\mu\Vert^2,
\end{align*}
where we have used the fact $\Sigma_h=\mathbb{E}\left[\phi\left(s_h^{(k)},a_h^{(k)}\right) \phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\right]$. It follows that
\begin{align*} 
    \textrm{Var}_1\left(Y_{\theta,h}^{(k)}\right):=&\mathbb{E}\left[\left(Y_{\theta,h}^{(k)}-\mathbb{E}\left[Y_{\theta,h}^{(k)}\right]\right)\left(Y_{\theta,h}^{(k)}-\mathbb{E}\left[Y_{\theta,h}^{(k)}\right]\right)^\top\right]\preceq\mathbb{E}\left[Y_{\theta,h}^{(k)}\left(Y_{\theta,h}^{(k)}\right)^\top\right]\preceq C_1dI_d. 
\end{align*} 
Analogously,
\begin{align*} 
	\textrm{Var}_2\left(Y_{\theta,h}^{(k)}\right):=&\mathbb{E}\left[\left(Y_{\theta,h}^{(k)}-\mathbb{E}\left[Y_{\theta,h}^{(k)}\right]\right)^\top\left(Y_{\theta,h}^{(k)}-\mathbb{E}\left[Y_{\theta,h}^{(k)}\right]\right)\right]\preceq\mathbb{E}\left[\left(Y_{\theta,h}^{(k)}\right)^\top Y_{\theta,h}^{(k)}\right]\\
	\preceq& C_1d\Sigma_{h+1}^{-\frac{1}{2}}\mathbb{E}\left[\phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)\phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)^\top\right]\Sigma_{h+1}^{-\frac{1}{2}}. 
\end{align*}
Therefore, $\max\left\{\left\Vert\textrm{Var}_1\left(Y_{\theta,h}^{(k)}\right)\right\Vert, \left\Vert\textrm{Var}_2\left(Y_{\theta,h}^{(k)}\right)\right\Vert\right\}\leq C_1d\left(\kappa_2^2\vee 1\right)$. It also holds that $\Vert Y_{\theta,h}^{(k)}\Vert\leq C_1d$. Hence,
\begin{align*}
    \left\Vert Y_{\theta,h}^{(k)}-\Sigma_h^{\frac{1}{2}}M_{\theta,h}\Sigma_{h+1}^{-\frac{1}{2}} \right\Vert\leq 2C_1d. 
\end{align*}
Applying Matrix Bernstein's inequality, we derive for any $\varepsilon>0$,
\begin{align*}
    \mathbb{P}\left(\left\Vert\sum_{k=1}^K\left(Y_{\theta,h}^{(k)}-\Sigma_h^{\frac{1}{2}}M_{\theta,h}\Sigma_{h+1}^{-\frac{1}{2}}\right)\right\Vert>\varepsilon\right)\leq 2d\exp\left(-\frac{\varepsilon^2/2}{C_1dK\left(\kappa_2^2\vee 1\right)+2C_1d\varepsilon/3}\right), 
\end{align*}
which implies \eqref{3_1} holds with probability $1-\frac{\delta}{2}$. For \eqref{3_2}, notice that for any $j\in[m]$, we have $\Sigma_h^{-\frac{1}{2}}(\nabla_\theta^j(\Delta Y_{\theta,h}))\Sigma_{h+1}^{-\frac{1}{2}}=\frac{1}{K}\sum_{k=1}^K\left(\nabla_\theta^j Y_{\theta,h}^{(k)}-\mathbb{E}\left[\nabla_\theta^j Y_{\theta,h}^{(k)}\right]\right)$, and $\nabla_\theta^j Y_{\theta,h}^{(k)}=\Sigma_h^{-\frac{1}{2}}\phi\left(s_h^{(k)},a_h^{(k)}\right) \left(\nabla_\theta^j\phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)\right)^\top\Sigma_{h+1}^{-\frac{1}{2}}$. For any $\mu\in \mathbb{R}^d$, we have
\begin{align*}
    \mu^\top\mathbb{E}\left[\left(\nabla_\theta^j Y_{\theta,h}^{(k)}\right) \left(\nabla_\theta^j Y_{\theta,h}^{(k)}\right)^\top\right]\mu=&\mathbb{E}\left[\left\Vert\Sigma_{h+1}^{-\frac{1}{2}}\left(\nabla_\theta^j\phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)\right)\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma_h^{-\frac{1}{2}}\mu\right\Vert^2\right]\\
    \leq&\mathbb{E}\left[\left\Vert\Sigma_{h+1}^{-\frac{1}{2}}\nabla_\theta^j\phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)\right\Vert^2\left\Vert\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma_h^{-\frac{1}{2}}\mu\right\Vert^2\right]. 
\end{align*}
Since we have 
\begin{align*}
    &\left(\nabla_\theta^j\phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)\right)^\top\Sigma_{h+1}^{-1}\nabla_\theta^j\phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)\\
    =&\int_{\mathcal{A}\times\mathcal{A}}\pi_{\theta,h+1}\left(a\left\vert s_{h+1}^{(k)}\right.\right)\pi_{\theta,h+1}\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\left(\nabla_\theta^j\log\pi_{\theta,h+1}\left(a\left\vert s_{h+1}^{(k)}\right.\right)\right)\left(\nabla_\theta^j\log\pi_{\theta,h+1}\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\right)\\
    &\cdot\phi\left(s_{h+1}^{(k)},a\right)^\top\Sigma_{h+1}^{-1}\phi\left(s_{h+1}^{(k)}, a^\prime\right)\mathrm{d}a\mathrm{d}a^\prime\\
    \leq&G^2\int_{\mathcal{A}\times\mathcal{A}}\pi_{\theta,h+1}\left(a\vert s_{h+1}^{(k)}\right)\pi_{\theta,h+1}\left(a^\prime\vert s_{h+1}^{(k)}\right)\left\Vert \Sigma_{h+1}^{-\frac{1}{2}}\phi\left(s_{h+1}^{(k)},a\right)\right\Vert\left\Vert\Sigma_{h+1}^{-\frac{1}{2}}\phi\left(s_{h+1}^{(k)},a^\prime\right)\right\Vert\mathrm{d}a\mathrm{d}a^\prime\leq G^2C_1d,
\end{align*}
which implies 
\begin{align*}
    \mu^\top\mathbb{E}\left[\left(\nabla_\theta^j Y_{\theta,h}^{(k)}\right)\left(\nabla_\theta^j Y_{\theta,h}^{(k)}\right)^\top \right]\mu\leq G^2C_1d\mathbb{E}\left[\left\Vert\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma_h^{-\frac{1}{2}}\mu\right\Vert^2\right]=G^2C_1d\Vert\mu\Vert^2. 
\end{align*}
Therefore, 
\begin{align*}
    \textrm{Var}_1\left(\nabla_\theta^j Y_{\theta,h}^{(k)}\right):=&\mathbb{E}\left[\left(\nabla_\theta^j Y_{\theta,h}^{(k)}-\mathbb{E}\left[\nabla_\theta^j Y_{\theta,h}^{(k)}\right]\right)\left(\nabla_\theta^j Y_{\theta,h}^{(k)}-\mathbb{E}\left[\nabla_\theta^j Y_{\theta,h}^{(k)}\right]\right)^\top\right]\preceq\mathbb{E}\left[\left(\nabla_\theta^j Y_{\theta,h}^{(k)}\right)\left(\nabla_\theta^j Y_{\theta,h}^{(k)}\right)^\top\right]\preceq G^2C_1dI_d. 
\end{align*}
Meanwhile, we have
\begin{align*} 
	\textrm{Var}_2\left(\nabla_\theta^j Y_{\theta,h}^{(k)}\right)\preceq\mathbb{E}\left[\left(\nabla_\theta^j Y_{\theta,h}^{(k)}\right)^\top\nabla_\theta^j Y_{\theta,h}^{(k)}\right]\preceq C_1d\Sigma_h^{-\frac{1}{2}}\mathbb{E}\left[\left(\nabla_\theta^j \phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)\right)\left(\nabla_\theta^j \phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)\right)^\top\right]\Sigma_h^{-\frac{1}{2}}. 
\end{align*}
In conclusion, we get
\begin{align*}
    \max\left\{\left\Vert\textrm{Var}_1\left(\nabla_\theta^j Y_{\theta,h}^{(k)}\right)\right\Vert,\left\Vert\textrm{Var}_2\left(\nabla_\theta^j Y_{\theta,h}^{(k)}\right)\right\Vert\right\}\leq G^2C_1d\left(\kappa_3^2\vee 1\right),
\end{align*}
Note that $\left\Vert\nabla_\theta^j Y_{\theta,h}^{(k)}\right\Vert\leq C_1dG$, we know $\left\Vert\nabla_\theta^j Y_{\theta,h}^{(k)}-\mathbb{E}\left[\nabla_\theta^j Y_{\theta,h}^{(k)}\right]\right\Vert\leq 2C_1dG$. By Matrix Bernstein's inequality, we get for any $\varepsilon>0$,
\begin{align*}
    \mathbb{P}\left(\left\Vert\sum_{k=1}^K\left(\nabla_\theta^j Y_{k,h}^\theta-\Sigma_h^{\frac{1}{2}}\left(\nabla_\theta^j M_{\theta,h}\right) \Sigma_{h+1}^{-\frac{1}{2}}\right)\right\Vert\geq\varepsilon\right)\leq 2d \exp\left(-\frac{\varepsilon^2/2}{G^2C_1dK\left(\kappa_3^2\vee 1\right)+2C_1dG \varepsilon/3}\right), 
\end{align*}
taking a union bound over all $j\in[m]$ and $h\in[H]$ proves that \eqref{3_2} holds with probability $1-\frac{\delta}{2}$. Using a union bound argument again, we know with probability $1-\delta$, \eqref{3_1} and \eqref{3_2} hold simultaneously, which has finished the proof.
\end{proof}
\begin{lemma}
\label{eps}
For $h\in[H]$, with probability at least $1-\delta$, the following inequalities hold simultaneously:
\begin{align}
    \label{w1}
    \left\Vert\Sigma_h^{-\frac{1}{2}}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right\Vert&\leq\sqrt{d}(H-h+1)\left(\sqrt{\frac{2\log\frac{8dH}{\delta}}{K}}+\frac{2\sqrt{C_1d}\log\frac{8dH}{\delta}}{K}+\frac{2C_1d\left(\log\frac{8dH}{\delta}\right)^{\frac{3}{2}}}{3K^{\frac{3}{2}}}\right)\\
    \label{w2}
    \left\Vert\Sigma_h^{-\frac{1}{2}}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta\right\Vert&\leq 2\sqrt{d}G(H-h)^2\left(\sqrt{\frac{2\log\frac{8mdH}{\delta}}{K}}+\frac{2\sqrt{C_1d}\log\frac{8mdH}{\delta}}{K}+\frac{2C_1d\left(\log\frac{8mdH}{\delta}\right)^{\frac{3}{2}}}{3K^{\frac{3}{2}}}\right),\quad\forall j\in[m].
\end{align}
\end{lemma}
\begin{proof}
Let $X_{\theta,h}^{(k)}:=\Sigma_h^{-\frac{1}{2}}\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\in\mathbb{R}^d$ and let $\mathcal{F}_{h,k}$ be $\sigma$-algebra generated by the history up to step $h$ at episode $k$, we have $\mathbb{E}\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]=0$. We apply matrix-form Freedman's inequality to analyze the concentration property. Consider conditional variances $\textrm{Var}_1\left[\left. X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]:=\mathbb{E}\left[\left.X_{\theta,h}^{(k)} \left(X_{\theta,h}^{(k)}\right)^\top\right\vert\mathcal{F}_{h,k}\right]\in\mathbb{R}^{d\times d}$ and $\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]:=\mathbb{E} \left[\left(X_{\theta,h}^{(k)}\right)^\top X_{\theta,h}^{(k)}\vert\mathcal{F}_{h,k}\right]\in\mathbb{R}$. It holds that
\begin{align*} 
    \left\Vert\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert=&\left\Vert\mathbb{E}\left[\left.X_{\theta,h}^{(k)}\left(X_{\theta,h}^{(k)}\right)^\top\right\vert\mathcal{F}_{h,k}\right]\right\Vert\leq\mathbb{E}\left[\left.\left\Vert X_{\theta,h}^{(k)}\left(X_{\theta,h}^{(k)}\right)^\top\right\Vert\right\vert\mathcal{F}_{h,k}\right]\\
    =&\mathbb{E}\left[\left.\left\Vert X_{\theta,h}^{(k)}\right\Vert^2\right\vert\mathcal{F}_{h,k}\right]= \textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right] 
\end{align*}
and
\begin{align*} 
    \textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]=&\mathbb{E}\left[\left.\left\Vert X_{\theta,h}^{(k)}\right\Vert^2\right\vert\mathcal{F}_{h,k}\right]=\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma_h^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right)\textrm{Var}\left[\left.\varepsilon_{h,k}^\theta\right\vert s_h^{(k)},a_h^{(k)},h\right]\\
    \leq&(H-h+1)^2\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma_h^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right), 
\end{align*}
where we have used $\textrm{Var}\left[\left.\varepsilon_{h,k}^\theta\right\vert s_h^{(k)},a_h^{(k)},h \right]\leq(H-h+1)^2$. Note that
\begin{align*} 
    \sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma_h^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right)=&Kd+K\textrm{Tr}\left(\Sigma_h^{-\frac{1}{2}}\left(\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\right)\Sigma_h^{-\frac{1}{2}}-I_d\right)\\
    \leq&Kd+Kd\left\Vert\Sigma_h^{-\frac{1}{2}}\left(\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\right)\Sigma_h^{-\frac{1}{2}}-I_d\right\Vert. 
\end{align*}
We take
\begin{align} 
    \label{sigma2} 
    \sigma^2:=Kd(H-h+1)^2\left(1+\sqrt{\frac{2C_1d\log\frac{8dH}{\delta}}{K}}+\frac{2C_1d\log\frac{8dH}{\delta}}{3K}\right). 
\end{align}
According to Lemma \ref{dsig1}, it holds that
\begin{align}
    \label{Var2} 
    \mathbb{P}\left(\left\Vert\sum_{k=1}^K\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert\leq\sum_{k=1}^K\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\leq\sigma^2\right)\geq 1-\frac{\delta}{4}. 
\end{align}
Additionally, we have $\left\Vert X_{\theta,h}^{(k)}\right\Vert\leq(H-h+1)\sqrt{C_1d}$. The Freedman's inequality therefore implies that for any $\varepsilon>0$,
\begin{align} 
    \label{Freedman2} 
    \begin{aligned}
        &\mathbb{P}\left(\left\vert\sum_{k=1}^K X_{\theta,h}^{(k)}\right\vert\geq \varepsilon,\ \max\left\{\left\Vert\sum_{k=1}^K\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert,\sum_{k=1}^K\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\}\leq\sigma^2\right)\leq 2d\exp\left(-\frac{\varepsilon^2/2}{\sigma^2+(H-h+1)\sqrt{C_1d}\varepsilon/3} \right),
    \end{aligned} 
\end{align}
where $\sigma^2$ is defined in \eqref{sigma2}. We take
\begin{align*}
    \varepsilon:=\sigma\sqrt{2\log\frac{8d}{\delta}}+\frac{2(H-h+1)\sqrt{C_1d}}{3}\log\frac{8d}{\delta}.
\end{align*}
Then we get
\begin{align*}
    \mathbb{P}\left(\left\vert\sum_{k=1}^K X_{\theta,h}^{(k)}\right\vert\geq\varepsilon,\ \max\left\{\left\Vert\sum_{k=1}^K\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert,\sum_{k=1}^K\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\}\leq\sigma^2\right)\leq\frac{\delta}{4},
\end{align*}
which implies
\begin{align*}
    \mathbb{P}\left(\left\vert\sum_{k=1}^K X_{\theta,h}^{(k)}\right\vert\geq \varepsilon\right)\leq&\mathbb{P}\left(\left\vert\sum_{k=1}^K X_{\theta,h}^{(k)}\right\vert\geq \varepsilon,\ \max\left\{\left\Vert\sum_{k=1}^K\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert,\sum_{k=1}^K\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\}\leq\sigma^2\right)\\
    &+\mathbb{P}\left(\max\left\{\left\Vert\sum_{k=1}^K\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert,\sum_{k=1}^K\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\}>\sigma^2\right)\leq\frac{\delta}{2}.
\end{align*}
which, combined with a union bound over $h\in[H]$, has proved \eqref{w1}. We use Freedman's inequality again to prove \eqref{w2}. For a fixed $j\in[m]$, we have
\begin{align*} 
    \left\Vert\textrm{Var}_1\left[\left.\nabla_\theta^j X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert=&\left\Vert\mathbb{E}\left[\left.\left(\nabla_\theta^j X_{\theta,h}^{(k)}\right)\left(\nabla_\theta^j X_{\theta,h}^{(k)}\right)^\top\right\vert\mathcal{F}_{h,k}\right]\right\Vert\leq\mathbb{E}\left[\left.\left\Vert\left(\nabla_\theta^j X_{\theta,h}^{(k)}\right)\left(\nabla_\theta^j X_{\theta,h}^{(k)}\right)^\top\right\Vert\right\vert\mathcal{F}_{h,k}\right]\\
    =&\mathbb{E}\left[\left.\left\Vert\nabla_\theta^j X_{\theta,h}^{(k)}\right\Vert^2\right\vert\mathcal{F}_{h,k}\right]= \textrm{Var}_2\left[\left.\nabla_\theta^j X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right] 
\end{align*}
and
\begin{align*} 
    \textrm{Var}_2\left[\left.\nabla_\theta^j X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]=&\mathbb{E}\left[\left.\left\Vert\nabla_\theta^j X_{\theta,h}^{(k)}\right\Vert^2\right\vert\mathcal{F}_{h,k}\right]=\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma_h^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right)\textrm{Var}\left[\left.\nabla_\theta^j\varepsilon_{h,k}^\theta\right\vert s_h^{(k)},a_h^{(k)}\right]\\
    \leq&4G^2(H-h)^2\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma_h^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right),  
\end{align*}
where we have used $\textrm{Var}\left[\left.\nabla_\theta^j\varepsilon_{h,k}^\theta\right\vert s_h^{(k)}, a_h^{(k)},h \right]\leq 4G^2(H-h)^2$. Furthermore, notice that $\left\Vert\nabla_\theta^j X_{\theta,h}^{(k)}\right\Vert\leq 2G(H-h)\sqrt{C_1d}$, the remaining steps will be exactly the same as those in the proof of \eqref{w1}, combined with a union bound over $j\in[m]$. In this way, we have proved \eqref{w2}. Taking a union bound again finishes the proof. 
\end{proof}
%\begin{lemma}
%\label{M_concentration}
%Define $\xi_{k, h}^\theta=\phi_\theta\left(s_{k, h+1}\right)^\top-\phi\left(s_{k, h}, a_{k, h}\right)^\top M_\theta$. Let $\mathcal{D}^*$ be the bootstrapped data and define
%\begin{align*}
%    \widehat{M^*}=\left(\widehat{\Sigma^*}\right)^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_{k,h}^*,a_{k,h}^*\right)\phi_\theta(s_{k,h+1}^*)^\top,\quad\widehat{\nabla_\theta M^*}=\left(\widehat{\Sigma^*}\right)^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_{k,h}^*,a_{k,h}^*\right)\nabla_\theta\phi_\theta(s_{k,h+1}^*)^\top
%\end{align*}
%where $\widehat{\Sigma}^*=\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^{H}\phi(s_{k,h}^*,a_{k,h}^*)\phi(s_{k,h}^*,a_{k,h}^*)^\top\in\mathbb{R}^{d\times d}$. We have
%\begin{align}
%    \label{4-1}
%    \sqrt{N}\mathrm{vec}\left(\widehat{M}_\theta-M_\theta,\widehat{\nabla_\theta M_\theta}-\nabla_\theta M_\theta\right)\stackrel{d}{\rightarrow} N\left(0,\tilde{\Delta}\right),\\
%    \label{4-2}
%    \sqrt{N}\mathrm{vec}\left(\widehat{M}^*_\theta-\widehat{M}_\theta,\widehat{\nabla_\theta M^*_\theta}-\widehat{\nabla_\theta M_\theta}\right)\stackrel{d}{\rightarrow}N\left(0,\tilde{\Delta}\right). 
%\end{align}
%where $\tilde{\Delta}$ is defined by
%\begin{align*}
%    \tilde{\Delta}\left(\left(M_\theta\right)_{ij}, \left(M_\theta\right)_{kl}\right) &= \mathbb{E}\left[ \left(\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_{1,h}, a_{1,h})\xi_{1, h}^\theta\right)_{ij} \left(\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_{1,h}, a_{1,h})\xi_{1,h}^\theta\right)_{kl}\right]\\
%    \tilde{\Delta}\left(\left(\nabla_\theta^p M_\theta\right)_{ij},\left(M_\theta\right)_{kl}\right) &= \mathbb{E}\left[ \left(\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_{1,h}, a_{1,h})\nabla_\theta^p\xi_{1, h}^\theta\right)_{ij} \left(\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_{1,h},a_{1,h})\xi_{1,h}^\theta\right)_{kl}\right]\\
%    \tilde{\Delta}\left(\left(\nabla_\theta^p M_\theta\right)_{ij}, \left(\nabla_\theta^q M_\theta\right)_{kl}\right) &= \mathbb{E}\left[ \left(\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_{1,h}, a_{1,h})\nabla_\theta^p\xi_{1,h}^\theta\right)_{ij} \left(\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_{1,h}, a_{1,h})\nabla_\theta^q\xi_{1,h}^\theta\right)_{kl}\right]
%\end{align*}
%\end{lemma}
%\begin{proof}
%In order to simplify the derivation, we assume $\lambda=0$ and the empirical covariance matrix $\sum_{n=1}^N \phi(s_n,a_n)\phi(s_n,a_n)^\top$ is invertible in this proof since the effect of $\lambda$ is asymptotically negligible. We have
%\begin{align*}
%    &\sqrt{N}\textrm{vec}\left(\widehat{M_\theta}-M_\theta,\widehat{\nabla_\theta M_\theta}-\nabla_\theta M_\theta\right)\\
%    =&\textrm{vec}\left(\sqrt{N}\widehat{\Sigma}^{-1}\frac{1}{N}\sum_{n=1}^N\phi(s_n,a_n)\left(\phi_\theta(s_n^\prime)^\top - \phi(s_n,a_n)^\top M_\theta\right), \sqrt{N}\widehat{\Sigma}^{-1}\frac{1}{N}\sum_{n=1}^N\phi(s_n,a_n)\left(\nabla_\theta\phi_\theta(s_n^\prime)^\top - \phi(s_n,a_n)^\top \nabla_\theta M_\theta\right)\right)\\
%    =&\textrm{vec}\left(\widehat{\Sigma}^{-1}\frac{1}{\sqrt{K}}\sum_{k=1}^K\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi(s_{k,h},a_{k,h})\xi_{k,h}^\theta,\widehat{\Sigma}^{-1}\frac{1}{\sqrt{K}}\sum_{k=1}^K\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi(s_{k,h},a_{k,h})\nabla_\theta\xi_{k,h}^\theta\right)\\
%    =&\left(I_{(m+1)d}\otimes\widehat{\Sigma}^{-1}\right)\frac{1}{\sqrt{K}}\sum_{k=1}^K\textrm{vec}\left(\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi(s_{k,h},a_{k,h})\xi_{k,h}^\theta,\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi(s_{k,h},a_{k,h})\nabla_\theta\xi_{k,h}^\theta\right)
%\end{align*}
%Note that we have
%\begin{align*}
%    &\mathbb{E}\left[\frac{1}{\sqrt{H}}\sum_{h=1}^H\xi_{k,h}^\theta\vert\mathcal{F}_k\right]=\mathbb{E}\left[\frac{1}{\sqrt{H}}\sum_{h=1}^H\left(\phi_\theta(s_{k,h}^\prime)^\top-\phi(s_{k,h},a_{k,h})^\top M_\theta\right)\vert\mathcal{F}_k\right]=0\\
%    &\mathbb{E}\left[\frac{1}{\sqrt{H}}\sum_{h=1}^H\nabla_\theta\xi_{k,h}^\theta\vert\mathcal{F}_k\right]=\mathbb{E}\left[\frac{1}{\sqrt{H}}\sum_{h=1}^H\left(\nabla_\theta\phi_\theta(s_{k,h}^\prime)^\top-\phi(s_{k,h},a_{k,h})^\top\nabla_\theta M_\theta\right)\vert\mathcal{F}_k\right]=0
%\end{align*}
%Therefore, again with martingale central limit theorem and independence between each episode, and And notice that by weak law of large number, we have $\widehat{\Sigma}\stackrel{p}{\rightarrow}\Sigma$, we get as $K\rightarrow\infty$,
%\begin{align*}
%    \left(I_{(m+1)d}\otimes\widehat{\Sigma}^{-1}\right)\frac{1}{\sqrt{K}}\sum_{k=1}^K\textrm{vec}\left(\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi(s_{k,h},a_{k,h})\xi_{k,h}^\theta,\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi(s_{k,h},a_{k,h})\nabla_\theta\xi_{k,h}^\theta\right)\stackrel{d}{\rightarrow} N\left(0, \tilde{\Delta}\right)
%\end{align*}
%which has proved \eqref{4-1}. We prove the remaining result using the Mallows metric as a central tool. The Mallows metric, relative to the Euclidean norm $\Vert\cdot\Vert$, for two probability measures $\mu,\nu$ in $\mathbb{R}^d$ is defined as
%\begin{align*}
%    \Lambda_l(\mu,\nu)=\inf_{U\sim\mu,V\sim\nu} \mathbb{E}^{1/l}(\Vert U-V\Vert^l),
%\end{align*}
%where $U$ and $V$ are two random vectors that $U$ has law $\mu$ and $V$ has law $\nu$. For random variables $U,V$, we sometimes write $\Lambda^l(U,V)$ as the $\Lambda^l$-distance between the laws of $U$ and $V$. We refer Bickel \& Freedman (1981); Freedman et al. (1981) for more details about the properties of Mallows metric. Suppose the common distribution of original K episodes $\{\mathcal{D}_1,\ldots, \mathcal{D}_K\}$ is $\mu$ and their empirical distribution is $\mu_K$. Both $\mu$ and $\mu_K$ are probability in $\mathbb{R}^{2Hd+H}$. From Lemma \ref{C1}, we know that $\Lambda^4(\mu_K,\mu)\rightarrow 0$ a.e. as $K\rightarrow\infty$.
%\begin{itemize}
%    \item \textbf{Step 1. } We prove $\widehat{\Sigma}^*$ converges in conditional probability to $\Sigma$. From the bootstrap design, $\frac{1}{H}\sum_{h=1}^H\phi_{kh}^*\left(\phi_{kh}^*\right)^\top$ is independent of $\frac{1}{H}\sum_{h=1}^H\phi_{k^\prime h}^*\left(\phi_{k^\prime h}^*\right)^\top$ for any $k\neq k^\prime$. According to Lemma \ref{C3}, we have
%    \begin{align*}
%        \Lambda_1\left(\frac{1}{K}\sum_{k=1}^K\frac{1}{H}\sum_{h=1}^H\phi_{kh}^*\left(\phi_{kh}^*\right)^\top, \frac{1}{K}\sum_{k=1}^K\frac{1}{H}\sum_{h=1}^H\phi_{kh}\phi_{kh}^\top\right)\leq& \frac{1}{K}\sum_{k=1}^K\Lambda_1\left(\frac{1}{H}\sum_{h=1}^H\phi_{kh}^*\left(\phi_{kh}^*\right)^\top, \frac{1}{H}\sum_{h=1}^H\phi_{kh}\phi_{kh}^\top\right)\\
%        =&\Lambda_1\left(\frac{1}{H}\sum_{h=1}^H\phi_{1h}^*\left(\phi_{1h}^*\right)^\top, \frac{1}{H}\sum_{h=1}^H\phi_{1h}\phi_{1h}^\top\right)
%    \end{align*}
%    Both sides of the above inequality are random variables such that the distance is computed between the conditional distribution of the starred quantity and the unconditional distribution of the unstarred quantity. Define a mapping $f:\mathbb{R}^{Hd}\rightarrow\mathbb{R}^{d\times d}$ such that for any $x_1,\dots, x_H\in\mathbb{R}^d$,
%    \begin{align*}
%        f(x_1,\dots,x_H) = \frac{1}{H}\sum_{h=1}^H x_h x_h^\top.
%    \end{align*}
%    From Lemma \ref{C2} with $f$, we have as $K$ goes to infinity
%    \begin{align*}
%        \Lambda_1\left(\frac{1}{H}\sum_{h=1}^H\phi_{1h}^*\left(\phi_{1h}^*\right)^\top, \frac{1}{H}\sum_{h=1}^H\phi_{1h}\phi_{1h}^\top\right)\rightarrow 0.
%    \end{align*}
%    which implies 
%    \begin{align*}
%        \Lambda_1\left(\frac{1}{K}\sum_{k=1}^K\frac{1}{H}\sum_{h=1}^H\phi_{kh}^*\left(\phi_{kh}^*\right)^\top, \frac{1}{K}\sum_{k=1}^K\frac{1}{H}\sum_{h=1}^H\phi_{kh}\phi_{kh}^\top\right)\rightarrow 0
%    \end{align*}
%    By the law of large numbers, 
%    \begin{align*}
%        \widehat{\Sigma}=\frac{1}{K}\sum_{k=1}^K\frac{1}{H}\sum_{h=1}^H \phi_{kh}\phi_{kh}^\top\stackrel{p}{\rightarrow}\Sigma.
%    \end{align*}
%    This further implies the conditioned on $\mathcal{D}$, we have $\widehat{\Sigma}^*\stackrel{p}{\rightarrow}\Sigma$.
%    \item\textbf{Step 2. } From Lemma \ref{C4}, 
%    \begin{align*}
%        &\Lambda_2\left(\sum_{k=1}^K\frac{1}{\sqrt{K}}\textrm{vec}\left(\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{kh}^*\xi_{k,h}^{\theta*},\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{kh}^*\nabla_\theta\xi_{k,h}^{\theta*}\right),\sum_{k=1}^K\frac{1}{\sqrt{K}}\textrm{vec}\left(\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{kh}\xi_{k,h}^\theta,\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{kh}\nabla_\theta\xi_{k,h}^\theta\right)\right)^2\\
%        \leq&\Lambda_2\left(\textrm{vec}\left(\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{1h}^*\xi_{1,h}^{\theta*},\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{1h}^*\nabla_\theta\xi_{1,h}^{\theta*}\right),\textrm{vec}\left(\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{1h}\xi_{1,h}^\theta,\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{1h}\nabla_\theta\xi_{1,h}^\theta\right)\right)^2
%    \end{align*}
%    Using Lemma \ref{C5} with $y_h=\left(\xi^\theta_{1,h},\nabla_\theta\xi^\theta_{1,h}\right)$, we have the right side converges to $0$, a.e. as $K\rightarrow\infty$, which implies
%    \begin{align*}
%        \sum_{k=1}^K\frac{1}{\sqrt{K}}\textrm{vec}\left(\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{kh}^*\xi_{k,h}^{\theta*},\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{kh}^*\nabla_\theta\xi_{k,h}^{\theta*}\right)\stackrel{d}{\rightarrow}\sum_{k=1}^K\frac{1}{\sqrt{K}}\textrm{vec}\left(\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{kh}\xi_{k,h}^\theta,\frac{1}{\sqrt{H}}\sum_{h=1}^H\phi_{kh}\nabla_\theta\xi_{k,h}^\theta\right)
%    \end{align*}
%\end{itemize}
%Combing the results of Step 1 and Step 2, we know the conditional law of $\textrm{vec}\left(\widehat{M_\theta^*}-\widehat{M_\theta},\widehat{\nabla_\theta M_\theta^*}-\widehat{\nabla_\theta M_\theta}\right)$ is close to the unconditional law of $\textrm{vec}\left(\widehat{M_\theta}-M_\theta, \widehat{\nabla_\theta M_\theta}-\nabla_\theta M_\theta\right)$ as $K\rightarrow\infty$. Combining the result of \eqref{4-1}, we have proved \eqref{4-2}.
%\end{proof}
%\begin{lemma}
%\label{dr}
%Let 
%\begin{align*}
%    \widehat{w_r^*} = \left(\widehat{\Sigma^*}\right)^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H r^*_{kh}\phi^*_{kh},\quad \widehat{w_r} = \widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H r_{kh}\phi_{kh},
%\end{align*}
%we have
%\begin{align*}
%    \sqrt{N}\left(\widehat{w_r^*}-w_r\right)\rightarrow_p 0,\quad\sqrt{N}\left(\widehat{w_r}-w_r\right)\rightarrow_p 0.
%\end{align*}
%\begin{proof}
%The result $\sqrt{N}\left(\widehat{w_r}-w_r\right)\rightarrow_p 0$ is simply due to the law of large number that $\widehat{\Sigma}\rightarrow_p\Sigma$ and 
%\begin{align*}
%    \left\Vert\sqrt{N}\left(w_r-\widehat{w}_r\right)\right\Vert&=\left\Vert\lambda \frac{1}{\sqrt{N}}\widehat{\Sigma}^{-1} w_r\right\Vert\rightarrow_p 0.
%\end{align*}
%And using Lemma \ref{C4}, we have
%\begin{align*}
%    \Lambda_2\left(\frac{1}{\sqrt{K}}\sum_{k=1}^K\frac{1}{\sqrt{H}}\sum_{h=1}^Hr_{kh}^*\phi_{kh}^*,\frac{1}{\sqrt{K}}\sum_{k=1}^K\frac{1}{\sqrt{H}}\sum_{h=1}^Hr_{kh}\phi_{kh}\right)^2\leq\Lambda_2\left(\frac{1}{\sqrt{H}}\sum_{h=1}^Hr_{kh}^*\phi_{kh}^*,\frac{1}{\sqrt{H}}\sum_{h=1}^Hr_{kh}\phi_{kh}\right)^2
%\end{align*}
%The right hand side goes to $0$ as $K\rightarrow\infty$. Therefore, we know $\sqrt{N}\left(\widehat{w_r^*}-w_r\right)\rightarrow_p 0$. 
%\end{proof}
%\end{lemma}

%\section{Results on Mallows metric}
%\begin{lemma}[Lemma 8.4 in Bickel \& Freedman (1981)]
%\label{C1}
%Let $\{X_i\}^n_{i=1}$ be independent random variables with common distribution $\mu$. Let $\mu_n$ be the empirical distribution of $X_1,\ldots,X_n$. Then $\Lambda_l(\mu_n,\mu)\rightarrow 0$ a.e..
%\end{lemma}
%\begin{lemma}[Lemma 8.5 in Bickel \& Freedman (1981)] 
%\label{C2}
%Suppose $X_n,X$ are random variables and $\Lambda_l(X_n,X)\rightarrow 0$. Let $f$ be a continuous function. Then $\Lambda_l(f(X_n),f(X))\rightarrow 0$.
%\end{lemma}
%\begin{lemma}[Lemma 8.6 of Bickel \& Freedman (1981)]
%\label{C3}
%Let $\{U_i\}^n_{i=1},\{V_i\}^n_{i=1}$ be independent random vectors. Then we have
%\begin{align*}
%    \Lambda_1\left(\sum_{i=1}^n U_i,\sum_{i=1}^n V_i\right)\leq\sum_{i=1}^n\Lambda_1\left(U_i,V_i\right).
%\end{align*}
%\end{lemma}
%\begin{lemma}[Lemma 8.7 of Bickel \& Freedman (1981)]
%\label{C4}
%Let $\{U_i\}^n_{i=1},\{V_i\}^n_{i=1}$ be independent random vectors and $\mathbb{E}[U_j]=\mathbb{E}[V_j]$. Then we have
%\begin{align*}
%    \Lambda_2\left(\sum_{i=1}^n U_i,\sum_{i=1}^n V_i\right)^2\leq\sum_{i=1}^n\Lambda_2\left(U_i,V_i\right)^2
%\end{align*}
%\end{lemma}
%Let $\mu_K$ and $\mu$ be probabilities on $\mathbb{R}^{2Hd}$. A data point in $\mathbb{R}^{2Hd}$ can be written as $(x_1,\ldots,x_H,y_1,\ldots,y_H)$ where $x_h\in\mathbb{R}^d$ and $y_h\in\mathbb{R}^d$. Denote
%\begin{align*}
%    &\Sigma(\mu)=\int\frac{1}{H}\sum_{h=1}^Hx_hx_h^\top\mu\left(\mathrm{d}x_1,\ldots,\mathrm{d}x_H,\mathrm{d}y_1,\ldots,\mathrm{d}y_H\right)\\
%    &M(\mu)=\Sigma(\mu)^{-1}\int\frac{1}{H}\sum_{h=1}^Hx_hy_h^\top\mu\left(\mathrm{d}x_1,\ldots,\mathrm{d}x_H,\mathrm{d}y_1,\ldots,\mathrm{d}y_H\right)\\
%    &\varepsilon(\mu,x_1,\ldots,x_H,y_1,\ldots,y_H)=\sum_{h=1}^H(y_h-x_h^\top M(\mu))
%\end{align*}
%\begin{lemma}[Lemma 7 in Eck (2018)]
%\label{C5}
%If $\Lambda_4(\mu_K,\mu)\rightarrow 0$ as $K\rightarrow\infty$, then we have the $\mu_K$-law of \\$\mathrm{vec}(\sum^H_{h=1}\varepsilon(\mu_K,x_1,\ldots,x_H, y_1,\ldots,y_H)x_h^\top)$ converges to the $\mu$-law of $\mathrm{vec}(\sum^H_{h=1}\varepsilon(\mu_K,x_1,\ldots,x_H,y_1,\ldots,y_H)x_h^\top)$ in $\Lambda_2$. 
%\end{lemma}

%\section{Discussion on the Error Rate of the FPG Estimator for Inhomogenous MDPs}
%\section{Extension to Time-inhomogeneous Case}
%\label{asym_rate}
%\subsection{The Time-inhomogeneous Setting}
%Our algorithm is based on a time-homogeneous MDP, which means the transition probability and reward function remain the same for different time step $h$. But one may notice that it would be straightforward to extend the algorithm to a time-inhomogeneous setting because every time-inhomogeneous MDP can be transformed into a time-homogeneous MDP after making some modifications to the state and action spaces. Suppose the transition operator for a time inhomogenous MDP is defined by
%\begin{align*}
%    \mathcal{P}_{\theta,h}f = \mathbb{E}^{\pi_\theta}[f(s^\prime,a^\prime)\vert s, a, h], \quad\forall f:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}. 
%\end{align*}
%Under this setting, we restate the time-inhomogeneous versions of our assumptions: Again we assume $\mathcal{F}^\prime$ is a class of linear functions with feature $\phi^\prime:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{d^\prime}$, and define 
%\begin{align*}
%    \Sigma_h = \mathbb{E}\left[\phi^\prime(s_h,a_h)\phi^\prime(s_h,a_h)^\top\right]. 
%\end{align*}
%\begin{assumption}
%\label{app_a1}
%For any $f\in\mathcal{F}^\prime$ and $h\in[H]$, we have $\mathcal{P}_{\theta, h} f\in\mathcal{F}$, and we suppose $r_h\in\mathcal{F}^\prime,\forall h\in[H]$. It follows that $Q^{\theta}_h\in\mathcal{F}^\prime, \forall h\in[H],\theta\in\Theta$.
%\end{assumption}
%\begin{assumption}[Boundedness Conditions]
%\label{app_a2}
%	Assume $\Sigma_h, h\in[H]$ is invertible. There exists absolute constants $C_1^\prime, G$ such that for any $(s,a)\in\mathcal{S}\times\mathcal{A},h\in[H],j\in[m]$, we have
%	\begin{align*}
%		\phi^\prime(s,a)^\top\Sigma^{-1}_h\phi^\prime(s,a)\leq C_1^\prime d^\prime,\quad\vert\nabla_\theta^j\log\pi_{h,\theta}(a\vert s)\vert\leq G.
%	\end{align*}
%\end{assumption}

%\subsection{Conversion to Time-homogeneous Setting}
%One would notice that by augmenting the original state space with the time step $h$, i.e., let $\mathcal{S}^\prime = \{s^\prime = (s,h)\vert s\in\mathcal{S},h\in[H]\}$, and define the transition operator $\mathcal{P}_\theta$ and reward $r$ by
%\begin{align*}
%    (\mathcal{P}_\theta f)((s,h),a) &:= \mathbb{E}^{\pi_\theta}[f((s^\prime,h+1), a^\prime)\vert s, a, h],\quad\forall f:\mathcal{S}^\prime\times\mathcal{A}\rightarrow\mathbb{R}, \\
%    r((s,h),a) &:= r_h(s, a)
%\end{align*}
%One would directly get a time-homogeneous MDP. Furthermore, define feature vectors $\phi:\mathcal{S}^\prime\times\mathcal{A}\rightarrow\mathbb{R}^{Hd^\prime}$ by
%\begin{align*}
%    \phi((s,h), a)^\top = [\bm{0}^\top_{d^\prime}, \bm{0}^\top_{d^\prime}, \ldots, \phi^\prime(s, a), \ldots, \bm{0}^\top_{d^\prime}]
%\end{align*}
%where the only the $h$th entry is non-zero and is set to be the value of the original feature vector. Let $\mathcal{F}$ be the class of linear functions of $\phi$, it is easy to verify that $\mathcal{F}$ is closed under the transition operator $\mathcal{P}_\theta$. Define $w_r^\top = [w_{r,1}^\top, \ldots, w_{r,H}^\top]$, one could verify the relation $r = \phi^\top w_r$, i.e., $r\in\mathcal{F}$. And we may verify that
%\begin{align*}
%    \Sigma = \frac{1}{H}\mathbb{E}\left[\sum_{h=1}^H\phi((s_h,h),a_h)\right] = \frac{1}{H}\textrm{diag}(\Sigma_1,\Sigma_2,\ldots,\Sigma_H)
%\end{align*}
%and therefore, 
%\begin{align*}
%    \phi((s,h),a)^\top \Sigma^{-1}\phi((s,h),a) \leq C_1^\prime d^\prime H.
%\end{align*}
%And we have recovered the assumptions in the main article with $d=d^\prime H,C_1=C_1^\prime$, and all of the algorithms and results in the main article apply to our setting then. 

%\subsection{A Better Convergence Rate for Time-inhomogeneous Setting}
%One special property is when the MDP is time-inhomogeneous, the covariance matrix $\Lambda_\theta$ can be further simplified, which leads to an error bound of order $O(\sqrt{\frac{d^\prime H^5}{N}})$. In this part, we also assume $\phi(s^\prime,a^\prime)^\top\Sigma^{-\frac{1}{2}}\phi(s,a)\geq 0, \forall (s,a),(s^\prime,a^\prime)\in\mathcal{S}\times\mathcal{A}$. We have
%\begin{align*}
%   \Lambda_\theta=&\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta\left(\varepsilon^\theta_{h,1}\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma_h^{-1}\nu_h^\theta\right)\right)^\top\left(\nabla_\theta\left(\varepsilon^\theta_{h,1}\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma_h^{-1}\nu_h^\theta\right)\right)\right]
%\end{align*}
%By Lemma \ref{gd_bound}, we have 
%\begin{align*}
%    \left\vert\left(\nabla^j_\theta\nu^\theta_{h}\right)^\top\Sigma_h^{-1}\phi(s,a)\right\vert\leq Gh\left(\nu^\theta_{h}\right)^\top\Sigma_h^{-1}\phi(s,a),\quad\forall s\in\mathcal{S}, a\in\mathcal{A}, h\in[H],j\in[m]
%\end{align*}
%Pick $t=e_j, j\in[m]$, we have
%\begin{align*}
%    t^\top \Lambda_\theta t =& \mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j\left(\varepsilon^\theta_{h,1}\phi(s_{h}^{(1)},a_{h}^{(1)})^\top\Sigma_h^{-1}\nu_{h}^\theta\right)\right)^2\right]\\
%    \leq&2\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j \varepsilon^\theta_{h,1}\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma_h^{-1}\nu_{h}^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^H\left(\varepsilon^\theta_{h,1}\phi(s_{h}^{(1)},a_{h}^{(1)})^\top\Sigma_h^{-1}\nabla_\theta^j\nu_{h}^\theta\right)^2\right]\\
%    =&2\mathbb{E}\left[\left(\sum_{h=1}^H\nabla_\theta^j \varepsilon^\theta_{h,1}\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma_h^{-1}\nu_{h}^\theta\right)^2\right]+2\mathbb{E}\left[\left(\sum_{h=1}^H\varepsilon^\theta_{h,1}\phi(s_{h}^{(1)},a_{h}^{(1)})^\top\Sigma_h^{-1}\nabla_\theta^j\nu_{h}^\theta\right)^2\right]\\
%    \leq&2\mathbb{E}\left[\left(\sum_{h=1}^H\frac{\sqrt{\left(\nu_h^\theta\right)^\top\Sigma_h^{-1}\nu_h^\theta}}{(H-h)^2G}\left(\nabla_\theta^j \varepsilon^\theta_{h,1}\right)^2\right)\left(\sum_{h=1}^H\frac{(H-h)^2G}{\sqrt{\left(\nu_h^\theta\right)^\top\Sigma_h^{-1}\nu_h^\theta}}\left(\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma_h^{-1}\nu_{h}^\theta\right)^2\right)\right]\\
%    &+2\mathbb{E}\left[\left(\sum_{h=1}^H\frac{\sqrt{\left(\nabla_\theta^j\nu_h^\theta\right)^\top\Sigma_h^{-1}\left(\nabla_\theta^j\nu_h^\theta\right)}}{H-h+1}\left(\varepsilon^\theta_{h,1}\right)^2\right)\left(\sum_{h=1}^H\frac{H-h+1}{\sqrt{\left(\nabla_\theta^j\nu_h^\theta\right)^\top\Sigma_h^{-1}\left(\nabla_\theta^j\nu_h^\theta\right)}}\left(\phi(s_{h}^{(1)},a_{h}^{(1)})^\top\Sigma_h^{-1}\nabla_\theta^j\nu_{h}^\theta\right)^2\right)\right]\\
%     \leq&2\mathbb{E}\left[\left(\sum_{h=1}^H(H-h)^2G\sqrt{\left(\nu_h^\theta\right)^\top\Sigma_h^{-1}\nu_h^\theta}\right)\left(\sum_{h=1}^H\frac{(H-h)^2G}{\sqrt{\left(\nu_h^\theta\right)^\top\Sigma_h^{-1}\nu_h^\theta}}\left(\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma_h^{-1}\nu_{h}^\theta\right)^2\right)\right]\\
%    &+2\mathbb{E}\left[\left(\sum_{h=1}^H(H-h+1)\sqrt{\left(\nabla_\theta^j\nu_h^\theta\right)^\top\Sigma_h^{-1}\left(\nabla_\theta^j\nu_h^\theta\right)}\right)\left(\sum_{h=1}^H\frac{H-h+1}{\sqrt{\left(\nabla_\theta^j\nu_h^\theta\right)^\top\Sigma_h^{-1}\left(\nabla_\theta^j\nu_h^\theta\right)}}\left(\phi(s_{h}^{(1)},a_{h}^{(1)})^\top\Sigma_h^{-1}\nabla_\theta^j\nu_{h}^\theta\right)^2\right)\right]\\
%     \leq&2\left(\sum_{h=1}^H(H-h)^2G\sqrt{\left(\nu_h^\theta\right)^\top\Sigma_h^{-1}\nu_h^\theta}\right)^2+2\left(\sum_{h=1}^H(H-h+1)\sqrt{\left(\nabla_\theta^j\nu_h^\theta\right)^\top\Sigma_h^{-1}\left(\nabla_\theta^j\nu_h^\theta\right)}\right)^2\\
%     \leq&2\left(\sum_{h=1}^H(H-h)^2G\sqrt{\left(\nu_h^\theta\right)^\top\Sigma_h^{-1}\nu_h^\theta}+\sum_{h=1}^H(H-h+1)\sqrt{\left(\nabla_\theta^j\nu_h^\theta\right)^\top\Sigma_h^{-1}\left(\nabla_\theta^j\nu_h^\theta\right)}\right)^2
%\end{align*}

%\begin{align*}
%    t^\top \Lambda_\theta t =& \mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j\left(\varepsilon^\theta_{h,1}\phi(s_{h}^{(1)},a_{h}^{(1)})^\top\Sigma_h^{-1}\nu_{h}^\theta\right)\right)^2\right]\\
%    \leq&2\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j \varepsilon^\theta_{h,1}\right)^2\left(\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma_h^{-1}\nu_{h}^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^H\left(\varepsilon^\theta_{h,1}\right)^2\left(\phi(s_{h}^{(1)},a_{h}^{(1)})^\top\Sigma_h^{-1}\nabla_\theta^j\nu_{h}^\theta\right)^2\right]\\
%    \leq&\frac{2}{H}\mathbb{E}\left[\sum_{h=1}^H\left(\left(\nabla_\theta \varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)\cdot t\right)^2\left(\phi(s_{h},a_{h})^\top\Sigma^{-1}\nu_{h}^\theta\right)^2\right] \\
%    &+ 2mG^2H\Vert t\Vert^2\mathbb{E}\left[\sum_{h=1}^H \left(\varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)^2\left(\phi(s_{h},a_{h})^\top\Sigma^{-1}\nu_{h}^\theta\right)^2\right]\\
%    \leq&2C_1d^\prime\mathbb{E}\left[\sum_{h=1}^H\left(\left(\nabla_\theta \varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)\cdot t\right)^2\left(\nu_h^\theta\right)^\top\Sigma^{-1}\nu_h^\theta\right] \\
%    &+ 2C_1d^\prime mG^2H^2\Vert t\Vert^2\mathbb{E}\left[\sum_{h=1}^H \left(\varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)^2\left(\nu_h^\theta\right)^\top\Sigma^{-1}\nu_h^\theta\right]
%\end{align*}
%where we use the boundedness assumption $\phi(s,a)^\top\Sigma^{-1}\phi(s,a)\leq C_1d = C_1Hd^\prime,\ \forall s\in\mathcal{S}, a\in\mathcal{A}$. In the remaining part of this section, we introduce the distribution mismatch term
%\begin{align*}
%    C_2=\mathbb{E}^{\pi_\theta}[\phi^\prime(s_h,a_h)]^\top\Sigma_h^{-1}\mathbb{E}^{\pi_\theta}[\phi^\prime(s_h,a_h)]=\frac{1}{H}\sup_{\theta\in\Theta}\left(\nu_h^\theta\right)^\top\Sigma^{-1}\nu_h^\theta
%\end{align*}
%{\color{red} Alternative definition: $C_2^\prime = \sup_{s, a, h,\theta}\phi(s,a)^\top\Sigma^{-1}\nu_h^\theta$. In this case, the bound will be sharper and the definition matches the definition in \cite{kallus2020statistically}.} 
%One has
%\begin{align*}
%    t^\top\Lambda_\theta t\leq2C_1C_2d^\prime H\mathbb{E}\left[\sum_{h=1}^H\left(\left(\nabla_\theta \varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)\cdot t\right)^2\right] + 2C_1C_2d^\prime mG^2H^3\Vert t\Vert^2\mathbb{E}\left[\sum_{h=1}^H \left(\varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)^2\right]
%\end{align*}
%For the first term, we have
%\begin{align*}
%    &\mathbb{E}\left[\sum_{h=1}^H\left(\left(\nabla_\theta \varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)\cdot t\right)^2\right]\\
%    =&\mathbb{E}\left[\sum_{h=1}^H\left(\left(\nabla_\theta Q_h^\theta(s_h, a_h) - \int_{\mathcal{A}}\pi_\theta(a\vert s_{h+1})\left(\nabla_\theta Q_{h+1}^\theta(s_{h+1}, a) + Q_{h+1}^\theta(s_{h+1}, a)\nabla_\theta\log\pi_\theta(a\vert s_{h+1})\right)\mathrm{d}a\right)\cdot t\right)^2\right]\\
%    \leq&\mathbb{E}\left[\sum_{h=1}^H\left(\left(\nabla_\theta Q_h^\theta(s_h, a_h) - \int_{\mathcal{A}}\pi_\theta(a\vert s_{h+1})\left(\nabla_\theta Q_{h+1}^\theta(s_{h+1}, a) + Q_{h+1}^\theta(s_{h+1}, a)\nabla_\theta\log\pi_\theta(a\vert s_{h+1})\right)\mathrm{d}a\right)\cdot t\right)^2\right]\\
%    &+\mathbb{E}\Bigg[\sum_{h=1}^H\Bigg(\left(\int_{\mathcal{A}}\pi_\theta(a\vert s_{h+1})\Bigg(\nabla_\theta Q_{h+1}^\theta(s_{h+1}, a) + Q_{h+1}^\theta(s_{h+1}, a)\nabla_\theta\log\pi_\theta(a\vert s_{h+1})\right)\mathrm{d}a \\
%    &- \nabla_\theta Q_{h+1}^\theta(s_{h+1}, a_{h+1}) - Q_{h+1}^\theta(s_{h+1}, a_{h+1})\nabla_\theta \log\pi_\theta(a_{h+1}\vert s_{h+1})\Bigg)\cdot t\Bigg)^2\Bigg]\\
%    =&\mathbb{E}\left[\sum_{h=1}^H (\varepsilon_h^2 + \eta_h^2)\right]
%\end{align*}
%where we define
%\begin{align*}
%    \varepsilon_h &= \left(\nabla_\theta Q_h^\theta(s_h, a_h) - \int_{\mathcal{A}}\pi_\theta(a\vert s_{h+1})\left(\nabla_\theta Q_{h+1}^\theta(s_{h+1}, a) + Q_{h+1}^\theta(s_{h+1}, a)\nabla_\theta\log\pi_\theta(a\vert s_{h+1})\right)\mathrm{d}a\right)\cdot t\\
%    \eta_h &= \left(\int_{\mathcal{A}}\pi_\theta(a\vert s_{h+1})\Bigg(\nabla_\theta Q_{h+1}^\theta(s_{h+1}, a) + Q_{h+1}^\theta(s_{h+1}, a)\nabla_\theta\log\pi_\theta(a\vert s_{h+1})\right)\mathrm{d}a \\
%    &- \nabla_\theta Q_{h+1}^\theta(s_{h+1}, a_{h+1}) - Q_{h+1}^\theta(s_{h+1}, a_{h+1})\nabla_\theta \log\pi_\theta(a_{h+1}\vert s_{h+1})\Bigg)\cdot t
%\end{align*}
%Note that the sequence $\varepsilon_1,\eta_1,\varepsilon_2,\eta_2\ldots, \varepsilon_H,\eta_H$ forms a martingale difference sequence, therefore, we have
%\begin{align*}
%    \mathbb{E}\left[\sum_{h=1}^H (\varepsilon_h^2 + \eta_h^2)\right] =& \mathbb{E}\left[\left(\sum_{h=1}^H\left(\varepsilon_h+\eta_h\right)\right)^2\right]\\
%    =&\mathbb{E}\left[\left(\left(\nabla_\theta Q_1^\theta(s_1,a_1) - \sum_{h=1}^HQ_{h+1}^\theta(s_{h+1}, a_{h+1})\nabla_\theta \log\pi_\theta(a_{h+1}\vert s_{h+1})\right)\cdot t\right)^2\right]\\
%    \leq& 2mH^4G^2\Vert t\Vert^2
%\end{align*}
%Therefore, we get
%\begin{align*}
%    2C_1C_2d^\prime H\mathbb{E}\left[\sum_{h=1}^H\left(\left(\nabla_\theta \varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)\cdot t\right)^2\right]\leq 4C_1C_2d^\prime mH^5G^2\Vert t\Vert^2
%\end{align*}
%For the second term, we have
%\begin{align*}
%    &\mathbb{E}\left[\sum_{h=1}^H \left(\varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)^2\right]=\mathbb{E}\left[\sum_{h=1}^H\left(Q_h^\theta(s_h,a_h)-r(s_h,a_h)-\int_{\mathcal{A}}\pi_\theta(a\vert s_{h+1})Q_{h+1}^\theta(s_{h+1},a)\mathrm{d}a\right)^2\right]\\
%    &+\mathbb{E}\left[\sum_{h=1}^H\left(\int_{\mathcal{A}}\pi_\theta(a\vert s_{h+1})Q_{h+1}^\theta(s_{h+1},a)\mathrm{d}a - Q_{h+1}^\theta(s_{h+1},a_{h+1})\right)^2\right]\\
%    =&\mathbb{E}\left[\left(Q_1^\theta(s_h,a_h)-\sum_{h=1}^Hr(s_h,a_h)\right)^2\right]\leq H^2
%\end{align*}
%Therefore, 
%\begin{align*}
%    2C_1C_2d^\prime mG^2H^3\Vert t\Vert^2\mathbb{E}\left[\sum_{h=1}^H \left(\varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)^2\right] \leq 2C_1C_2d^\prime mG^2H^5\Vert t\Vert^2
%\end{align*}
%In summary, we get
%\begin{align*}
%    t^\top \Lambda_\theta t \leq 6C_1C_2d^\prime mG^2H^5\Vert t\Vert^2
%\end{align*}
%Using this inequality, the result in the finite sample bound of \ref{thm2_var} is upper bounded by
%\begin{align*}
%    &\vert\langle t, \widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\rangle \vert\leq\tilde{O}\left(\sqrt{\frac{C_1C_2d^\prime mG^2H^5\Vert t\Vert^2}{N}}\right)
%\end{align*}
%When $t$ is a one-hot vector $t = e_j$, we have an improved bound of $t^\top\Lambda_\theta t$: 
%\begin{align*}
%    t^\top \Lambda_\theta t\leq&\frac{2}{H}\mathbb{E}\left[\sum_{h=1}^H\left(\left(\nabla_\theta \varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)\cdot t\right)^2\left(\phi(s_{h},a_{h})^\top\Sigma^{-1}\nu_{h}^\theta\right)^2\right] \\
%    &+ \frac{2}{H}\mathbb{E}\left[\sum_{h=1}^H\left(\varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)^2\left(\phi(s_{h},a_{h})^\top\Sigma^{-1}\nabla_\theta^j\nu_{h}^\theta\right)^2\right]\\
%    \leq&2C_1d^\prime\mathbb{E}\left[\sum_{h=1}^H\left(\left(\nabla_\theta \varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)\cdot t\right)^2\left(\nu_h^\theta\right)^\top\Sigma^{-1}\nu_h^\theta\right] \\
%    &+ 2C_1d^\prime G^2H^2\mathbb{E}\left[\sum_{h=1}^H \left(\varepsilon^\theta_{h}(s_{h},a_{h},s_{h+1})\right)^2\left(\nu_h^\theta\right)^\top\Sigma^{-1}\nu_h^\theta\right]
%\end{align*}
%and 
%\begin{align*}
%    \mathbb{E}\left[\sum_{h=1}^H (\varepsilon_h^2 + \eta_h^2)\right] =&\mathbb{E}\left[\left(\nabla_\theta^j Q_1^\theta(s_1,a_1) - \sum_{h=1}^HQ_{h+1}^\theta(s_{h+1}, a_{h+1})\nabla_\theta^j\log\pi_\theta(a_{h+1}\vert s_{h+1})\right)^2\right]\leq 2H^4G^2
%\end{align*}
%Repeating previous steps, we now get
%\begin{align*}
%    [\Lambda_\theta]_{jj} \leq 6C_1C_2d^\prime G^2H^5
%\end{align*}
%Taking a union bound over $j\in[m]$, we finally get 
%\begin{align*}
%    &\Vert\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta \Vert = \sqrt{\sum_{j=1}^m\left(\widehat{\nabla_\theta^j v_\theta}-\nabla_\theta^j v_\theta \right)^2}\leq\tilde{O}\left(\sqrt{\frac{C_1C_2d^\prime mG^2H^5}{N}}\right)
%\end{align*}
%\paragraph{The Case of Tabular MDP. } When the MDP is tabular with one-hot features, the bound can be improved further by removing the dimension $d^\prime$ since we can directly calculate the expression $\phi(s_h,a_h)^\top\Sigma^{-1}\nu_h^\theta = H\frac{d^{\pi_\theta}_h(s_h,a_h)}{d^{\bar\pi}_h(s_h, a_h)}$, where $d^{\pi}_h$ is the state-action distribution at step $h$ under policy $\pi$. In this case, if we adopt definition of the distribution mismatch term from \cite{kallus2020statistically}, i.e.,
%\begin{align*}
%    C_2^\prime = \sup_{\theta\in\Theta, s\in\mathcal{S}, a\in\mathcal{A}, h\in[H]} \frac{d^{\pi_\theta}_h(s,a)}{d^{\bar\pi}_h(s, a)}.
%\end{align*}
%Repeating the previous proof steps, we get the bound
%\begin{align*}
%    \vert\langle t, \widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\rangle \vert&\leq\tilde{O}\left(\sqrt{\frac{C_2^\prime mG^2H^5\Vert t\Vert^2}{N}}\right)\\
%    \Vert\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta \Vert &\leq\tilde{O}\left(\sqrt{\frac{C_2mG^2H^5}{N}}\right). 
%\end{align*}
%Note that \cite{kallus2020statistically} provided an estimator called EOPPG with an asymptotic convergence result
%\begin{align*}
%    \sqrt{N}\left(\widehat{\nabla_\theta v_\theta^\textrm{EOPPG}} - \nabla_\theta v_\theta\right)\rightarrow\mathcal{N}\left(0, \Lambda_\theta^{\textrm{EOPPG}}\right)
%\end{align*}
%where they provided a minimax optimal bound, which, written under our notation, is $\Vert\Lambda_\theta^{\textrm{EOPPG}}\Vert \leq O(C_2^\prime mG^2 H^5)$ (Note that they assume the 2-norm of $\nabla_\theta\log\pi_\theta$ is upper bounded by some $G^\prime$, which should be of order $\sqrt{m}G$ under our notation. And their sample size $n$ equals $N/H$ under our notation). Our bound exactly matches their result. Furthermore, our result is stronger since we provided a finite sample result, which not only applies the tabular MDPs but also the more general linear MDPs. 

\section{Proofs of Main Theorems}
Define $\widehat{\nu}^\theta_h:=\left(\prod_{h^\prime=1}^{h-1}\widehat{M}_{\theta,h^\prime}^\top\right)\nu_1^\theta$.  We may prove the following decomposition of $\nabla_\theta v_\theta-\widehat{\nabla_\theta v_\theta}$:
\begin{lemma}
\label{error_decomp}
We have $\nabla_\theta v_\theta-\widehat{\nabla_\theta v_\theta}=E_1+E_2+E_3$, where 
\begin{align*}
    E_1=&\sum_{h=1}^H\nabla_\theta\left[\left(\nu^\theta_h\right)^\top\Sigma^{-1}_h\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right]\\
    E_2=&\sum_{h=1}^H\nabla_\theta\left[\left(\left(\widehat{\nu}^\theta_h\right)^\top\widehat{\Sigma}_h^{-1}-\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\right)\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right]\\
    E_3=&\frac{\lambda}{K}\sum_{h=1}^H\nabla_\theta\left[\left(\widehat{\nu}^\theta_h\right)^\top\widehat{\Sigma}_h^{-1}w_h^\theta\right].
\end{align*}
\end{lemma}
The proof of Lemma \ref{error_decomp} is deferred to appendix \ref{missing_proof}. Based on this observation, here we show the proofs of our main theorems. 

\subsection{Proof of Theorem \ref{thm2_var}}
\label{pfthm2_var}
\begin{proof}
We use Lemma \ref{error_decomp} to decompose $\langle\nabla_\theta v_\theta - \widehat{\nabla_\theta v_\theta}, t\rangle=\langle E_1, t\rangle+\langle E_2,t\rangle+\langle E_3,t\rangle$. To bound each term individually, we introduce the following lemmas, whose proofs are deferred to appendix \ref{missing_proof}. 
\begin{lemma}
\label{e1_finite_product}
For any $t\in\mathbb{R}^m$, with probability $1-\delta$, we have
\begin{align*}
    \vert\langle E_1, t\rangle\vert\leq\sqrt{\frac{2t^\top\Lambda_\theta t\log(2/\delta)}{K}}+\frac{2\log(2/\delta)\sqrt{C_1md}\Vert t\Vert B}{3K}. 
\end{align*}
where $B=\sum_{h=1}^H(H-h+1)\max_{j\in[m]}\sqrt{\left(\nabla^j_\theta\nu^\theta_h\right)^\top\Sigma_h^{-1}\nabla^j_\theta\nu^\theta_h}+2G\sum_{h=1}^H(H-h)^2\sqrt{\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\nu^\theta_h}$.
\end{lemma}
\begin{lemma}
\label{e2}
Let $E_2^j$ be the $j$th entry of $E_2$, suppose $K\geq 36\kappa_1(4+\kappa_2+\kappa_3)^2C_1dH^2\log\frac{24dmH}{\delta}$ and\\
$\lambda\leq C_1d\min_{h\in[H]}\sigma_{\min}(\Sigma_h)\log\frac{24dmH}{\delta}$, then with probability $1-\delta$, 
\begin{align*}
    \vert E_2^j\vert\leq 240\sqrt{\kappa_1}(2+\kappa_2+\kappa_3)\sqrt{C_1}dH^3\left(\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+HG\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\max_{h\in[H]}\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\frac{\log\frac{24dmH}{\delta}}{K},\quad\forall j\in[m].
\end{align*}
\end{lemma}
\begin{lemma}
\label{e3}
Let $E_3^j$ be the $j$th entry of $E_3$, suppose $K\geq 36\kappa_1(4+\kappa_2+\kappa_3)^2\log\frac{24dmH}{\delta}C_1dH^3$ and\\
$\lambda\leq C_1dH\min_{h\in[H]}\sigma_{\min}(\Sigma_h)\log\frac{24dmH}{\delta}$, with probability $1-\delta$, 
\begin{align*}
    \vert E_3^j\vert\leq&6C_1dH^2\max_{h\in[H]}\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^{\theta}_1\right\Vert+HG\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\frac{\log\frac{24dmH}{\delta}}{K},\quad\forall j\in[m]. 
\end{align*}
\end{lemma}
Let $B_1^j=\sum_{h=1}^H(H-h+1)\sqrt{\left(\nabla^j_\theta\nu^\theta_h\right)^\top\Sigma_h^{-1}\nabla^j_\theta\nu^\theta_h},\ B_2=\sum_{h=1}^H(H-h)^2G\sqrt{\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\nu^\theta_h}$, then we have the relation $B = \max_{j\in[m]}B_1^j + 2B_2$. For any $j\in[m]$, note that
\begin{align*}
    B_1^j=&\sum_{h=1}^H (H-h+1)\left\Vert \Sigma_h^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_h\right\Vert\leq \sum_{h=1}^H H\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma_{\theta,h}^{-\frac{1}{2}}\nabla_\theta^j\left(\left(\prod_{h^\prime=1}^{h-1}M_{\theta,h^\prime}\right)^\top\nu^\theta_1\right)\right\Vert\\
    =&\sum_{h=1}^H H\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\\
    \cdot&\left(\left(\prod_{h^\prime=1}^{h-1}\left\Vert\Sigma_{\theta,h^\prime}^{\frac{1}{2}}M_{\theta,h^\prime}\Sigma_{\theta,h^\prime+1}^{-\frac{1}{2}}\right\Vert\right)\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^{\theta}_1\right\Vert+\sum_{h^\prime=1}^{h-1}\left(\prod_{h^{\prime\prime}\neq h^\prime}\left\Vert\Sigma_{\theta,h^{\prime\prime}}^{\frac{1}{2}}M_{\theta,h^{\prime\prime}}\Sigma_{\theta,h^{\prime\prime}+1}^{-\frac{1}{2}}\right\Vert\right)\left\Vert\Sigma_{\theta,h^\prime}^{\frac{1}{2}}\left(\nabla_\theta^j M_{\theta,h^\prime}\right)\Sigma_{\theta,h^\prime+1}^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\\
    \leq&\sum_{h=1}^H H\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+HG\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\leq H^2\max_{h\in[H]}\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+HG\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right),
\end{align*}
where we use the result of Lemma \ref{ineq}. Similarly, 
\begin{align*}
    B_2&=\sum_{h=1}^H(H-h)^2G\left\Vert\Sigma_h^{-\frac{1}{2}}\nu^\theta_h\right\Vert\leq \sum_{h=1}^H H^2G\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\prod_{h^\prime=1}^{h-1}\left\Vert\Sigma_{\theta,h^\prime}^{\frac{1}{2}}M_{\theta,h^\prime}\Sigma_{\theta,h^\prime+1}^{-\frac{1}{2}}\right\Vert\right)\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\\
    &\leq\sum_{h=1}^H H^2G\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\leq H^3G\max_{h\in[H]}\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert.
\end{align*}
We conclude that when $K\geq 36C_1dH^2\kappa_1(4+\kappa_2+\kappa_3)^2\log\frac{24dmH}{\delta}$, we have
\begin{align*}
    \left(\max_{j\in[m]}B_1^j+2B_2\right)\frac{2\log\frac{2}{\delta}\sqrt{C_1md}\Vert t\Vert}{K}\leq H^2\max_{h\in[H]}\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\max_{j\in[m]}\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+2HG\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\frac{2\log\frac{2}{\delta}\sqrt{C_1md}\Vert t\Vert}{K},
\end{align*}
and therefore, with probability $1-3\delta$, we have
\begin{align*}
    &\vert\langle E_1,t\rangle\vert+\vert\langle E_2,t\rangle\vert+\vert \langle E_3, t\rangle\vert\leq\sqrt{\frac{2t^\top\Lambda_\theta t\log(2/\delta)}{K}}\\
    &+\sqrt{\kappa_1}(5+\kappa_2+\kappa_3)\left(\max_{j\in[m]}\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+HG\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\max_{h\in[H]}\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\frac{240C_1dH^3\log\frac{24dmH}{\delta}}{K}\\
    \leq&\sqrt{\frac{2t^\top\Lambda_\theta t\log(2/\delta)}{K}}+\kappa_1(5+\kappa_2+\kappa_3)\left(\max_{j\in[m]}\left\Vert \Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+HG\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\frac{240C_1dH^3\sqrt{m}\Vert t\Vert\log\frac{24dmH}{\delta}}{K}.
\end{align*}
replacing $\delta$ by $\frac{\delta}{3}$, we have finished the proof. 
\end{proof}

\subsection{Proof of Theorem \ref{thm2}}
\label{pfthm2}
\begin{proof}
According to the result of Theorem \ref{thm2_var}, we know
\begin{align*}
    &\vert\langle t,\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\rangle\vert\leq\sqrt{\frac{2t^\top\Lambda_\theta t}{K}\cdot \log\frac{8}{\delta}}+\frac{C_\theta\Vert t\Vert\log\frac{72mdH}{\delta}}{K},
\end{align*}
Pick $t=e_j, j\in[m]$, we have
\begin{align*}
    t^\top \Lambda_\theta t=&\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nu_h^\theta\right)\right)^2\right]\\
    \leq&2\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nu_h^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^H\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nabla_\theta^j\nu_h^\theta\right)^2\right]\\
    \leq&2\mathbb{E}\left[\sum_{h=1}^HG^2(H-h)^4\left(\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nu_h^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^H(H-h+1)^2\left(\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nabla_\theta^j\nu_h^\theta\right)^2\right]\\
     \leq&2\sum_{h=1}^H\left((H-h)^4G^2\left\Vert\Sigma_h^{-\frac{1}{2}}\nu_h^\theta\right\Vert^2+(H-h+1)^2\left\Vert\Sigma_h^{-\frac{1}{2}}\nabla_\theta^j\nu_h^\theta\right\Vert^2\right).
\end{align*}
On the other hand, we have
\begin{align*}
    t^\top \Lambda_\theta t=& \mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j\left(\varepsilon^\theta_{h,1}\phi\left(s_{h}^{(1)},a_{h}^{(1)}\right)^\top\Sigma_h^{-1}\nu_h^\theta\right)\right)^2\right]\\
    \leq&2\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nu_h^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^H\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nabla_\theta^j\nu_h^\theta\right)^2\right]\\
    \leq&2C_1d\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j \varepsilon^\theta_{h,1}\right)^2\right]\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nu_h^\theta\right\Vert^2+2C_1d\mathbb{E}\left[\sum_{h=1}^H\left(\varepsilon^\theta_{h,1}\right)^2\right]\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nabla_\theta^j\nu_h^\theta\right\Vert^2.
\end{align*}
Define
\begin{align*}
    \zeta_h&=\nabla_\theta^j Q_h^\theta\left(s_h^{(1)},a_h^{(1)}\right)-\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a\left\vert s_{h+1}^{(1)}\right.\right)\left(\nabla_\theta^j Q_{h+1}^\theta\left(s_{h+1}^{(1)},a\right)+Q_{h+1}^\theta\left(s_{h+1}^{(1)}, a\right)\nabla_\theta^j\log\pi_{\theta,h+1}\left(a\left\vert s_{h+1}^{(1)}\right.\right)\right)\mathrm{d}a\\
    \eta_h&=\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a\left\vert s_{h+1}^{(1)}\right.\right)\left(\nabla_\theta^j Q_{h+1}^\theta\left(s_{h+1}^{(1)},a\right)+Q_{h+1}^\theta\left(s_{h+1}^{(1)},a\right)\nabla_\theta^j\log\pi_{\theta,h+1}\left(a\left\vert s_{h+1}^{(1)}\right.\right)\right)\mathrm{d}a\\
    &-\nabla_\theta^j Q_{h+1}^\theta\left(s_{h+1}^{(1)},a_{h+1}^{(1)}\right)-Q_{h+1}^\theta\left(s_{h+1}^{(1)},a_{h+1}^{(1)}\right)\nabla_\theta^j\log\pi_{\theta,h+1}\left(a_{h+1}^{(1)}\left\vert s_{h+1}^{(1)}\right.\right).
\end{align*}
Note that the sequence $\zeta_1,\eta_1,\zeta_2,\eta_2\ldots, \zeta_H,\eta_H$ forms a martingale difference sequence, therefore, we have
\begin{align*}
    \mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j\varepsilon^\theta_{h,1}\right)^2\right]=&\mathbb{E}\left[\sum_{h=1}^H\zeta_h^2\right]\leq\mathbb{E}\left[\sum_{h=1}^H(\zeta_h^2+\eta_h^2)\right]=\mathbb{E}\left[\left(\sum_{h=1}^H\left(\zeta_h+\eta_h\right)\right)^2\right]\\
    =&\mathbb{E}\left[\left(\nabla_\theta Q_1^\theta\left(s_1^{(1)},a_1^{(1)}\right)-\sum_{h=1}^HQ_{h+1}^\theta\left(s_{h+1}^{(1)},a^{(1)}_{h+1}\right)\nabla_\theta \log\pi_{\theta,h+1}\left(a_{h+1}^{(1)}\left\vert s_{h+1}^{(1)}\right.\right)\right)^2\right]\\
    \leq& 4H^4G^2.
\end{align*}
Similarly, 
\begin{align*}
    \mathbb{E}\left[\sum_{h=1}^H\left(\varepsilon^\theta_{h,1}\right)^2\right]=&\mathbb{E}\left[\sum_{h=1}^H\left(Q_h^\theta\left(s_h^{(1)},a_h^{(1)}\right)-r_h^{(1)}-\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a\left\vert s^{(1)}_{h+1}\right.\right)Q_{h+1}^\theta\left(s^{(1)}_{h+1},a\right)\mathrm{d}a\right)^2\right]\\
    &+\mathbb{E}\left[\sum_{h=1}^H\left(\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a\left\vert s_{h+1}^{(1)}\right.\right)Q_{h+1}^\theta\left(s_{h+1}^{(1)},a\right)\mathrm{d}a-Q_{h+1}^\theta\left(s_{h+1}^{(1)},a_{h+1}^{(1)}\right)\right)^2\right]\\
    =&\mathbb{E}\left[\left(Q_1^\theta\left(s_1^{(1)},a_1^{(1)}\right)-\sum_{h=1}^Hr_h^{(1)}\right)^2\right]\leq H^2.
\end{align*}
Therefore, 
\begin{align*}
    t^\top \Lambda_\theta t \leq&8C_1dH^4G^2\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nu_h^\theta\right\Vert^2+2C_1dH^2\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nabla_\theta^j\nu_h^\theta\right\Vert^2.
\end{align*}
Therefore, taking a union bound over $j$, we get
\begin{align*}
\left\vert\widehat{\nabla_\theta^j v_\theta}-\nabla_\theta^j v_\theta\right\vert\leq 4b_\theta\sqrt{\frac{\min\left\{C_1d,H\right\}\log\frac{8m}{\delta}}{K}}+\frac{2C_\theta\sqrt{m}\log\frac{72mdH}{\delta}}{K},\quad\forall j\in[m],
\end{align*}
where 
\begin{align*}
b_\theta=H^2G\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nu_h^\theta\right\Vert+H\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nabla_\theta^j\nu_h^\theta\right\Vert.
\end{align*}
When we in addition have $\phi(s^\prime,a^\prime)^\top\Sigma_h^{-1}\phi(s,a)\geq 0,\forall h\in[H],(s,a),(s^\prime,a^\prime)\in\mathcal{S}\times\mathcal{A}$, we have for any $(s,a)\in\mathcal{S}\times\mathcal{A}$, 
\begin{align*}
    \left\vert\left(\nabla^j_\theta\nu^\theta_{h}\right)^\top\Sigma^{-1}_h\phi(s,a)\right\vert =& \left\vert\mathbb{E}^{\pi_\theta}\left[\phi(s_h,a_h)\Sigma^{-1}_h\phi(s,a)\sum_{h^\prime=1}^h\nabla_\theta^j\log\pi_{\theta,h^\prime}(a_{h^\prime}\vert s_{h^\prime})\right]\right\vert\\
    \leq&\mathbb{E}^{\pi_\theta}\left[\phi(s_h, a_h)\Sigma^{-1}_h\phi(s, a)\sum_{h^\prime=1}^h\left\vert\nabla_\theta^j\log\pi_{\theta,h^\prime}(a_{h^\prime}\vert s_{h^\prime})\right\vert\right]\\
    \leq&Gh\mathbb{E}^{\pi_\theta}\left[\phi(s_h,a_h)\Sigma^{-1}_h\phi(s,a)\right]\\
    =&Gh\left(\nu^\theta_{h}\right)^\top\Sigma^{-1}_h\phi(s,a),
\end{align*}
which implies
\begin{align*}
    \left(\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nabla_\theta^j\nu_{h}^\theta\right)^2\leq G^2h^2\left(\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nu_h^\theta\right)^2.
\end{align*}
Therefore, we get 
\begin{align*}
    t^\top\Lambda_\theta t=&\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nu_h^\theta\right)\right)^2\right]\\
    \leq&2\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nu_h^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^H\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nabla_\theta^j\nu_h^\theta\right)^2\right]\\
    \leq&2\mathbb{E}\left[\sum_{h=1}^HG^2(H-h)^4\left(\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nu_h^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^HG^2h^2(H-h+1)^2\left(\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nu_h^\theta\right)^2\right]\\
     \leq&2H^2G^2\sum_{h=1}^H(H-h+1)^2\left\Vert\Sigma_h^{-\frac{1}{2}}\nu_h^\theta\right\Vert^2,
\end{align*}
and
\begin{align*}
    t^\top\Lambda_\theta t\leq&2\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j \varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nu_h^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^H\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma_h^{-1}\nabla_\theta^j\nu_h^\theta\right)^2\right]\\
    \leq&2C_1d\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j \varepsilon^\theta_{h,1}\right)^2\right]\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nu_h^\theta\right\Vert^2+2C_1dG^2H^2\mathbb{E}\left[\sum_{h=1}^H\left(\varepsilon^\theta_{h,1}\right)^2\right]\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nu_h^\theta\right\Vert^2
\end{align*}
Repeating the steps that we bound $\mathbb{E}\left[\sum_{h=1}^H \left(\varepsilon^\theta_{h,1}\right)^2\right]$ and $\mathbb{E}\left[\sum_{h=1}^H \left(\nabla_\theta^j\varepsilon^\theta_{h,1}\right)^2\right]$, we get
\begin{align*}
    \left\vert\widehat{\nabla_\theta^j v_\theta}-\nabla_\theta^j v_\theta\right\vert\leq 4H^2G\sqrt{\frac{\min\{C_1d,H\}\log\frac{8m}{\delta}}{K}}\max_{h\in[H]}\left\Vert\Sigma_h^{-\frac{1}{2}}\nu_h^\theta\right\Vert+\frac{2C_\theta\log\frac{72mdH}{\delta}}{K},\quad\forall j\in[m].
\end{align*}
\end{proof}

%\subsection{Proof of Theorem \ref{thm3}}
%\label{pfthm3}
%\begin{proof}
%According to the result of Theorem \ref{thm2_var}, we know
%\begin{align*}
%    &\vert\langle t, \widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\rangle \vert\leq  \sqrt{\frac{2t^\top\Lambda_\theta t}{K}\cdot \log\frac{8m}{\delta}}+\frac{C\sqrt{m}\Vert t\Vert\log\frac{72mdH}{\delta}}{K},
%\end{align*}
%Pick $t=e_j, j\in[m]$, we have
%\begin{align*}
%    t^\top \Lambda_\theta t =& \mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j\left(\varepsilon^\theta_{h,1}\phi(s_{h}^{(1)},a_{h}^{(1)})^\top\Sigma_h^{-1}\nu_{h}^\theta\right)\right)^2\right]\\
%    \leq&2\mathbb{E}\left[\sum_{h=1}^H\left(\nabla_\theta^j \varepsilon^\theta_{h,1}\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma_h^{-1}\nu_{h}^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^H\left(\varepsilon^\theta_{h,1}\phi(s_{h}^{(1)},a_{h}^{(1)})^\top\Sigma_h^{-1}\nabla_\theta^j\nu_{h}^\theta\right)^2\right]\\
%    \leq&2\mathbb{E}\left[\sum_{h=1}^HG^2(H-h)^4\left(\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma_h^{-1}\nu_{h}^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^H(H-h+1)^2\left(\phi(s_{h}^{(1)},a_{h}^{(1)})^\top\Sigma_h^{-1}\nabla_\theta^j\nu_{h}^\theta\right)^2\right]\\
%     \leq&2\sum_{h=1}^H\left((H-h)^4G^2\left\Vert\Sigma_h^{-\frac{1}{2}}\nu_h^\theta\right\Vert^2+(H-h+1)^2\left\Vert\Sigma_h^{-\frac{1}{2}}\nabla_\theta^j\nu_h^\theta\right\Vert^2\right)
%\end{align*}

%With similar steps as in the proof of Theorem \ref{thm2_var}, we get with probability $1-3\delta$, 
%\begin{align*}
%    &\vert \nabla_\theta^j v_\theta - \widehat{\nabla_\theta^j v_\theta}\vert\leq(\tilde{B}_1+2\tilde{B}_2)\sqrt{\frac{2\log\frac{8m}{\delta}}{N}}+\kappa_1(3+\kappa_2+\kappa_3)\left(\Vert \Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\Vert+HG\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\Vert\right)\frac{240C_1dH^{3.5}\log\frac{24dmH}{\delta}}{N}
%\end{align*}
%Notice the fact that $\phi(s^\prime, a^\prime)^\top\Sigma^{-1}\phi(s,a) \geq 0,\ \forall (s,a),(s^\prime, a^\prime)\in\mathcal{S}\times\mathcal{A}$, we have
%\begin{align*}
%    \left(\sum_{h=1}^H h(H-h+1)\nu_h^\theta\right)^\top\Sigma^{-1}\left(\sum_{h=1}^H (H-h)^2\nu_h^\theta\right)\geq 0.
%\end{align*}
%Using the relation $\Vert a\Vert + \Vert b\Vert \leq 2\Vert a+b\Vert$ whenever $\langle a,b\rangle\geq 0$, we get
%\begin{align*}
%    \tilde{B}_1+2\tilde{B}_2 \leq 2(\tilde{B}_1+\tilde{B}_2)\leq 4HG\left\Vert \sum_{h=1}^H (H-h+1)\Sigma^{-\frac{1}{2}}\nu_h^\theta\right\Vert
%\end{align*}
%which implies
%\begin{align*}
%    \vert \nabla_\theta^j v_\theta - \widehat{\nabla_\theta^j v_\theta}\vert\leq &4HG\left(\sup_{f\in\mathcal{F}}\frac{\sum_{h=1}^H(H-h+1)\mathbb{E}^{\pi_\theta}[f(s_h,a_h)\vert s_1\sim\xi_1]}{\sqrt{\mathbb{E}[\frac{1}{H}\sum_{h=1}^H f^2(s_{1,h},a_{1,h})]}}\right)\sqrt{\frac{2\log\frac{8m}{\delta}}{N}}\\
%    &+\kappa_1(3+\kappa_2+\kappa_3)\left(\Vert \Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\Vert+HG\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\Vert\right)\frac{240C_1dH^{3.5}\log\frac{24dmH}{\delta}}{N}\\
%    \leq &4HG\left(\sum_{h=1}^H(H-h+1)\right)\left(\sup_{f\in\mathcal{F}}\frac{\mathbb{E}^{\pi_\theta}\left[\frac{\sum_{h=1}^H(H-h+1)f(s_h,a_h)}{\sum_{h=1}^H H-h+1}\vert s_1\sim\xi_1\right]}{\sqrt{\mathbb{E}[\frac{1}{H}\sum_{h=1}^H f^2(s_{1,h},a_{1,h})]}}\right)\sqrt{\frac{2\log\frac{8m}{\delta}}{N}}\\
%    &+\kappa_1(3+\kappa_2+\kappa_3)\left(\Vert \Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\Vert+HG\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\Vert\right)\frac{240C_1dH^{3.5}\log\frac{24dmH}{\delta}}{N}\\
%    \leq &4H^3G\sqrt{1+\chi_\mathcal{F}^2(\mu_\theta,\bar{\mu})}\sqrt{\frac{2\log\frac{8m}{\delta}}{N}}\\
%    &+\kappa_1(3+\kappa_2+\kappa_3)\left(\Vert \Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\Vert+HG\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\Vert\right)\frac{240C_1dH^{3.5}\log\frac{24dmH}{\delta}}{N},\quad\forall j\in[m].
%\end{align*}
%Replacing $\delta$ by $\frac{\delta}{3}$, we have finished the proof. 
%\end{proof}

\subsection{Proof of Theorem \ref{thm1}}
\label{pfthm1}
\begin{proof}
We use the same decomposition as in Theorem \ref{thm2_var}. Define a martingale difference sequence $\{e_k^\theta\}_{k=1}^K$ by
\begin{align*}
    e_k^\theta&=\frac{1}{\sqrt{K}}\sum_{h=1}^H\nabla_\theta\left(\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\phi(s_h^{(k)},a_h^{(k)})\varepsilon_{h,k}^{\theta}\right)\\
    &=\frac{1}{\sqrt{K}}\sum_{h=1}^H\left(\nabla_\theta\nu^\theta_h\right)^\top\Sigma_h^{-1}\phi(s_h^{(k)},a_h^{(k)})\varepsilon_{h,k}^\theta+\frac{1}{\sqrt{K}}\sum_{h=1}^H\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\phi(s_h^{(k)},a_h^{(k)})\nabla_\theta\varepsilon_{h,k}^\theta,
\end{align*}
we have 
\begin{align*}
    \Vert e_k^\theta\Vert_\infty\leq\frac{1}{\sqrt{K}}\sum_{h=1}^H\max_{j\in[m]}\Vert\Sigma_h^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_h\Vert\sqrt{C_1d}(H-h+1)+\frac{2}{\sqrt{K}}\sum_{h=1}^H\Vert\Sigma_h^{-\frac{1}{2}}\nu^\theta_h\Vert\sqrt{C_1d}(H-h)^2G\rightarrow 0, 
\end{align*}
where we use the result of Lemma \ref{upbd}. Furthermore, 
\begin{align*}
    \mathbb{E}\left[e_k^\theta\left(e_k^\theta\right)^\top\right]_{ij}=&\frac{1}{K}\mathbb{E}\left[\sum_{h=1}^H\left[\nabla_{\theta_1}^i\left(\left(\nu^{\theta_1}_h\right)^\top\Sigma_h^{-1}\phi(s_h^{(k)},a_h^{(k)})\varepsilon_{h,k}^{\theta_1}\right)\right]\left[\nabla_{\theta_2}^j\left(\left(\nu^{\theta_2}_h\right)^\top\Sigma_h^{-1}\phi(s_h^{(k)},a_h^{(k)})\varepsilon_{h,k}^{\theta_2}\right)\right]^\top\right]\Bigg\vert_{\theta_1=\theta_2=\theta}=\frac{[\Lambda_\theta]_{ij}}{K}.
\end{align*}
Therefore,by WLLN, we have
\begin{align*}
    \sum_{k=1}^K\left[e_k^\theta\left(e_k^\theta\right)^\top\right]_{ij}\rightarrow_p\sum_{k=1}^K\mathbb{E}\left[e_k^\theta\left(e_k^\theta\right)^\top\right]_{ij}=\left[\Lambda_\theta\right]_{ij},
\end{align*}
To finish the rest of the proof, we introduce the following lemmas, 
\begin{lemma}[Martingale CLT, Corollary 2.8 in (McLeish et al., 1974)] \label{CLT}
Let $\left\{X_{mn},n=1,\ldots,k_m\right\}$ be a martingale difference array (row-wise) on the probability triple $(\Omega, \mathcal{F}, P)$.Suppose $X_{mn}$ satisfy the following two conditions:
\begin{align*}
    \max _{1\leq n\leq k_m}\left\vert X_{mn}\right\vert\stackrel{p}{\rightarrow}0,\textrm{ and } \sum_{n=1}^{k_m}X_{mn}^2\stackrel{p}{\rightarrow}\sigma^2
\end{align*}
for $k_m\rightarrow\infty$. Then $\sum_{n=1}^{k_m}X_{mn}\stackrel{d}{\rightarrow}\mathcal{N}\left(0,\sigma^2\right)$.
\end{lemma}
\begin{lemma}[CramÃ©râ€“Wold Theorem] 
\label{CW_thm}
Let $X_n=(X_n^1,X_n^2,\ldots,X_n^k)^\top$ be a $k$-dimensional random vector series and $X=(X^1,X^2,\ldots,X^k)^\top$ be a random vector of same dimension. Then $X_n$ converges in distribution to $X$ if and only if for any constant vector $t=(t_1,t_2,\ldots,t_k)^\top$, $t^\top X_n$ converges to $t^\top X$ in distribution.
\end{lemma}
Lemma \ref{CLT} implies $\sum_{k=1}^K t^\top e_k\rightarrow_d\mathcal{N}(0,t^\top\Lambda_\theta t)$ for any $t$, and Lemma \ref{CW_thm} implies
\begin{align*}
    \sqrt{K}E_1=\sum_{k=1}^K e_k\rightarrow_p\mathcal{N}(0,\Lambda_\theta). 
\end{align*}
Furthermore, notice that the results of Lemma \ref{e2} and Lemma \ref{e3} imply $\sqrt{K}E_2\rightarrow_p 0, \sqrt{K}E_3\rightarrow_p 0$. Combining the above results, we have finished the proof. 
\end{proof}

\subsection{Proof of Theorem \ref{thm4}}
\label{pfthm4}
\begin{proof}
Our proof is similar to that of \cite{bootstrap}. We first derive the influence function of policy gradient estimator for sake of completeness. We denote each of the $K$ sampled trajectories as
$$
\boldsymbol{\tau}:=\left(s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, \ldots, s_{H}, a_{H}, r_{H}, s_{H+1}\right)
$$
We denote $\bar{\pi}(a \mid s)$ as the behavior policy. The distribution of trajectory is then given by
$$
\mathcal{P}(d \boldsymbol{\tau})= \bar{\xi}\left(d s_{1}, d a_{1}\right) p_1\left(d s_{2} \mid s_{1}, a_{1}\right) \bar{\pi}_2\left(d a_{2} \mid s_{2}\right) \ldots \bar{\pi}_H\left(d a_{H} \mid s_{H}\right) p_H\left(d s_{H+1} \mid s_{H}, a_{H}\right)
$$
Define $p_{\eta} = p + \eta\Delta p$ as a new transition probability function and $\mathcal{P}_{\eta}:=\mathcal{P}+\eta \Delta\mathcal{P}$ where $\Delta \mathcal{P}$ satisfies
$$(\Delta \mathcal{P}_h) \mathcal{F} \subseteq \mathcal{F},\forall h\in[H].$$ 
Define $g_{\eta,h}\left(s^{\prime} \mid s, a\right):=\frac{\partial}{\partial \eta} \log p_{\eta,h}\left( s^{\prime} \mid s, a\right)$ and the score function as 
$$
g_{\eta}(\boldsymbol{\tau}):=\frac{\partial}{\partial \eta} \log \mathcal{P}_{\eta}(d \boldsymbol{\tau})=\sum_{h=1}^{H} g_{\eta,h}\left(s_{h+1} \mid s_{h}, a_{h}\right).
$$
Without loss of generality, we assume $p_{\eta}$ is continuously derivative with respect to $\eta.$ This guarantees that we can change the order of taking derivatives with respect to $\eta$ and $\theta.$ When the subscript $\eta$ vanishes, it means $\eta = 0$ and the underlying transition probability is $p(s^{\prime}|s,a),$ i.e. $p_0(s^{\prime} |s,a) = p(s^{\prime} |s,a).$ Then we denote $g_h(s^{\prime}|s,a) := \left.\frac{\partial}{\partial \eta}\log p_{\eta,h}(s^{\prime}|s,a)\right|_{\eta = 0},$ and $g(\boldsymbol{\tau}) = \sum_{h=1}^H g_h(s_{h+1}|s_h,a_h).$ We define the policy value under new transition kernel is
\begin{equation*}
    v_{\theta,\eta} := \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{h=1}^H r_h(s_h,a_h) \right| s_1 \sim \xi, \mathcal{P}_{\eta}\right]
\end{equation*}
Then, our objective function is
$$
\psi_{\eta} := \nabla_{\theta} v_{\theta,\eta} =\mathbb{E}^{\pi_{\theta}}\left[\left.\sum_{h=1}^{H} \nabla_{\theta} \log \pi_{\theta,h}\left(a_{h} \mid s_{h}\right) \cdot\left(\sum_{h^{\prime}=h}^{H} r_h\left(s_{h^{\prime}}, a_{h^{\prime}}\right)\right) \right| s_{1} \sim \xi, \mathcal{P}_{\eta}\right].
$$
We are going to compute the influence function with respect to the above objective function. We denote this influence function as $\mathcal{I}(\boldsymbol{\tau}).$ By definition, it satisfies that
\begin{equation*}
    \left.\frac{\partial}{\partial \eta} \psi_{\eta}\right|_{\eta = 0} = \mathbb{E}\left[g(\boldsymbol{\tau}) \mathcal{I}(\boldsymbol{\tau})\right].
\end{equation*}
By exchanging the order of derivatives, we find that
\begin{equation*}
    \left.\frac{\partial}{\partial \eta} \psi_{\eta}\right|_{\eta = 0} = \nabla_{\theta} \left[\left.\frac{\partial}{\partial \eta} v_{\theta,\eta}\right|_{\eta = 0}\right].
\end{equation*}
Therefore, we calculate the derivatives.
\begin{align*}
    \frac{\partial}{\partial \eta} v_{\theta, \eta}
    &= \frac{\partial}{\partial \eta}\left[\sum_{h=1}^{H} \int_{(\mathcal{S} \times \mathcal{A})^{h}} r_h\left(s_{h}, a_{h}\right) \xi(s_1) \prod_{j=1}^{h-1} p_{\eta,j}\left(s_{j+1} \mid s_{j}, a_{j}\right) \prod_{j=1}^{h} \pi_{\theta,j}\left(a_{j} \mid s_{j}\right)d\boldsymbol{\tau}_h\right] \\
    &=\sum_{h=1}^{H} \int_{(\mathcal{S} \times \mathcal{A})^{h}} r_h\left(s_{h}, a_{h}\right)\left(\sum_{j=1}^{h-1} g_{\eta,j}\left(s_{j+1} \mid s_{j}, a_{j}\right)\right) \xi(s_1) \prod_{j=1}^{h-1} p_{\eta,j}\left(s_{j+1} \mid s_{j}, a_{j}\right) \prod_{j=1}^{h} \pi_{\theta,j}\left(a_{j} \mid s_{j}\right) d\boldsymbol{\tau}_h\\
    &= \int_{(\mathcal{S} \times \mathcal{A})^{H}} \sum_{h=1}^{H} r_h\left(s_{h}, a_{h}\right)\left(\sum_{j=1}^{h-1} g_{\eta,j}\left(s_{j+1} \mid s_{j}, a_{j}\right)\right) \left[\xi(s_1) \prod_{j=1}^{H} p_{\eta,j}\left(s_{j+1} \mid s_{j}, a_{j}\right) \prod_{j=1}^{H} \pi_{\theta,j}\left(a_{j} \mid s_{j}\right)\right] d\boldsymbol{\tau}.
\end{align*}
We denote $Q_{h,\eta}^{\theta}$ and  $\nabla_{\theta}Q_{h,\eta}^{\theta}$ as the state-action function and its gradient with underlying transition probability being $p_{\eta}.$ For sake of simplicity, we define the state value function as
\begin{equation*}
    V_h^{\theta}(s) := \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{h^{\prime} = h}^H r_h(s_h,a_h) \right| s_h = s, \mathcal{P}\right].
\end{equation*}
We denote $V_{h,\eta}^{\theta}(s)$ as the same function except for transition probability substituted by $p_{\eta}.$ Therefore,
\begin{align*}
    \frac{\partial}{\partial \eta} v_{\theta, \eta}
    &= \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{h=1}^{H} r_h\left(s_{h}, a_{h}\right)\left(\sum_{j=1}^{h-1} g_{\eta,j}\left(s_{j+1} \mid s_{j}, a_{j}\right)\right) \right| s_1 \sim \xi, \mathcal{P}_{\eta} \right] \\
    &= \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{j=1}^{H} g_{\eta,j}\left(s_{j+1} \mid s_{j}, a_{j}\right) \sum_{h=j+1}^H r_h\left(s_{h}, a_{h}\right) \right| s_1 \sim \xi, \mathcal{P}_{\eta} \right] \\
    &= \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{j=1}^{H} g_{\eta,j}\left(s_{j+1} \mid s_{j}, a_{j}\right) \cdot \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{h=j+1}^H r_h\left(s_{h}, a_{h}\right) \right| s_{j+1}\right]\right| s_1 \sim \xi, \mathcal{P}_{\eta} \right] \\
    &= \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{j=1}^{H} \mathbb{E}\left[\left. g_{\eta,j}\left(s_{j+1} \mid s_{j}, a_{j}\right) V_{j+1,\eta}^{\theta}(s_{j+1}) \right| s_j,a_j \right]\right| s_1 \sim \xi, \mathcal{P}_{\eta} \right].
\end{align*}
Therefore,
\begin{equation}\label{influence_function1}
    \left.\frac{\partial}{\partial \eta} v_{\theta, \eta}\right|_{\eta=0} = \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{h=1}^{H} \mathbb{E}\left[\left. g_h\left(s_{h+1} \mid s_{h}, a_{h}\right) V_{h+1}^{\theta}(s_{h+1}) \right| s_h,a_h \right]\right| s_1 \sim \xi, \mathcal{P}_{\eta} \right].
\end{equation}
We notice that $\Sigma_h = \mathbb{E}\left[\phi(s_h^{(1)},a_h^{(1)})\phi(s_h^{(1)},a_h^{(1)})^{\top}\right]$. We denote $w_h(s,a) := \phi^{\top}(s,a)\Sigma_h^{-1} \nu_h^{\theta} = \phi^{\top}(s,a)\Sigma_h^{-1} \mathbb{E}^{\pi_{\theta}} \left[\phi(s_h,a_h) \mid s_1 \sim \xi\right].$ We leverage the following fact to rewrite \eqref{influence_function1}: for any $f(s,a) = w_f^{\top} \phi(s,a) \in \mathcal{F}$ where $w_f \in \mathbb{R}^d,$ we have
\begin{align*}
\mathbb{E}^{\pi_{\theta}}\left[f(s_h,a_h)\right]
&= \mathbb{E}^{\pi_{\theta}} \left[ w_f^{\top} \phi(s_h,a_h)\right] \\
&= \mathbb{E}^{\pi_{\theta}} \left[ w_f^{\top} \mathbb{E}\left[\phi(s_h^{(1)},a_h^{(1)})\phi^{\top}(s_h^{(1)},a_h^{(1)})\right] \Sigma_h^{-1} \phi(s_h,a_h) \right] \\
&= \mathbb{E} \left[w_f^{\top}\phi(s_h^{(1)},a_h^{(1)})\phi^{\top}(s_h^{(1)},a_h^{(1)})\Sigma_h^{-1}\mathbb{E}^{\pi_{\theta}} \left[\phi(s_h,a_h)\right]\right]\\
&= \mathbb{E}\left[ f(s_h^{(1)},a_h^{(1)}) w_h(s_h^{(1)},a_h^{(1)})\right]
\end{align*}
Since 
\begin{equation*}
    \mathbb{E} \left[ g_h\left(s^{\prime} \mid s, a\right) V_{h+1}^{\theta}(s^{\prime}) | s, a \right] = \left.\frac{\partial}{\partial \eta} \left(Q_{h,\eta}^{\theta}(s,a) - r_{\eta}(s,a)\right)\right|_{\eta = 0} \in \mathcal{F},
\end{equation*}
we have
\begin{align*}
    \left.\frac{\partial}{\partial\eta} v_{\theta,\eta}\right|_{\eta = 0}
    &= \mathbb{E}\left[\sum_{h=1}^{H} w_{h}(s_h^{(1)},a_h^{(1)}) \mathbb{E}\left[g_h\left(s^{\prime} \mid s_h^{(1)},a_h^{(1)}\right) \cdot  V_{h+1}^{\theta}\left(s^{\prime}\right) \mid s_h^{(1)},a_h^{(1)}\right]\right] \\
    &=\mathbb{E}\left[\mathbb{E}_{s^{\prime} \sim p(\cdot \mid s_h^{(1)},a_h^{(1)})}\left[\sum_{h=1}^{H} w_{h}(s_h^{(1)},a_h^{(1)}) g_h\left(s^{\prime} \mid s_h^{(1)},a_h^{(1)}\right) \cdot  V_{h+1}^{\theta}\left(s^{\prime}\right)\right]\right] \\
    &=\mathbb{E}\left[\mathbb{E}_{s^{\prime} \sim p(\cdot \mid s_h^{(1)},a_h^{(1)})}\left[\sum_{h=1}^{H} w_{h}(s_h^{(1)},a_h^{(1)}) g_h\left(s^{\prime} \mid s_h^{(1)},a_h^{(1)}\right)\left( V_{h+1}^{\theta}\left(s^{\prime}\right)-\mathbb{E}\left[V_{h+1}^{\theta}\left(s^{\prime}\right) \mid s_h^{(1)},a_h^{(1)}\right]\right)\right]\right]\\
    &=\mathbb{E}\left[\sum_{h=1}^{H} w_{h}(s_h^{(1)},a_h^{(1)}) g_h\left(s_{h+1}^{(1)} \mid s_h^{(1)},a_h^{(1)}\right)\left( V_{h+1}^{\theta}\left(s_{h+1}^{(1)}\right)-\mathbb{E}\left[V_{h+1}^{\theta}\left(s_{h+1}^{(1)}\right) \mid s_h^{(1)},a_h^{(1)}\right]\right)\right]\\
    &= \mathbb{E} \left[g\left(\boldsymbol{\tau}\right)\sum_{h=1}^H w_{h}(s_h^{(1)}, a_h^{(1)})\left( V_{h+1}^{\theta}\left(s_{h+1}^{(1)}\right)-\mathbb{E}\left[V_{h+1}^{\theta}\left(s_{h+1}^{(1)}\right)\mid s_h^{(1)},a_h^{(1)}\right]\right)\right].
\end{align*}
Taking gradient in both sides and we have
\begin{equation*}
    \nabla_{\theta}\left(\left.\frac{\partial}{\partial\eta} v_{\theta,\eta}\right|_{\eta = 0}\right) = \mathbb{E}\left\{g\left(\boldsymbol{\tau}\right) \cdot \nabla_{\theta} \left[\sum_{h=1}^H w_{h}(s_h^{(1)}, a_h^{(1)}) \left( V_{h+1}^{\theta}\left(s_{h+1}^{(1)}\right)-\mathbb{E}\left[V_{h+1}^{\theta}\left(s_{h+1}^{(1)}\right) \mid s_h^{(1)}, a_h^{(1)}\right]\right)\right]\right\}.
\end{equation*}
The implies that the influence function we want is
\begin{equation*}
    \mathcal{I}(\boldsymbol{\tau}) = \nabla_{\theta} \left[\sum_{h=1}^H w_{h}(s_h^{(1)}, a_h^{(1)}) \left( V_{h+1}^{\theta}\left(s_{h+1}^{(1)}\right)-\mathbb{E}\left[V_{h+1}^{\theta}\left(s_{h+1}^{(1)}\right) \mid s_h^{(1)}, a_h^{(1)}\right]\right)\right].
\end{equation*}
Insert the expression of $w_h(s,a)$ and exploit $\varepsilon_{h,k}^{\theta}=Q_{h}^{\theta}(s_h^{(k)}, a_h^{(k)})-r_h^{(k)}-\int_{\mathcal{A}} \pi_{\theta,h+1}\left(a^{\prime} \mid s_{h+1}^{(k)}\right) Q_{h+1}^{\theta}\left(s_{h+1}^{(k)}, a^{\prime}\right) \mathrm{d} a^{\prime},$ we can rewrite the influence function as
\begin{equation*}
    \mathcal{I}(\boldsymbol{\tau})=-\nabla_\theta\left[\sum_{h=1}^H\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma_h^{-1}\varepsilon_{h,1}^\theta\nu_h^\theta\right]
\end{equation*}
Therefore, since the cross terms vanish by taking conditional expectation, we have
\begin{align*}
    &\mathbb{E}\left[\mathcal{I}(\boldsymbol{\tau})^{\top} \mathcal{I}(\boldsymbol{\tau})\right]=\mathbb{E}\Bigg[\sum_{h=1}^H\left(\nabla_\theta\left(\varepsilon^\theta_{h,1}\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma^{-1}_h\nu_{h}^\theta\right)\right)^\top\nabla_\theta\left(\varepsilon^\theta_{h,1}\phi(s_h^{(1)},a_h^{(1)})^\top\Sigma^{-1}_h\nu_{h}^\theta \right)\Bigg]=\Lambda_{\theta}.
\end{align*}
For any vector $t \in \mathbb{R}^m,$ when it comes to $\left\langle t,\psi_{\eta}\right\rangle,$ by linearity we have
\begin{equation*}
    \left.\frac{\partial}{\partial \eta} \left\langle t,\psi_{\eta}\right\rangle\right|_{\eta=0}=\mathbb{E}[g(\boldsymbol{\tau}) \left\langle t,\mathcal{I}(\boldsymbol{\tau})\right\rangle].
\end{equation*}
Then the influence function of $\left\langle t,\nabla_{\theta}v_{\theta}\right\rangle$ is $\left\langle t,\mathcal{I}(\boldsymbol{\tau})\right\rangle.$ The Cramer-Rao lower bound for $\left\langle t,\nabla_{\theta}v_{\theta}\right\rangle$ is
\begin{equation*}
    \mathbb{E}\left[\left\langle t,\mathcal{I}(\boldsymbol{\tau})\right\rangle^2\right] = t^{\top} \mathbb{E}\left[\mathcal{I}(\boldsymbol{\tau})^{\top}\mathcal{I}(\boldsymbol{\tau})\right] t = t^{\top} \Lambda_{\theta} t.
\end{equation*}
By continuous mapping theorem, a trivial corollary of Theorem \ref{thm4} is that for any $t \in \mathbb{R}^m,$
\begin{equation*}
    \sqrt{K}\left(\left\langle t,\widehat{\nabla_{\theta} v_{\theta}}-\nabla_{\theta} v_{\theta}\right\rangle\right) \stackrel{d}{\rightarrow} \mathcal{N}\left(0, t^{\top} \Lambda_{\theta} t\right).
\end{equation*} 
This implies that the variance of any unbiased estimator for $\left\langle t, \nabla_{\theta} v_{\theta} \right\rangle \in \mathbb{R}$ is lower bounded by $\frac{1}{\sqrt{K}}t^{\top} \Lambda_{\theta} t.$
\end{proof}

%\subsection{Proof of Theorem \ref{dis_cons}}
%\label{pfdis_cons}
%\begin{proof}
%According to the result of Lemma \ref{M_concentration}, we have
%\begin{align}
%\label{base_concentration}
%\sqrt{N}\operatorname{vec}\left(\widehat{M}_\theta-M_\theta,\widehat{\nabla_\theta M_\theta}-\nabla_\theta M_\theta\right)\stackrel{d}{\rightarrow}N\left(0,\tilde{\Delta}\right), \\
%\label{base_concentration_star}
%\sqrt{N}\operatorname{vec}\left(\widehat{M}^*_\theta - \widehat{M}_\theta, \widehat{\nabla_\theta M^*_\theta} - \widehat{\nabla_\theta M_\theta}\right) \stackrel{d}{\rightarrow} N\left(0, \tilde{\Delta}\right). 
%\end{align}
%where the definition of $\tilde{\Delta}$ is stated in Lemma \ref{M_concentration}. Define function $g: M_\theta\rightarrow\mathbb{R}$ by $g(M_\theta) =\sum_{h=1}^H\left(\nu_1^\theta\right)^\top M_\theta^{h-1}w_r$ and vector function $\tilde{g}: \left(M_\theta, \nabla_\theta M_\theta\right)\rightarrow \mathbb{R}^{m}$ by
%\begin{align*}
%\tilde{g}_p(M_\theta, \nabla_\theta M_\theta) &= \frac{\partial}{\partial\theta_p}g(M_\theta) = \sum_{h=1}^H\left(\nabla_\theta^p\nu_1^\theta\right)^\top M_\theta^{h-1}w_r + \sum_{h=1}^H\sum_{h'=1}^{h-1}\left(\nu_1^\theta\right)^\top M_\theta^{h'-1}(\nabla_\theta^p M_\theta)M_\theta^{h-h'-1}w_r,\quad p=1,2,\ldots, m. 
%\end{align*}
%Applying multivariate delta theorem, and Lemma \ref{CW_thm}, we have
%\begin{align*}
%    \sqrt{N}\left(\tilde{g}(\widehat{M_\theta^*}, \widehat{\nabla_\theta M_\theta^*})-\tilde{g}(\widehat{M_\theta}, \widehat{\nabla_\theta M_\theta})\right) \stackrel{d}{\rightarrow} \mathcal{N}\left(0, \tilde{\Lambda}(\widehat{M_\theta}, \widehat{\nabla_\theta M_\theta})\right)
%\end{align*}
%where $\tilde{\Lambda}\in\mathbb{R}^{m\times m}$ is defined by
%\begin{align*}
%    \left[\tilde{\Lambda}(\widehat{M_\theta}, \widehat{\nabla_\theta M_\theta})\right]_{pq} = \textrm{vec}\left(\nabla \tilde{g}_p(\widehat{M_\theta}, \widehat{\nabla_\theta M_\theta})\right)^\top \tilde{\Lambda}\textrm{vec}\left(\nabla \tilde{g}_q\left(\widehat{M_\theta}, \widehat{\nabla_\theta M_\theta}\right)\right)
%\end{align*}
%and the gradient of $\tilde{g}$ is taken over $M_\theta$ and $\nabla_\theta M_\theta$. According to the result of \eqref{base_concentration}, we have $\widehat{M}_\theta \stackrel{d}{\rightarrow} M_\theta$ and $\widehat{\nabla_\theta M_\theta} \stackrel{d}{\rightarrow} \nabla_\theta M_\theta$, by continuous mapping theorem, we get 
%\begin{align*}
%    \sqrt{N}\left(\tilde{g}(\widehat{M_\theta^*},\widehat{\nabla_\theta M_\theta^*})-\tilde{g}(\widehat{M_\theta}, \widehat{\nabla_\theta M_\theta})\right)\stackrel{d}{\rightarrow}\mathcal{N}\left(0,\tilde{\Lambda}(M_\theta,\nabla_\theta M_\theta)\right)
%\end{align*}
%Note that we have
%\begin{align*}
%    \frac{\partial}{\partial M_\theta}\tilde{g}_p(M_\theta, \nabla_\theta M_\theta) =& \frac{\partial}{\partial M_\theta}\left[\sum_{h=1}^H\left(\nabla_\theta^p\nu_1^\theta\right)^\top M_\theta^{h-1}w_r + \sum_{h=1}^H\sum_{h'=1}^{h-1}\left(\nu_1^\theta\right)^\top M_\theta^{h'-1}(\nabla_\theta^p M_\theta)M_\theta^{h-h'-1}w_r\right]\\
%    =& \sum_{h=1}^H\sum_{h'=1}^{h-1}\left(M_\theta^{h'-1}\right)^\top\left(\nabla_\theta^p\nu_1^\theta\right) w_r^\top \left(M_\theta^{h-h'-1}\right)^\top \\
%    &+ \sum_{h=1}^H\sum_{h'=1}^{h-1}\sum_{h^{\prime\prime}=1}^{h'-1}\left(M_\theta^{h''-1}\right)^\top\nu_1^\theta w_r^\top \left(M_\theta^{h-h'-1}\right)^\top(\nabla_\theta^p M_\theta)^\top\left(M_\theta^{h^\prime-h^{\prime\prime}-1}\right)^\top\\
%    &+ \sum_{h=1}^H\sum_{h'=1}^{h-1}\sum_{h''=1}^{h-h^\prime-1}\left(M_\theta^{h''-1}\right)^\top(\nabla_\theta^p M_\theta)^\top\left(M_\theta^{h'-1}\right)^\top\nu_1^\theta w_r^\top \left(M_\theta^{h-h'-h''-1}\right)^\top\\
%    =& \sum_{h=1}^H\sum_{h'=1}^{h-1}\left(M_\theta^{h^\prime-1}\right)^\top\left(\nabla_\theta^p\nu_1^\theta\right) w_r^\top \left(M_\theta^{h-h'-1}\right)^\top \\
%    &+ \sum_{h=1}^H\sum_{h'=1}^{h-1}\sum_{h''=1}^{h'-1}\left(M_\theta^{h''-1}\right)^\top\nu_1^\theta w_r^\top \left(M_\theta^{h-h'-1}\right)^\top(\nabla_\theta^p M_\theta)^\top\left(M_\theta^{h'-h''-1}\right)^\top\\
%    &+ \sum_{h=1}^H\sum_{\tilde{h}^\prime=1}^{h-1}\sum_{\tilde{h}^{\prime\prime}=1}^{\tilde{h}^\prime-1}\left(M_\theta^{\tilde{h}''-1}\right)^\top(\nabla_\theta^p M_\theta)^\top\left(M_\theta^{\tilde{h}'-\tilde{h}''-1}\right)^\top\nu_1^\theta w_r^\top \left(M_\theta^{h-\tilde{h}'-1}\right)^\top\\
%    =&\frac{\partial}{\partial\theta_p}\left(\sum_{h=1}^H\sum_{h'=1}^{h-1}\left(M_\theta^{h'-1}\right)^\top\nu_1^\theta w_r^\top\left(M_\theta^{h-h'-1}\right)^\top\right)\\
%    =&\frac{\partial}{\partial\theta_p}\frac{\partial}{\partial M_\theta}\left(\sum_{h=1}^H\left(\nu_1^\theta\right)^\top M_\theta^{h-1}w_r\right), 
%\end{align*}
%and
%\begin{align*}
%    &\frac{\partial}{\partial\left(\nabla_\theta^p M_\theta\right)}\tilde{g}_p(M_\theta,\nabla_\theta M_\theta)=\sum_{h=1}^H\sum_{h^\prime=1}^{h-1}\left(M_\theta^{h^\prime-1}\right)^\top\nu_1^\theta w_r^\top\left(M_\theta^{h-h^\prime-1}\right)^\top=\frac{\partial}{\partial M_\theta}\left(\sum_{h=1}^H\left(\nu_1^\theta\right)^\top M_\theta^{h-1}w_r\right),\\
%    &\frac{\partial}{\partial \left(\nabla_\theta^q M_\theta\right)}\tilde{g}_p(M_\theta,\nabla_\theta M_\theta)=0,\ q\neq p.
%\end{align*}
%Therefore, we get
%\begin{align*}
%    &\tilde{\Lambda}(M_\theta, \nabla_\theta M_\theta)_{pq} = \textrm{vec}\left(\frac{\partial g_p}{\partial M_\theta}, \frac{\partial g_p}{\partial \left(\nabla_\theta M_\theta\right)}\right)^\top \tilde{\Delta}     \ \textrm{vec}\left(\frac{\partial g_q}{\partial M_\theta}, \frac{\partial g_q}{\partial \left(\nabla_\theta M_\theta\right)}\right)\\
%    =&\mathbb{E}\left[\textrm{Tr}\left(\left(\frac{\partial g_p}{\partial M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\xi_{h,\theta}^1\right)\cdot \textrm{Tr}\left(\left(\frac{\partial g_q}{\partial M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\xi_{h,\theta}^1\right)\right]\\
%    &+\mathbb{E}\left[\textrm{Tr}\left(\left(\frac{\partial g_p}{\partial \left(\nabla_\theta^p M_\theta\right)}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\nabla_\theta^p\xi_{h,\theta}^1\right)\cdot \textrm{Tr}\left(\left(\frac{\partial g_q}{\partial M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\xi_{h,\theta}^1\right)\right]\\
%    &+\mathbb{E}\left[\textrm{Tr}\left(\left(\frac{\partial g_p}{\partial  M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\nabla_\theta^p\xi_{h,\theta}^1\right)\cdot \textrm{Tr}\left(\left(\frac{\partial g_q}{\partial \left(\nabla_\theta^q M_\theta\right)}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\nabla_\theta^q\xi_{h,\theta}^1\right)\right]\\
%    &+\mathbb{E}\left[\textrm{Tr}\left(\left(\frac{\partial g_p}{\partial \left(\nabla_\theta^p M_\theta\right)}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\nabla_\theta^p\xi_{h,\theta}^1\right)\cdot \textrm{Tr}\left(\left(\frac{\partial g_q}{\partial \left(\nabla_\theta^q M_\theta\right)}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\nabla_\theta^q\xi_{h,\theta}^1\right)\right]\\
%    =&\mathbb{E}\left[\textrm{Tr}\left(\left(\frac{\partial}{\partial\theta_p}\frac{\partial g}{\partial M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\xi_{h,\theta}^1\right)\cdot \textrm{Tr}\left(\left(\frac{\partial}{\partial\theta_q}\frac{\partial g}{\partial M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\xi_{h,\theta}^1\right)\right]\\
%    &+\mathbb{E}\left[\textrm{Tr}\left(\left(\frac{\partial g}{\partial M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\nabla_\theta^p\xi_{h,\theta}^1\right)\cdot \textrm{Tr}\left(\left(\frac{\partial}{\partial \theta_q}\frac{\partial g}{\partial M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\xi_{h,\theta}^1\right)\right]\\
%    &+\mathbb{E}\left[\textrm{Tr}\left(\left(\frac{\partial}{\partial\theta_p}\frac{\partial g}{\partial M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\xi_{h,\theta}^1\right)\cdot \textrm{Tr}\left(\left(\frac{\partial g}{\partial M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\nabla_\theta^q\xi_{h,\theta}^1\right)\right]\\
%    &+\mathbb{E}\left[\textrm{Tr}\left(\left(\frac{\partial g}{\partial M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\nabla_\theta^p\xi_{h,\theta}^1\right)\cdot \textrm{Tr}\left(\left(\frac{\partial g}{\partial M_\theta}\right)^\top\Sigma^{-1}\frac{1}{\sqrt{H}}\sum_{h=1}^H \phi(s_h^1, a_h^1)\nabla_\theta^q\xi_{h,\theta}^1\right)\right]\\
%    =&\frac{1}{H}\frac{\partial^2}{\partial\theta_{1,p}\partial\theta_{2,q}}\mathbb{E}\left[\textrm{Tr}\left(\left(\frac{\partial g}{\partial M_{\theta_1}}\right)^\top\Sigma^{-1}\sum_{h=1}^H \phi(s_h^1, a_h^1)\xi_{h,\theta_1}^1\right)\cdot \textrm{Tr}\left(\left(\frac{\partial g}{\partial M_{\theta_2}}\right)^\top\Sigma^{-1}\sum_{h=1}^H \phi(s_h^1, a_h^1)\xi_{h,\theta_2}^1\right)\right]
%\end{align*}
%Note that
%\begin{align*}
%\textrm{Tr}\left(\left(\frac{\partial g}{\partial M_\theta}\right)^\top\Sigma^{-1}\sum_{h=1}^H \phi(s_h^1, a_h^1)\xi_{h,\theta}^1\right)=&\textrm{Tr}\left(\sum_{h=1}^H\sum_{h'=1}^{h-1}M_{\theta}^{h-h'-1}w_r\left(\nu_{h'}^{\theta} \right)^\top\Sigma^{-1}\sum_{h''=1}^H \phi(s_{h''}^1, a_{h''}^1)\xi_{h'',\theta}^1\right)\\
%=&\sum_{h=1}^H\sum_{h'=1}^{h-1}\left(\nu_{h'}^{\theta} \right)^\top\Sigma^{-1}\sum_{h''=1}^H \phi(s_{h''}^1, a_{h''}^1)\xi_{h'',\theta}^1M_{\theta}^{h-h'-1}w_r\\
%=&\sum_{h'=1}^H\sum_{h=1}^{H-h'}\left(\nu_{h'}^{\theta} \right)^\top\Sigma^{-1}\sum_{h''=1}^H \phi(s_{h''}^1, a_{h''}^1)\xi_{h'',\theta}^1M_{\theta}^{h-1}w_r\\
%=&\sum_{h'=1}^H\sum_{h=1}^{H-h'}\sum_{h''=1}^H\left(\nu_{h'}^{\theta} \right)^\top\Sigma^{-1} \phi(s_{h''}^1, a_{h''}^1)\xi_{h'',\theta}^1M_{\theta}^{h-1}w_r
%\end{align*}
%And by definition, we have the relation
%\begin{align*}
%\varepsilon_{h^{\prime},h^{\prime\prime}}^{\theta} 
%&= Q_{h^\prime}^{\theta}\left(s_{h^{\prime\prime}}^{1}, a_{h^{\prime\prime}}^{1}\right)-\left(r_{h^{\prime\prime}}^{1}+\int \pi_\theta(a'\vert s_{h^{\prime\prime}+1})Q_{h^\prime+1}^{\theta}\left(s_{h^{\prime\prime}+1}^{1}, a'\right)\mathrm{d}a'\right) \\
%&=\sum_{h=1}^{H-h^\prime} \phi\left(s_{h^{\prime\prime}}^1, a_{h^{\prime\prime}}^{1}\right)^\top M_{\theta}^{h} w_{r}-\sum_{h=1}^{H - h^{\prime}} \phi^{\theta}\left(s_{h^{\prime\prime}+1}^{\prime}\right)^{\top} M_{\theta}^{h-1} w_{r} \\
%&=\sum_{h=1}^{H-h^\prime} \xi_{h^{\prime\prime}, \theta}^{1} M_{\theta}^{h-1} w_{r}.
%\end{align*}
%We get
%\begin{align*}
%\textrm{Tr}\left(\left(\frac{\partial g}{\partial M_\theta}\right)^\top\Sigma^{-1}\sum_{h=1}^H \phi(s_h^1, a_h^1)\xi_{h,\theta}^1\right)=&\sum_{h'=1}^H\sum_{h''=1}^H\left(\nu_{h'}^{\theta} \right)^\top\Sigma^{-1} \phi(s_{h''}^1, a_{h''}^1)\varepsilon_{h^{\prime},h^{\prime\prime}}^{\theta}\\
%=&\sum_{h=1}^H\sum_{h'=1}^H\left(\nu_{h}^{\theta} \right)^\top\Sigma^{-1} \phi(s_{h'}^1, a_{h'}^1)\varepsilon_{h,h^{\prime}}^{\theta}
%\end{align*}
%Therefore, 
%\begin{align*}
%    &\tilde{\Lambda}(M_\theta, \nabla_\theta M_\theta)_{pq}\\
%    =&\frac{1}{H}\frac{\partial^2}{\partial\theta_{1,p}\partial\theta_{2,q}}\mathbb{E}\left[\left(\sum_{h=1}^H\sum_{h'=1}^H\left(\nu_{h}^{\theta_1} \right)^\top\Sigma^{-1} \phi(s_{h'}^1, a_{h'}^1)\varepsilon_{h,h^{\prime}}^{\theta_1}\right)\cdot \left(\sum_{h=1}^H\sum_{h'=1}^H\left(\nu_{h}^{\theta_2} \right)^\top\Sigma^{-1} \phi(s_{h'}^1, a_{h'}^1)\varepsilon_{h,h^{\prime}}^{\theta_2}\right)\right]\\
%    =&\frac{1}{H}\sum_{h_1=1}^H\sum_{h_2=1}^H\frac{\partial^2}{\partial\theta_{1,p}\partial\theta_{2,q}}\mathbb{E}\left[\left(\sum_{h'=1}^H\left(\nu_{h_1}^{\theta_1} \right)^\top\Sigma^{-1} \phi(s_{h'}^1, a_{h'}^1)\varepsilon_{h_1,h^{\prime}}^{\theta_1}\right)\cdot \left(\sum_{h'=1}^H\left(\nu_{h_2}^{\theta_2} \right)^\top\Sigma^{-1} \phi(s_{h'}^1, a_{h'}^1)\varepsilon_{h_2,h^{\prime}}^{\theta_2}\right)\right]\\
%    =&\sum_{h_1=1}^H\sum_{h_2=1}^H\frac{\partial^2}{\partial\theta_{1,p}\partial\theta_{2,q}}\left[\left(\nu_{h_1}^{\theta_1}\right)^\top\Sigma^{-1}\mathbb{E}\left[\frac{1}{H}\left(\sum_{h'=1}^H \phi(s_{h'}^1, a_{h'}^1)\varepsilon_{h_1,h^{\prime}}^{\theta_1}\right)\cdot \left(\sum_{h'=1}^H\phi(s_{h'}^1, a_{h'}^1)^\top\varepsilon_{h_2,h^{\prime}}^{\theta_2}\right)\right]\Sigma^{-1}\nu_{h_2}^{\theta_2}\right]\\
%    =&\sum_{h_1=1}^H\sum_{h_2=1}^H\frac{\partial^2}{\partial\theta_{1,p}\partial\theta_{2,q}}\left[\left(\nu_{h_1}^{\theta_1}\right)^\top\Sigma^{-1}\mathbb{E}\left[\frac{1}{H}\left(\sum_{h=1}^H \phi(s_{h}^1, a_{h}^1)\phi(s_h^1,a_h^1)^\top\varepsilon_{h_1,h}^{\theta_1}\varepsilon_{h_2,h}^{\theta_2}\right)\right]\Sigma^{-1}\nu_{h_2}^{\theta_2}\right]\\
%    =&\sum_{h_1=1}^H\sum_{h_2=1}^H\frac{\partial^2}{\partial\theta_{1,p}\partial\theta_{2,q}}\left[\left(\nu_{h_1}^{\theta_1}\right)^\top\Sigma^{-1}\Omega_{h_1, h_2}^{\theta_1, \theta_2}\Sigma^{-1}\nu_{h_2}^{\theta_2}\right]
%\end{align*}
%where we use the fact
%\begin{align*}
%    \mathbb{E}\left[\phi(s_h^1,a_h^1)\phi(s_{h^\prime}^1,a_{h^\prime}^1)^\top\varepsilon_{h_1,h}^{\theta_1}\varepsilon_{h_2,h^{\prime}}^{\theta_2}\right] = 0 \textrm{ whenever } h\neq h^\prime
%\end{align*}
%and 
%\begin{align*}
%    \Omega_{h_1,h_2}^{\theta_1,\theta_2}=\mathbb{E}\left[\frac{1}{H}\sum_{h=1}^H\phi(s_{1,h},a_{1,h})\phi(s_{1,h},a_{1,h})^\top\varepsilon^{\theta_1}_{h_1,h}\varepsilon^{\theta_2}_{h_2,h}\right].
%\end{align*}
%This term is exactly equal to as the definition of $\Lambda_\theta$ in Theorem \ref{thm1}. Therefore, we have proved
%\begin{align*}
%    \sqrt{N}\left(\tilde{g}(\widehat{M_\theta^*},\widehat{\nabla_\theta M_\theta^*})-\tilde{g}(\widehat{M_\theta},\widehat{\nabla_\theta M_\theta})\right)\stackrel{d}{\rightarrow}\mathcal{N}\left(0,\Lambda_\theta\right).
%\end{align*}
%Furthermore,due to Lemma \ref{dr}, we have
%\begin{align*}
%    \sqrt{N}\widehat{\nabla_\theta v_\theta^*}\rightarrow_p \sqrt{N}\tilde{g}(\widehat{M_\theta^*},\widehat{\nabla_\theta M_\theta^*}),\quad\sqrt{N}\widehat{\nabla_\theta v_\theta}\rightarrow_p \sqrt{N}\tilde{g}(\widehat{M_\theta},\widehat{\nabla_\theta M_\theta})
%\end{align*}
%which has finished the proof. 
%\end{proof}

\section{Missing Proofs}
\label{missing_proof}
\subsection{Proof of Proposition \ref{lin_rep}}
\begin{proof}
The differentiablity of $w_h^\theta$ comes from the differentiablity of $Q_h^\theta$. And simply taking derivatives w.r.t. $\theta$ on both sides of $Q_h^\theta=\phi^\top w_h^\theta$, we get the desired result. 
\end{proof}

\subsection{Proof of Lemma \ref{equiv_mb}}
\begin{proof}
To prove the equality, it suffices to prove that given the same input $\widehat{Q}^\theta_{h+1}, \nabla_\theta^j\widehat{Q}^\theta_{h+1}$, we have
\begin{align*}
    &\mathop{\arg\min}_{f\in\mathcal{F}}\left(\sum_{k=1}^K\left(f(s_h^{(k)},a_h^{(k)})-r_h^{(k)}-\int_{\mathcal{A}}\pi_{\theta,h+1}(a^\prime\vert s_{h+1}^{(k)})\widehat{Q}_{h+1}^{\theta}(s_{h+1}^{(k)}, a^\prime)\mathrm{d}a^\prime\right)^2+\lambda\rho(f)\right) = \widehat{r}_h+\widehat{\mathcal{P}}_{\theta,h}\widehat{Q}_{h+1}^{\theta}\\
    &\mathop{\arg\min}_{f\in\mathcal{F}}\left(\sum_{k=1}^K\left(f(s_h^{(k)},a_h^{(k)})-\int_{\mathcal{A}}\pi_{\theta,h+1}(a^\prime\vert s_{h+1}^{(k)})\left(\left(\nabla_\theta^j\log\pi_{\theta,h+1}(a^\prime\vert s_{h+1}^{(k)})\right)\widehat{Q}_{h+1}^{\theta}(s_{h+1}^{(k)}, a^\prime)+\widehat{\nabla_\theta^j Q_{h+1}^{\theta}}(s_{h+1}^{(k)}, a^\prime)\right)\mathrm{d}a^\prime\right)^2+\lambda\rho(f)\right) \\
    &= \widehat{\mathcal{P}}_{\theta,h}\left(\left(\nabla_\theta^j\log\Pi_\theta\right)\widehat{Q}_{h+1}^{\theta}+\widehat{\nabla_\theta^j Q^{\theta}_{h+1}}\right).
\end{align*}
The second equation holds due to the definition of $\widehat{\mathcal{P}}_{\theta,h}$. For the first equation, note that when $\mathcal{F}$ is the class of linear functions and $\rho(\phi^\top w)=\Vert w\Vert^2$, the LHS has a closed form solution:
\begin{align*}
    &\mathop{\arg\min}_{f\in\mathcal{F}}\left(\sum_{k=1}^K\left(f(s_h^{(k)},a_h^{(k)})-r_h^{(k)}-\int_{\mathcal{A}}\pi_{\theta,h+1}(a^\prime\vert s_{h+1}^{(k)})\widehat{Q}_{h+1}^{\theta}(s_{h+1}^{(k)}, a^\prime)\mathrm{d}a^\prime\right)^2+\lambda\rho(f)\right) \\
    =& \phi^\top\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\left(r_h^{(k)} + \int_{\mathcal{A}}\pi_{\theta,h+1}(a^\prime\vert s_{h+1}^{(k)})\widehat{Q}_{h+1}^{\theta}(s_{h+1}^{(k)}, a^\prime)\mathrm{d}a^\prime\right)\\
    =&\phi^\top\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K r_h^{(k)} + \phi^\top\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K \int_{\mathcal{A}}\pi_\theta(a^\prime\vert s_{h+1}^{(k)})\widehat{Q}_{h+1}^{\theta}(s_{h+1}^{(k)}, a^\prime)\mathrm{d}a^\prime\\
    =&\widehat{r}_h+\widehat{\mathcal{P}}_{\theta,h}\widehat{Q}_{h+1}^{\theta}. 
\end{align*}
Therefore, we have finished the proof. 
\end{proof}

\subsection{Proof of Proposition \ref{union_bd}}
\begin{proof}
The result of Theorem \ref{thm2} implies for any fixed $\theta$, when we choose $\lambda \leq\log\frac{8dmH}{\delta}C_1d\min_{h\in[H]}\sigma_{\textrm{min}}(\Sigma_h)$, and $K\geq 36\kappa_1(4+\kappa_2+\kappa_3)^2\log\frac{8dmH}{\delta}C_1dH^2$ sufficiently large such that
\begin{align*}
    &4H^2G\sqrt{\min\{C_1d,H\}}\sqrt{1+\chi^2_{\mathcal{F}}(\{\mu_{\theta,h}\}_{h=1}^H,\{\bar{\mu}_h\}_{h=1}^H)}\sqrt{\frac{2\log\frac{24m}{\delta}}{K}}\\
    \geq&480C_1dm^{0.5}H^{3.5}\kappa_1(5+\kappa_2+\kappa_3)(\max_{j\in[m]}\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\Vert+HG\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\Vert)\frac{\log\frac{72mdH}{\delta}}{K},
\end{align*}
then we have
\begin{align*}
    \Vert\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\Vert \leq 8&H^2G\sqrt{m\min\{C_1d,H\}}\sqrt{1+\chi^2_{\mathcal{F}}(\{\mu_{\theta,h}\}_{h=1}^H,\{\bar{\mu}_h\}_{h=1}^H)}\sqrt{\frac{2\log\frac{24m}{\delta}}{K}}.
\end{align*}
Note that when the diameter of $\Theta$ is bounded by $D$, for any $\varepsilon > 0$, it's always possible to find an $\varepsilon$-net $\mathcal{N}_\varepsilon$ such that $\vert \mathcal{N}_\varepsilon\vert\leq\left(\frac{mD}{\varepsilon}\right)^m$. Taking a union bound over $\mathcal{N}_\varepsilon$, we get with probability $1-\delta$, 
\begin{align*}
     \Vert\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\Vert\leq 16H^2Gm\sqrt{\min\{C_1d,H\}}\sqrt{1+\chi^2_{\mathcal{F}}(\{\mu_{\theta,h}\}_{h=1}^H,\{\bar{\mu}_h\}_{h=1}^H)}\sqrt{\frac{\log\frac{24mD}{\delta\varepsilon}}{K}}, \quad\forall\theta\in\mathcal{N}_\varepsilon.
\end{align*}
Therefore, for any $\theta\in\Theta$, pick $\theta^\prime\in\mathcal{N}_\varepsilon$ such that $\Vert\theta-\theta^\prime\Vert\leq \varepsilon$, we have
\begin{align*}
    \Vert\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\Vert&\leq 2L\varepsilon + \Vert\widehat{\nabla_\theta v_{\theta^\prime}}-\nabla_\theta v_{\theta^\prime}\Vert\\
    &\leq 2L\varepsilon+16H^2Gm\sqrt{\min\{C_1d,H\}}\sqrt{1+\chi^2_{\mathcal{F}}(\{\mu_{\theta^\prime,h}\}_{h=1}^H,\{\bar{\mu}_h\}_{h=1}^H)}\sqrt{\frac{\log\frac{24mD}{\delta\varepsilon}}{K}}. 
\end{align*}
Because $\chi^2_{\mathcal{F}}(\{\mu_{\theta^\prime,h}\}_{h=1}^H,\{\bar{\mu}_h\}_{h=1}^H)$ is $L^\prime$-Lipschitz in $\theta$, we have
\begin{align*}
    \Vert\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\Vert\leq 2L\varepsilon+16H^2Gm\sqrt{\min\{C_1d,H\}}\sqrt{1+2L^\prime\varepsilon + \chi^2_{\mathcal{F}}(\{\mu_{\theta,h}\}_{h=1}^H,\{\bar{\mu}_h\}_{h=1}^H)}\sqrt{\frac{\log\frac{24mD}{\delta\varepsilon}}{K}}. 
\end{align*}
In particular, pick
\begin{align*}
    \varepsilon = \min\left\{\frac{1}{L^\prime},\frac{16H^2Gm}{L}\sqrt{\frac{\min\{C_1d,H\}}{K}}\right\},
\end{align*}
we get
\begin{align*}
    \Vert\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\Vert\leq 64H^2Gm\sqrt{\min\{C_1d,H\}}\sqrt{1+\chi^2_{\mathcal{F}}(\{\mu_{\theta,h}\}_{h=1}^H,\{\bar{\mu}_h\}_{h=1}^H)}\sqrt{\frac{\log\frac{24DKLL^\prime}{\delta HG}}{K}},\quad\forall\theta\in\Theta. 
\end{align*}
\end{proof}

\subsection{Proof of Lemma \ref{error_decomp}}
\begin{proof}
Note that
\begin{align*}
    \nabla_\theta Q_1^\theta-\widehat{\nabla_\theta Q_1^\theta} &= \sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\mathcal{P}_{\theta,h^\prime}\right)U_h^\theta-\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\tilde{U}_h^\theta\\
    &=\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\mathcal{P}_{\theta,h^\prime}\right)U_h^\theta-\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\widehat{U}_h^\theta+\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\left(\widehat{U}_h^\theta-\tilde{U}_h^\theta\right)\\
    &=\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\left( \nabla_\theta Q_h^\theta-\widehat{U}_h^\theta-\widehat{\mathcal{P}}_{\theta,h}\nabla_\theta Q_{h+1}^\theta\right)+\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\left(\widehat{U}_h^\theta -\tilde{U}_h^\theta\right).
\end{align*}
For the first term, we have
\begin{align*}
    &\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\left(\nabla_\theta Q_h^\theta-\widehat{U}_h^\theta-\widehat{\mathcal{P}}_{\theta,h}\nabla_\theta Q_{h+1}^\theta\right)\\
    =&\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\phi^\top\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\\
    &\cdot\left(\nabla_\theta Q_h^\theta\left(s_h^{(k)},a_h^{(k)}\right)-\int_{\mathcal{A}}\left(\left(\nabla_\theta\pi_{\theta,h+1}\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\right)Q_{h+1}^{\theta}\left(s_{h+1}^{(k)},a^\prime\right)+\pi_{\theta,h+1}\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\nabla_\theta Q_{h+1}^\theta\left(s_{h+1}^{(k)},a^\prime\right)\right)\mathrm{d}a^\prime\right)\\
    &+\frac{\lambda}{K}\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\phi^\top\widehat{\Sigma}^{-1}_h\nabla_\theta w_h^\theta\\
    =&\sum_{h=1}^H\phi^\top\left(\prod_{h^\prime=1}^{h-1}\widehat{M}_{\theta,h^\prime}\right)\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta\varepsilon_{h,k}^\theta+\frac{\lambda}{K}\sum_{h=1}^H\phi^\top\left(\prod_{h^\prime=1}^{h-1}\widehat{M}_{\theta,h^\prime}\right)\widehat{\Sigma}^{-1}_h\nabla_\theta w_h^\theta.
\end{align*}
Using the definition of $\widehat{\nu}_h^\theta$, we get
\begin{align}
    \label{p1}
    \begin{aligned}
        &\int_{\mathcal{S}\times\mathcal{A}}\xi(s)\pi_{\theta,1}(a\vert s)\left(\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\left(\nabla_\theta Q_h^\theta-\widehat{U_h^\theta}-\widehat{\mathcal{P}}_{\theta,h}\nabla_\theta Q_{h+1}^\theta\right)\right)(s,a)\mathrm{d}s\mathrm{d}a\\
        =&\sum_{h=1}^H\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}^{-1}_h\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta\varepsilon_{h,k}^\theta+\frac{\lambda}{K}\sum_{h=1}^H\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}\nabla_\theta w^\theta_h.
    \end{aligned}
\end{align}
For the second term,  by Lemma \ref{Q_decomp}, we have
\begin{align*}
    \sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\left( \widehat{U}_h^\theta-\tilde{U}_h^\theta\right)&=\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\left(\nabla_\theta\log\Pi_{\theta,h+1}\right)\left(Q_{h+1}^\theta-\widehat{Q}_{h+1}^\theta\right)\\
    &=\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\left(\nabla_\theta\log\Pi_{\theta,h+1}\right)\sum_{h^\prime=h+1}^H\left(\prod_{h^{\prime\prime}=h+1}^{h^\prime-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(Q_{h^\prime}^\theta-\widehat{r}_{h^\prime}- \widehat{\mathcal{P}}_{\theta,h^\prime}Q_{h^\prime+1}^\theta\right)\\
    &=\sum_{h=1}^H\sum_{h^\prime=1}^{h-1}\left(\prod_{h=1}^{h^\prime}\widehat{\mathcal{P}}_{\theta,h}\right)\left(\nabla_\theta\log\Pi_{\theta,h^\prime+1}\right)\left(\prod_{h^{\prime\prime}=h^\prime+1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(Q_{h}^\theta-\widehat{r}_{h}- \widehat{\mathcal{P}}_{\theta,h}Q_{h+1}^\theta\right).
\end{align*}
Meanwhile, again by Lemma \ref{Q_decomp}, we have
\begin{align*}
    \left(\nabla_\theta\log\Pi_{\theta,1}\right)(Q_1^\theta-\widehat{Q}_1^\theta)=\left(\nabla_\theta\log\Pi_{\theta,1}\right)\sum_{h=1}^{H}\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\left(Q_{h}^\theta-\widehat{r}_{h}-\widehat{\mathcal{P}}_{\theta,h}Q_{h+1}^\theta\right),
\end{align*}
which implies
\begin{align*}
    &\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\left(\widehat{U}_h^\theta-\tilde{U}_h^\theta\right)+\left(\nabla_\theta\log\Pi_{\theta,1}\right)(Q_1^\theta-\widehat{Q}_1^\theta)\\
    =&\sum_{h=1}^H\left(\left(\nabla_\theta\log\Pi_{\theta,1}\right)\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)+\sum_{h^\prime=1}^{h-1}\left(\prod_{h=1}^{h^\prime}\widehat{\mathcal{P}}_{\theta,h}\right)\left(\nabla_\theta\log\Pi_{\theta,h^\prime+1}\right)\left(\prod_{h^{\prime\prime}=h^\prime+1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\right)\left(Q_{h}^\theta-\widehat{r}_{h}-\widehat{\mathcal{P}}_{\theta,h}Q_{h+1}^\theta\right)\\
    =&\sum_{h=1}^H\sum_{h^\prime=0}^{h-1}\left(\prod_{h=1}^{h^\prime}\widehat{\mathcal{P}}_{\theta,h}\right)\left(\nabla_\theta\log\Pi_{\theta,h^\prime+1}\right)\left(\prod_{h^{\prime\prime}=h^\prime+1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(Q_{h}^\theta-\widehat{r}_{h}-\widehat{\mathcal{P}}_{\theta,h}Q_{h+1}^\theta\right)\\
    =&\sum_{h=1}^H\sum_{h^\prime=0}^{h-1}\left(\prod_{h=1}^{h^\prime}\widehat{\mathcal{P}}_{\theta,h}\right)\left(\nabla_\theta\log\Pi_{\theta,h^\prime+1}\right)\left(\prod_{h^{\prime\prime}=h^\prime+1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\phi^\top\widehat{\Sigma}_h^{-1}\\
    &\cdot\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\left(Q_h^\theta\left(s_h^{(k)},a_h^{(k)}\right)-r_h^{(k)}-\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)Q_{h+1}^\theta\left(s_{h+1}^{(k)},a^\prime\right)\mathrm{d}a^\prime\right)\\
    &+\frac{\lambda}{K}\sum_{h=1}^H\sum_{h^\prime=0}^{h-1}\left(\prod_{h^{\prime\prime}=1}^{h^\prime}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(\nabla_\theta\log\Pi_{\theta,h^\prime+1}\right)\left(\prod_{h^{\prime\prime}=h^\prime+1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\phi^\top\widehat{\Sigma}^{-1}_hw_h^\theta.
\end{align*}
For each $j\in[m]$, notice the relation
\begin{align*}
    \left(\nabla_\theta^j\widehat{\nu}_h^\theta\right)^\top&=\left(\nabla_\theta^j\nu_1^\theta\right)^\top\left(\prod_{h^\prime=1}^{h-1}\widehat{M}_{\theta,h^\prime}\right)+\sum_{h^\prime=1}^{h-1}\left(\nu_1^{\theta}\right)^\top\left(\prod_{h^{\prime\prime}=1}^{h^\prime-1}\widehat{M}_{\theta,h^{\prime\prime}}\right)\left(\widehat{\nabla_\theta^j M_{\theta,h^\prime}}\right)\left(\prod_{h^{\prime\prime}=h^\prime+1}^{h-1}\widehat{M}_{\theta,h^{\prime\prime}}\right)\\
    &=\int_{\mathcal{S}\times\mathcal{A}}\xi(s)\pi_{\theta,1}(a\vert s)\left(\sum_{h^\prime=0}^{h-1}\left(\prod_{h^{\prime\prime}=1}^{h^\prime}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\left(\nabla_\theta\log\Pi_{\theta,h^\prime+1}\right)\left(\prod_{h^{\prime\prime}=h^\prime+1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^{\prime\prime}}\right)\phi^\top\right)(s,a)\mathrm{d}s\mathrm{d}a.
\end{align*} 
Therefore, we have
\begin{align}
    \label{p2}
    \begin{aligned}
        &\left[\int\xi(s)\pi_{\theta,1}(a\vert s)\left(\sum_{h=1}^H\left(\prod_{h^\prime=1}^{h-1}\widehat{\mathcal{P}}_{\theta,h^\prime}\right)\left(\widehat{U}_h^\theta-\tilde{U}_h^\theta\right)+\left(\nabla_\theta\log\Pi_{\theta,1}\right)(Q_1^\theta-\widehat{Q_1^\theta})\right)(s,a)\mathrm{d}s\mathrm{d}a\right]_j\\
        =&\sum_{h=1}^H\left(\nabla_\theta^j\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\left(Q_h^\theta\left(s_h^{(k)},a_h^{(k)}\right)-r_h^{(k)}-\int_{\mathcal{A}}\pi_{\theta,h+1}\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)Q_{h+1}^\theta\left(s_{h+1}^{(k)},a^\prime\right)\mathrm{d}a^\prime\right)\\
        &+\frac{\lambda}{K}\sum_{h=1}^H\left(\nabla_\theta^j\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}w_h^\theta\\
        =&\sum_{h=1}^H\left(\nabla_\theta^j\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta+\frac{\lambda}{K}\sum_{h=1}^H\left(\nabla_\theta^j\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}w_h^\theta.
    \end{aligned}
\end{align}
Combing the results of \eqref{p1} and \eqref{p2}, we get for each $j\in[m]$, 
\begin{align*}
    &\nabla_\theta^j v_\theta-\widehat{\nabla_\theta^j v_\theta}=\int_{\mathcal{S}\times\mathcal{A}}\xi(s)\pi_{\theta,1}(a\vert s)\left(\nabla_\theta^j Q_1^\theta-\widehat{\nabla_\theta^j Q_1^\theta}+\left(\nabla_\theta^j\log\Pi_{\theta,1}\right)(Q_1^\theta-\widehat{Q_1^\theta})\right)(s,a)\textrm{d}s\textrm{d}a\\
    =&\sum_{h=1}^H\Bigg(\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta+\frac{\lambda}{K}\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}\nabla_\theta^j w^\theta_h+\left(\nabla_\theta^j\nu_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\\
    &+\frac{\lambda}{K}\left(\nabla_\theta^j\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}^{-1}_hw_h^\theta\Bigg)\\
    =&\sum_{h=1}^H\nabla_\theta^j\left(\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta+\frac{\lambda}{K}\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}w^{\theta}_h\right)\\
    =&\sum_{h=1}^H\nabla_\theta^j\left(\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta+\frac{\lambda}{K}\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}w^\theta_h+\left(\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}-\left(\nu_h^\theta\right)^\top\Sigma^{-1}_h\right)\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right).
\end{align*}
Rewriting the above decomposition in a vector form, we get
\begin{align*}
    \nabla_\theta v_\theta-\widehat{\nabla_\theta v_\theta}=&\sum_{h=1}^H\nabla_\theta\Bigg(\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}^{-1}_h\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\\
    &+\frac{\lambda}{K}\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}w^\theta_h+\left(\left(\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}_h^{-1}-\left(\nu_h^\theta\right)^\top\Sigma_h^{-1}\right)\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\Bigg),
\end{align*}
which is the desired result. 
\end{proof}

\subsection{Proof of Lemma \ref{e1_finite_product}}
\begin{proof}
Note that, 
\begin{align*}
    \langle E_1,t\rangle=&\sum_{h=1}^H\left\langle\nabla_\theta\left(\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right), t\right\rangle\\
    =&\sum_{h=1}^H\left\langle\left(\nabla_\theta\nu^\theta_h\right)^\top\Sigma_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta, t\right\rangle+\sum_{h=1}^H\left\langle\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta\varepsilon_{h,k}^\theta,t\right\rangle.
\end{align*}
Let $e_k = \sum_{h=1}^H\left\langle\nabla_\theta\left(\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right),t\right\rangle$, we have 
\begin{align*}
    \vert e_k\vert&\leq\sqrt{C_1dm}\Vert t\Vert\sum_{h=1}^H(H-h+1)\max_{j\in[m]}\sqrt{\left(\nabla^j_\theta\nu^\theta_h\right)^\top\Sigma_h^{-1}\nabla^j_\theta\nu^\theta_h}+2G\sqrt{C_1dm}\Vert t\Vert\sum_{h=1}^H(H-h)^2\sqrt{\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\nu^\theta_h}=B\sqrt{C_1dm}\Vert t\Vert.
\end{align*}
We have
\begin{align*}
    &\sum_{k=1}^K\textrm{Var}[e_k]=\sum_{k=1}^K\mathbb{E}\left[\left\langle\sum_{h=1}^H\nabla_\theta\left(\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right),t\right\rangle^2\right]=Kt^\top\Lambda_\theta t.
\end{align*}
We pick $\sigma^2=Kt^\top\Lambda_\theta t$, the Bernsteinâ€™s inequality implies that for any $\varepsilon\in\mathbb{R}$, 
\begin{align*}
    \mathbb{P}\left(\left\vert\sum_{k=1}^K e_k\right\vert\geq\varepsilon\right)\leq 2\exp\left(-\frac{\varepsilon^2/2}{\sigma^2+\sqrt{C_1dm}\Vert t\Vert B\varepsilon/3}\right).
\end{align*}
Therefore, if we pick $\varepsilon=\sigma\sqrt{2\log(2/\delta)}+2\log(2/\delta)\sqrt{C_1dm}\Vert t\Vert B/3$, we get
\begin{align*}
    \mathbb{P}\left(\left\vert\sum_{k=1}^K e_k\right\vert\geq\varepsilon\right)\leq\delta,
\end{align*}
i.e., we have with probability $1-\delta$, 
\begin{align*}
    \left\vert\frac{1}{K}\sum_{k=1}^K e_k\right\vert\leq\sqrt{\frac{2t^\top\Lambda_\theta t\log(2/\delta)}{K}}+\frac{2\log(2/\delta)\sqrt{C_1dm}\Vert t\Vert B}{3K}
\end{align*}
\end{proof}

\subsection{Proof of Lemma \ref{e2}}
\begin{proof}
For an arbitrarily given $\theta_0$, let $\Sigma_{\theta_0,h}=\mathbb{E}^{\pi_{\theta_0}}[\phi(s_h,a_h)\phi(s_h,a_h)^\top]$,  we have
\begin{align*}
    &\left(\left(\widehat{\nu}^\theta_h\right)^\top\widehat{\Sigma}_h^{-1}-\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\right)\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\\
    =&\left(\nu^\theta_1\right)^\top\left(\left(\prod_{h^\prime=1}^{h-1}\widehat{M}_{\theta,h^\prime}\right)\widehat{\Sigma}_h^{-1}-\left(\prod_{h^\prime=1}^{h-1}M_{\theta,h^\prime}\right)\Sigma_h^{-1}\right)\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\\
    =&\left(\Sigma_{\theta_0,1}^{-\frac{1}{2}}\nu^\theta_1\right)^\top\left(\left(\prod_{h^\prime=1}^{h-1}\Sigma_{\theta_0,h^\prime}^{\frac{1}{2}}\widehat{M}_{\theta,h^\prime}\Sigma_{\theta_0,h^\prime+1}^{-\frac{1}{2}}\right)\Sigma_{\theta_0,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\Sigma_h^{\frac{1}{2}}\widehat{\Sigma}_h^{-1}\Sigma_h^{\frac{1}{2}}-\left(\prod_{h^\prime=1}^{h-1}\Sigma_{\theta_0,h^\prime}^{\frac{1}{2}}M_{\theta,h^\prime}\Sigma_{\theta_0,h^\prime+1}^{-\frac{1}{2}}\right)\Sigma_{\theta_0,h}^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}_h\right)\\
    &\cdot\Sigma_h^{-\frac{1}{2}}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta.
\end{align*}
Taking derivatives on both sides, and let $\theta_0=\theta$, we get
\begin{align*}
    &\nabla_\theta^j E_2=\nabla_\theta^j\left(\sum_{h=1}^H\left(\left(\widehat{\nu}^\theta_h\right)^\top\widehat{\Sigma}_h^{-1}-\left(\nu^\theta_h\right)^\top\Sigma_h^{-1}\right)\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right)=E_{21}^j+E_{22}^j+E_{23}^j,
\end{align*}
where 
\begin{align*}
    E_{21}^j=&\sum_{h=1}^H\left(\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right)^\top\left(\left(\prod_{h^\prime=1}^{h-1}\Sigma_{\theta,h^\prime}^{\frac{1}{2}}\widehat{M}_{\theta,h^\prime}\Sigma_{\theta,h^\prime+1}^{-\frac{1}{2}}\right)\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\Sigma_h^{\frac{1}{2}}\widehat{\Sigma}_h^{-1}\Sigma_h^{\frac{1}{2}}-\left(\prod_{h^\prime=1}^{h-1}\Sigma_{\theta,h^\prime}^{\frac{1}{2}}M_{\theta,h^\prime}\Sigma_{\theta,h^\prime+1}^{-\frac{1}{2}}\right)\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right)\\
    &\cdot\Sigma_h^{-\frac{1}{2}}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta\\
    E_{22}^j=&\sum_{h=1}^H\left(\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right)^\top\left(\left(\prod_{h^\prime=1}^{h-1}\Sigma_{\theta,h^\prime}^{\frac{1}{2}}\widehat{M}_{\theta,h^\prime}\Sigma_{\theta,h^\prime+1}^{-\frac{1}{2}}\right)\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\Sigma_h^{\frac{1}{2}}\widehat{\Sigma}_h^{-1}\Sigma_h^{\frac{1}{2}}-\left(\prod_{h^\prime=1}^{h-1}\Sigma_{\theta,h^\prime}^{\frac{1}{2}}M_{\theta,h^\prime}\Sigma_{\theta,h^\prime+1}^{-\frac{1}{2}}\right)\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right)\\
    &\cdot\Sigma_h^{-\frac{1}{2}}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\\
    E_{23}^j=&\sum_{h=1}^H\left(\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right)^\top\\
    &\cdot\left(\left.\nabla_\theta^j\left(\prod_{h^\prime=1}^{h-1}\Sigma_{\theta_0,h^\prime}^{\frac{1}{2}}\widehat{M}_{\theta,h^\prime}\Sigma_{\theta_0,h^\prime+1}^{-\frac{1}{2}}\right)\right\vert_{\theta_0=\theta}\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\Sigma_h^{\frac{1}{2}}\widehat{\Sigma}_h^{-1}\Sigma_h^{\frac{1}{2}}-\left.\nabla_\theta^j\left(\prod_{h^\prime=1}^{h-1}\Sigma_{\theta_0,h^\prime}^{\frac{1}{2}}M_{\theta,h^\prime}\Sigma_{\theta_0,h^\prime+1}^{-\frac{1}{2}}\right)\right\vert_{\theta_0=\theta}\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right)\\
    &\cdot\Sigma_h^{-\frac{1}{2}}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta.
\end{align*}
Therefore, using the result of Lemma \ref{decomp}, we get
\begin{align*}
    \vert E_{21}^j\vert\leq&\sum_{h=1}^H\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\left(\prod_{h^\prime=1}^{h-1}\left(1+\left\Vert\Sigma_{\theta,h^\prime}^{\frac{1}{2}}\left(\Delta M_{\theta,h^\prime}\right)\Sigma_{\theta,h^\prime+1}^{-\frac{1}{2}}\right\Vert\right)\right)\left(1+\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert\right)-1\right)\\
    &\cdot\left\Vert\Sigma_h^{-\frac{1}{2}}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta\right\Vert\\
    \vert E_{22}^j\vert\leq&\sum_{h=1}^H\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\left(\prod_{h^\prime=1}^{h-1}\left(1+\left\Vert\Sigma_{\theta,h^\prime}^{\frac{1}{2}}\left(\Delta M_{\theta,h^\prime}\right)\Sigma_{\theta,h^\prime+1}^{-\frac{1}{2}}\right\Vert\right)\right)\left(1+\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert\right)-1\right)\\
    &\cdot\left\Vert\Sigma_h^{-\frac{1}{2}}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right\Vert\\
    \vert E_{23}^j\vert\leq&\sum_{h=1}^H\sum_{h^\prime=1}^{h-1}G\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\\
    &\cdot\left(\left(\prod_{h^{\prime\prime}\neq h^\prime}\left(1+\left\Vert\Sigma_{\theta,h^{\prime\prime}}^{\frac{1}{2}}\left(\Delta M_{\theta,h^{\prime\prime}}\right)\Sigma_{\theta,h^{\prime\prime}}^{-\frac{1}{2}}\right\Vert\right)\right)\left(1+\left\Vert\Sigma_{\theta,h^\prime}^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta M_{\theta,h^\prime}\right)}{G}\right)\Sigma_{\theta,h^\prime}^{-\frac{1}{2}}\right\Vert\right)\left(1+\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert\right)-1\right)\\
    &\cdot\left\Vert\Sigma_h^{-\frac{1}{2}}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right\Vert,
\end{align*}
where $\Delta\Sigma^{-1}_h=\widehat{\Sigma}^{-1}_h-\Sigma_h$ and we use the fact $\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}M_{\theta,h}\Sigma_{\theta,h+1}^{-\frac{1}{2}}\right\Vert\leq 1$ and $\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\left(\nabla_\theta^j M_{\theta,h}\right)\Sigma_{\theta,h+1}^{-\frac{1}{2}}\right\Vert\leq G$ from Lemma \ref{ineq}. Furthermore, we have
\begin{align*}
    \left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\left(\Delta M_{\theta,h}\right)\Sigma_{\theta,h+1}^{-\frac{1}{2}}\right\Vert &\leq\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma_{h+1}^{\frac{1}{2}}\Sigma_{\theta,h+1}^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta M_{\theta,h}\right)\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert=\sqrt{\kappa_1}\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta M_{\theta,h}\right)\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert\\
    &=\sqrt{\kappa_1}\left\Vert\Sigma_h^{\frac{1}{2}}\left(\widehat{\Sigma}_h^{-1}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\int_{\mathcal{A}}\phi\left(s_{h+1}^{(k)},a^\prime\right)\pi_{\theta,h+1}\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\mathrm{d}a^\prime- M_{\theta,h}\right)\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert\\
    &\leq\sqrt{\kappa_1}\left(\left(1+\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert\right)\left(1+\left\Vert\Sigma_h^{-\frac{1}{2}}\left(\Delta Y_{\theta,h}\right)\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert\right)-1\right),
\end{align*}
where $\Delta Y_{\theta,h}=\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\phi_{\theta,h+1}\left(s_{h+1}^{(k)}\right)^\top-\Sigma_h M_{\theta,h}$ and the last inequality uses Lemma \ref{decomp} again. Similarly, we have
\begin{align*}
    \left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta M_{\theta,h}\right)}{G}\right)\Sigma_{\theta,h+1}^{-\frac{1}{2}}\right\Vert\leq\sqrt{\kappa_1}\left(\left(1+\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h\right)\Sigma_h^{\frac{1}{2}}\right\Vert\right)\left(1+\left\Vert\Sigma_h^{-\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta Y_{\theta,h}\right)}{G}\right)\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert\right)-1\right).
\end{align*}
Now, define $\alpha=6\sqrt{\kappa_1}(4+\kappa_2+\kappa_3)\sqrt{\frac{C_1d\log\frac{8dmH}{\delta}}{K}}$ and pick
\begin{align*}
    K\geq 36\kappa_1(4+\kappa_2+\kappa_3)^2C_1dH^2\log\frac{8dmH}{\delta},\quad\lambda\leq C_1d\min_{h\in[H]}\sigma_{\textrm{min}}(\Sigma_h)\cdot\log\frac{8dmH}{\delta},
\end{align*}
we get $\alpha\leq\frac{1}{H}$. Using the results of Lemma \ref{dsig1}, Lemma \ref{dsig2}, Lemma \ref{dy}, we get with probability $1-2\delta$, 
\begin{align}
    \label{sig}
    \begin{aligned}
        \left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert\leq &2\left\Vert\Sigma_h^{-\frac{1}{2}}\widehat{\Sigma}_h\Sigma_h^{-\frac{1}{2}}-I_d\right\Vert\leq 2\sqrt{\frac{2C_1d\log\frac{2dH}{\delta}}{K}}+\frac{4C_1d\log\frac{2dH}{\delta}}{3K}+\frac{2\lambda\Vert\Sigma^{-1}_h\Vert}{K}\\
        \leq &4\sqrt{\frac{C_1d\log\frac{8dmH}{\delta}}{K}}\leq\alpha\leq 1, 
    \end{aligned}
\end{align}
and $\forall j\in[m]$, 
\begin{align*}
    \left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta Y_{\theta,h}\right)\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert\leq 2(\kappa_2+1)\sqrt{\frac{C_1d\log\frac{8dmH}{\delta}}{K}}\leq 1,\quad\left\Vert\Sigma_h^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta Y_{\theta,h}\right)}{G}\right)\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert\leq 2(\kappa_3+1)\sqrt{\frac{C_1d\log\frac{8dmH}{\delta}}{K}}\leq 1,
\end{align*}
which implies
\begin{align}
    \label{dm}
    \left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\left(\Delta M_{\theta,h}\right)\Sigma_{\theta,h+1}^{-\frac{1}{2}}\right\Vert\leq 2\sqrt{\kappa_1}\left(\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert+\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta Y_{\theta,h}\right)\Sigma_{h+1}^{-\frac{1}{2}}\right\Vert\right)\leq\alpha,
\end{align}
where we use the fact $(1+x_1)(1+x_2)-1\leq 2(x_1+x_2)$ whenever $x_1,x_2\in[0,1]$. Similarly, we get
\begin{align}
    \label{ddm}
    \left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta M_{\theta,h}\right)}{G}\right)\Sigma_{\theta,h+1}^{-\frac{1}{2}}\right\Vert\leq\alpha,\quad\forall j\in[m].
\end{align}
Meanwhile, by Lemma \ref{eps}, we get with probability $1-\delta$,
\begin{align}
    \label{ep1}
    \left\Vert\Sigma_h^{-\frac{1}{2}}\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right\Vert&\leq 4\sqrt{d}(H-h+1)\sqrt{\frac{\log\frac{8dmH}{\delta}}{K}}\\
    \label{ep2}
    \left\Vert\Sigma^{-\frac{1}{2}}_h\frac{1}{K}\sum_{k=1}^K\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta\right\Vert&\leq 8\sqrt{d}G(H-h)^2\sqrt{\frac{\log\frac{8dmH}{\delta}}{K}}.
\end{align}
Combining the results of \eqref{sig}, \eqref{dm}, \eqref{ddm}, \eqref{ep1},\eqref{ep2} and use a union bound, we have with probability $1-3\delta$, 
\begin{align*}
    \vert E_{21}^j\vert\leq&\sum_{h=1}^H16h\alpha\sqrt{d}G(H-h)^2\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert \sqrt{\frac{\log\frac{8dmH}{\delta}}{K}}\\
    \leq&16\alpha\sqrt{d}H^4G\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\max_{h\in[H]}\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\sqrt{\frac{\log\frac{8dmH}{\delta}}{K}},\\
    \vert E_{22}^j\vert\leq&\sum_{h=1}^H8h\alpha\sqrt{d}(H-h+1)\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\sqrt{\frac{\log\frac{8dmH}{\delta}}{K}}\\
    \leq &8\alpha\sqrt{d}H^3\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\max_{h\in[H]}\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\sqrt{\frac{\log\frac{8dmH}{\delta}}{K}}\\
    \vert E_{23}^j\vert\leq&\sum_{h=1}^H16Gh\alpha\sqrt{d}(H-h+1)(h-1)\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\sqrt{\frac{\log\frac{8dmH}{\delta}}{K}}\\
    \leq&16\alpha\sqrt{d}H^4G\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\max_{h\in[H]}\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\sqrt{\frac{\log\frac{8dmH}{\delta}}{K}},
\end{align*}
where we use the fact $(1+\alpha)^h-1\leq 2h\alpha$ whenever $\alpha h\leq 1$.  Summing up the above terms and using the definition of $\alpha$, we get
\begin{align*}
    \vert E_2^j\vert\leq 240\sqrt{\kappa_1}(4+\kappa_2+\kappa_3)\sqrt{C_1}dH^3\left(\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+HG\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\max_{h\in[H]}\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\frac{\log\frac{8dmH}{\delta}}{K},\quad\forall j\in[m]
\end{align*}
Replacing $\delta$ by $\frac{\delta}{3}$, we have finished the proof. 
\end{proof}

\subsection{Proof of Lemma \ref{e3}}
\begin{proof}
Similar to the decomposition in the proof of Lemma \ref{e2}, we have
\begin{align*}
    \left\vert E_3^j\right\vert=&\frac{\lambda}{K}\left\vert\sum_{h=1}^H\nabla_\theta^j\left(\left(\widehat{\nu}^\theta_h\right)^\top\widehat{\Sigma}_h^{-1}w_h^\theta\right)\right\vert\\
    \leq&\frac{\lambda}{K}\sum_{h=1}^H\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\prod_{h^\prime=1}^{h-1}\left(1+\left\Vert\Sigma_{\theta,h^\prime}^{\frac{1}{2}}\left(\Delta M_{\theta,h^\prime}\right)\Sigma_{\theta,h^\prime+1}^{-\frac{1}{2}}\right\Vert\right)\right)\left(1+\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert\right)\left\Vert\Sigma_h^{-1}\right\Vert\left\Vert\Sigma_h^{\frac{1}{2}}\nabla_\theta^j w_h^\theta\right\Vert\\
    +&\frac{\lambda}{K}\sum_{h=1}^H\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\prod_{h^\prime=1}^{h-1}\left(1+\left\Vert\Sigma_{\theta,h^\prime}^{\frac{1}{2}}\left(\Delta M_{\theta,h^\prime}\right)\Sigma_{\theta,h^\prime+1}^{-\frac{1}{2}}\right\Vert\right)\right)\left(1+\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert\right)\left\Vert\Sigma_h^{-1}\right\Vert\left\Vert\Sigma_h^{\frac{1}{2}}w_h^\theta\right\Vert\\
    +&\frac{\lambda}{K}\sum_{h=1}^H\sum_{h^\prime=1}^{h-1}G\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\\
    &\cdot\left(\prod_{h^{\prime\prime}\neq h^\prime}\left(1+\left\Vert\Sigma_{\theta,h^{\prime\prime}}^{\frac{1}{2}}\left(\Delta M_{\theta,h^{\prime\prime}}\right)\Sigma_{\theta,h^{\prime\prime}}^{-\frac{1}{2}}\right\Vert\right)\right)\left(1+\left\Vert\Sigma_{\theta,h^\prime}^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta M_{\theta,h^\prime}\right)}{G}\right)\Sigma_{\theta,h^\prime}^{-\frac{1}{2}}\right\Vert\right)\left(1+\left\Vert\Sigma_h^{\frac{1}{2}}\left(\Delta\Sigma_h^{-1}\right)\Sigma_h^{\frac{1}{2}}\right\Vert\right)\\
    &\cdot\left\Vert\Sigma_h^{-1}\right\Vert\left\Vert\Sigma_h^{\frac{1}{2}}w_h^\theta\right\Vert\\
    \leq&\frac{\lambda}{K}\sum_{h=1}^H\left(1+\alpha\right)^h\left\Vert\Sigma_h^{-1}\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_h^{\frac{1}{2}}\nabla_\theta^j w_h^\theta\right\Vert+\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\left\Vert\Sigma_h^{\frac{1}{2}}w_h^\theta\right\Vert+G(h-1)\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_h^{\frac{1}{2}}w_h^\theta\right\Vert\right),
\end{align*}
where $\alpha$ is defined in the same way as that in the proof of Lemma \ref{e2}. Similarly, we have $\alpha\leq\frac{1}{H}$ with probability $1-3\delta$ and we have
\begin{align*}
    \left\Vert\Sigma_h^{\frac{1}{2}}\nabla_\theta^j w_h^\theta\right\Vert^2&=\mathbb{E}\left[\left(\nabla_\theta^j Q^\theta_h\left(s_h^{(1)},a_h^{(1)}\right)\right)^2\right]\leq G^2(H-h)^4\\
    \left\Vert\Sigma_h^{\frac{1}{2}}w_h^\theta\right\Vert^2&=\mathbb{E}\left[\left(Q^\theta_h\left(s_h^{(1)},a_h^{(1)}\right)\right)^2\right]\leq(H-h+1)^2.
\end{align*}
We conclude
\begin{align*}
    \vert E_3^j\vert\leq&3\frac{\lambda}{K}\sum_{h=1}^H\left\Vert\Sigma_h^{-1}\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert G(H-h)^2+\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert(H-h+1)+G(h-1)\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert(H-h+1)\right)\\
    \leq&6\frac{\lambda}{K}H^2\max_{h\in[H]}\left\Vert\Sigma_h^{-1}\right\Vert\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert GH+\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\right)\\
    \leq&6\frac{\log\frac{8dmH}{\delta}C_1dH^2}{K}\max_{h\in[H]}\left\Vert\Sigma_{\theta,h}^{\frac{1}{2}}\Sigma_h^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nu^\theta_1\right\Vert GH+\left\Vert\Sigma_{\theta,1}^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\right).
\end{align*}
Replacing $\delta$ by $\frac{\delta}{3}$, we have finished the proof. 
\end{proof}

%\subsection*{Proof of Lemma \ref{e1_finite2}}
%\begin{proof}
%Similar to the proof of Lemma \ref{e1_finite}, we will bound the values of $e_{n1}^j=\sum_{h=1}^H\left(\nabla^j_\theta\nu^\theta_h\right)^\top\Sigma^{-1}\phi(s_n,a_n)\varepsilon_{h,n}^\theta$ and $e_{n2}^j=\sum_{h=1}^H\left(\nu^{\theta}_h\right)^\top\Sigma^{-1}\phi(s_n,a_n)\nabla_\theta^j\varepsilon_{h,n}^\theta$. We still have 
%\begin{align*}
%    \vert e_{n1}^j\vert\leq\sqrt{C_1d}\sum_{h=1}^H(H-h+1)\sqrt{\left(\nabla^j_\theta\nu^\theta_h\right)^\top\Sigma^{-1}\nabla^j_\theta\nu^\theta_h},\quad\vert e_{n2}^j\vert\leq 2G\sqrt{C_1d}\sum_{h=1}^H(H-h)^2\sqrt{\left(\nu^\theta_h\right)^\top\Sigma^{-1}\nu^\theta_h}.
%\end{align*}
%The only difference is the way we bound the variance. Let $B_1^j:=\sum_{h=1}^H(H-h+1)\sqrt{\left(\nabla^j_\theta\nu^\theta_h\right)^\top\Sigma^{-1}\nabla^j_\theta\nu^\theta_h}, \tilde{B}_1:=G\sqrt{\left(\sum_{h=1}^H h(H-h+1)\nu_h^\theta\right)^\top\Sigma^{-1}\left(\sum_{h=1}^H h(H-h+1)\nu_h^\theta\right)}$, we have
%\begin{align*}
%    &\textrm{Var}[e_{n1}^j\vert\mathcal{F}_n]=\mathbb{E}\left[\left(\sum_{h=1}^H\left(\nabla^j_\theta\nu^\theta_h\right)^\top\Sigma^{-1}\phi(s_n,a_n)\varepsilon_{h,n}^{\theta}\right)^2\vert\mathcal{F}_n\right]\\
%    =&\sum_{h_1=1}^H\sum_{h_2=1}^H\left(\left(\nabla^j_\theta\nu^\theta_{h_1}\right)^\top\Sigma^{-1}\phi(s_n,a_n)\right)\left(\left(\nabla^j_\theta\nu^\theta_{h_2}\right)^\top\Sigma^{-1}\phi(s_n,a_n)\right)\mathbb{E}\left[\varepsilon_{h_1,n}^\theta\varepsilon_{h_2,n}^\theta\vert s_n, a_n\right]
%\end{align*}
%Note that for any $h\in[H],\ n\in[N]$, 
%\begin{align*}
%    \left\vert\left(\nabla^j_\theta\nu^\theta_{h}\right)^\top\Sigma^{-1}\phi(s_n,a_n)\right\vert =& \left\vert\mathbb{E}^{\pi_\theta}\left[\phi(s_h, a_h)\Sigma^{-1}\phi(s_n, a_n)\sum_{h^\prime=1}^h\nabla_\theta^j\log\pi_\theta(a_{h^\prime}\vert s_{h^\prime})\right]\right\vert\\
%    \leq&\mathbb{E}^{\pi_\theta}\left[\phi(s_h, a_h)\Sigma^{-1}\phi(s_n, a_n)\sum_{h^\prime=1}^h\left\vert\nabla_\theta^j\log\pi_\theta(a_{h^\prime}\vert s_{h^\prime})\right\vert\right]\\
%    \leq&Gh\mathbb{E}^{\pi_\theta}\left[\phi(s_h,a_h)\Sigma^{-1}\phi(s_n,a_n)\right]\\
%    =&Gh\left(\nu^\theta_{h}\right)^\top\Sigma^{-1}\phi(s_n,a_n)
%\end{align*}
%where we use the fact $\left(\nu^\theta_h\right)^\top\Sigma^{-1}\phi(s_n, a_n)\geq 0$. Therefore, 
%\begin{align*}
%    &\textrm{Var}[e_{n1}^j\vert\mathcal{F}_n]=\mathbb{E}\left[\left(\sum_{h=1}^H\left(\nabla^j_\theta\nu^\theta_h\right)^\top\Sigma^{-1}\phi(s_n,a_n)\varepsilon_{h,n}^{\theta}\right)^2\vert\mathcal{F}_n\right]\\
%    \leq&G^2\sum_{h_1=1}^H\sum_{h_2=1}^Hh_1h_2\left(\left(\nu^\theta_{h_1}\right)^\top\Sigma^{-1}\phi(s_n,a_n)\right)\left(\left(\nu^\theta_{h_2}\right)^\top\Sigma^{-1}\phi(s_n,a_n)\right)\mathbb{E}\left[\varepsilon_{h_1,n}^\theta\varepsilon_{h_2,n}^\theta\vert s_n, a_n\right]\\
%    \leq&G^2\sum_{h_1=1}^H\sum_{h_2=1}^Hh_1h_2(H-h_1+1)(H-h_2+1)\left(\left(\nu^\theta_{h_1}\right)^\top\Sigma^{-1}\phi(s_n,a_n)\right)\left(\left(\nu^\theta_{h_2}\right)^\top\Sigma^{-1}\phi(s_n,a_n)\right)\\
%    =&G^2\left(\sum_{h=1}^H h\left(H-h+1\right)\left(\nu^\theta_h\right)^\top\Sigma^{-1}\phi(s_n,a_n)\right)^2
%\end{align*}
%Summing up over $n$, we get
%\begin{align*}
%    \sum_{n=1}^N\textrm{Var}[e_{n1}^j\vert\mathcal{F}_n]\leq&G^2\left(\sum_{h=1}^H h\left(H-h+1\right)\nu^\theta_h\right)^\top\Sigma^{-1}\left(\sum_{n=1}^N\phi(s_n,a_n)\phi(s_n,a_n)^\top\right)\Sigma^{-1}\left(\sum_{h=1}^H h\left(H-h+1\right)\nu^\theta_h\right)
%\end{align*}
%according to the result of Lemma \ref{dsig1}, we know with probability $1-\frac{\delta}{4m}$,
%\begin{align*}
%    \sum_{n=1}^N\textrm{Var}[e_{n1}^j\vert\mathcal{F}_n]\leq& G^2N\left\Vert\Sigma^{-\frac{1}{2}}\sum_{h=1}^Hh(H-h+1)\nu_h^\theta\right\Vert^2\left(1+\sqrt{\frac{2\log\frac{8md}{\delta}C_1dH}{N}}+\frac{2\log\frac{8md}{\delta}C_1dH}{3N}\right)\\
%    \leq&N\tilde{B}_1^2\left(1+\sqrt{\frac{2\log\frac{8md}{\delta}C_1dH}{N}}+\frac{2\log\frac{8md}{\delta}C_1dH}{3N}\right)
%\end{align*}
%We pick $\sigma_1^2=N\tilde{B}_1^2\left(1+\sqrt{\frac{2\log\frac{8md}{\delta}C_1dH}{N}}+\frac{2\log\frac{8md}{\delta}C_1dH}{3N}\right)$, the Freedmanâ€™s inequality implies that for any $\varepsilon\in\mathbb{R}$, 
%\begin{align*}
%    \mathbb{P}\left(\left\vert\sum_{n=1}^N e_{n1}^j\right\vert\geq\varepsilon,\sum_{n=1}^N\textrm{Var}[e_{n1}^j\vert\mathcal{F}_n]\leq\sigma_1^2\right)\leq 2\exp\left(-\frac{\varepsilon^2/2}{\sigma_1^2+\sqrt{C_1d}B_1^j\varepsilon/3}\right).
%\end{align*}
%Therefore, if we pick $\varepsilon=\sigma_1\sqrt{2\log(8m/\delta)}+2\log(8m/\delta)\sqrt{C_1d}B_1^j/3$, we get
%\begin{align*}
%    \mathbb{P}\left(\left\vert\sum_{n=1}^N e_{n1}^j\right\vert\geq\varepsilon,\ \sum_{n=1}^N\textrm{Var}[e_{n1}^j\vert\mathcal{F}_n]\leq\sigma_1^2\right)\leq\frac{\delta}{4m},
%\end{align*}
%which implies
%\begin{align*}
%    \mathbb{P}\left(\left\vert\sum_{n=1}^N e_{n1}^j\right\vert\geq\varepsilon\right)\leq\mathbb{P}\left(\left\vert\sum_{n=1}^N e_{n1}^j\right\vert\geq\varepsilon,\ \sum_{n=1}^N\textrm{Var}[e_{n1}^j\vert\mathcal{F}_n]\leq\left(\sigma_1^j\right)^2\right)+\mathbb{P}\left(\sum_{n=1}^N\textrm{Var}[e_{n1}^j\vert\mathcal{F}_n]\geq\left(\sigma_1^j\right)^2\right)\leq\frac{\delta}{2m}
%\end{align*}
%i.e., we have with probability $1-\frac{\delta}{2m}$, 
%\begin{align*}
%    \left\vert\frac{1}{N}\sum_{n=1}^N e_{n1}^j\right\vert\leq&\tilde{B}_1\left(\sqrt{\frac{2\log\frac{8m}{\delta}}{N}}+\frac{\sqrt{C_1dH}\log\frac{8md}{\delta}}{N} +\frac{2C_1dH\left(\log\frac{8md}{\delta}\right)^{\frac{3}{2}}}{3N^{\frac{3}{2}}}\right) + \frac{2\log(8m/\delta)\sqrt{C_1d}B_1^j}{3N}\\
%    \leq&\tilde{B}_1\sqrt{\frac{2\log\frac{8m}{\delta}}{N}}+\tilde{B}_1\left(\frac{\sqrt{C_1dH}\log\frac{8md}{\delta}}{N} +\frac{2C_1dH\left(\log\frac{8md}{\delta}\right)^{\frac{3}{2}}}{3N^{\frac{3}{2}}}\right)+\frac{2\log(8m/\delta)\sqrt{C_1d}B_1^j}{3N}
%\end{align*}
%Similarly, let $B_2:=\sum_{h=1}^H(H-h)^2G\sqrt{\left(\nu^\theta_h\right)^\top\Sigma^{-1}\nu^\theta_h}$ and $\tilde{B}_2=G\sqrt{\left(\sum_{h=1}^H (H-h)^2\nu_h^\theta\right)^\top\Sigma^{-1}\left(\sum_{h=1}^H h(H-h)^2\nu_h^\theta\right)}$, we have with probability $1-\frac{\delta}{2m}$, 
%\begin{align*}
%    \left\vert\frac{1}{N}\sum_{n=1}^ne_{n2}^j\right\vert\leq 2\tilde{B}_2\left(\sqrt{\frac{2\log\frac{8m}{\delta}}{N}}\right)+2B_2\left(\frac{2\sqrt{C_1dH}\log\frac{8md}{\delta}}{N}+\frac{2C_1dH\left(\log\frac{8md}{\delta}\right)^{\frac{3}{2}}}{3N^{\frac{3}{2}}}\right). 
%\end{align*}
%where we use Lemma \ref{upbd} again to get $\mathbb{E}[\left(\nabla_\theta^j\varepsilon_{h_1, n}^\theta\right)\nabla_\theta^j\varepsilon_{h_2, n}^\theta\vert \mathcal{F}_n] \leq 4(H-h)^4G^2,\ \forall h_1,h_2\in[H]$. Notice that
%\begin{align*}
%    \tilde{B}_1 + 2B_2&\leq 2\tilde{B}_1 + 2B_2\leq 2G\sum_{h=1}^H h(H-h+1)\sqrt{\left(\nu_h^\theta\right)^\top\Sigma^{-1}\nu_h^\theta} + 2G\sum_{h=1}^H (H-h)^2\sqrt{\left(\nu_h^\theta\right)^\top\Sigma^{-1}\nu_h^\theta}\\
%    &\leq 2GH\sum_{h=1}^H (H-h+1)\sqrt{\left(\nu_h^\theta\right)^\top\Sigma^{-1}\nu_h^\theta}:=2B_2^\prime
%\end{align*}
%Taking a union bound, we conclude that
%\begin{align*}
%    \vert E_1^j\vert\leq(\tilde{B}_1+2\tilde{B}_2)\sqrt{\frac{2\log\frac{8m}{\delta}}{N}}+(B_1^j+2B_2^\prime)\left(\frac{2\sqrt{C_1dH}\log\frac{8md}{\delta}}{N}+\frac{2C_1dH\left(\log\frac{8md}{\delta}\right)^{\frac{3}{2}}}{3N^{\frac{3}{2}}}\right).
%\end{align*}
%Taking a union bound over all $j\in[m]$, we have finished the proof. 
%\end{proof}

\section{Extension to Time-homogeneous Discounted MDP}
\subsection{Approach}
Our method can be easily extended to the case of time-homogeneous discounted MDP. Similar to the time-inhomogeneous case, under the setting of the time-homogeneous discounted MDP, an instance of MDP is defined by $(\mathcal{S},\mathcal{A},p,r, \xi,\gamma)$ where $\mathcal{S}$ and $\mathcal{A}$ are the state and action spaces, $\gamma\in(\frac{1}{2},1)$ is the discount factor, $p:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}_+$ is the transition probability, $r:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]$ is the reward function and $\xi:\mathcal{S}\rightarrow\mathbb{R}_+$ is the initial state distribution. Similarly, the policy $\pi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}_+$ is a distribution over the action space conditioned on an arbitrary given state $s$. We define the value function and $Q$ function by
\begin{align*}
v^\theta=\mathbb{E}\left[\sum_{h=1}^\infty\gamma^{h-1}r(s_h,a_h)\right],\quad Q^\theta(s,a)=\mathbb{E}\left[\left.\sum_{h=1}^\infty\gamma^{h-1}r(s_h,a_h)\right\vert s_1=s,a_1=a\right]
\end{align*}
Note that here the reward and Q function no longer contain the subscript $h$. We still consider the class of linear functions $\mathcal{F}$ with state-action feature $\phi$, and denote $\mathcal{P}_\theta$ as the transition operator where $\theta\in\mathbb{R}^m$ is the parameter of the policy. 
\begin{assumption}
\label{fclass_homo}
For any $f\in\mathcal{F}$, we have $\mathcal{P}_\theta f\in\mathcal{F}$, and we suppose $r\in\mathcal{F}$.
\end{assumption}
In addition, we assume that the constant function belongs to $\mathcal{F}$, i.e., there exists some $w_0$ such that $\phi(s,a)^\top w_0 = 1, \forall s\in\mathcal{S}, a\in\mathcal{A}$. Define the covariance matrix $\Sigma$ and its empirical version $\widehat{\Sigma}$ by
\begin{align*}
    \Sigma:=\mathbb{E}\left[\frac{1}{H}\sum_{h=1}^H\phi\left(s^{(1)}_h,a^{(1)}_h\right)\phi\left(s^{(1)}_h,a^{(1)}_h\right)^\top\right], \quad\widehat{\Sigma}:=\frac{1}{HK}\left(\lambda I_d+\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)}, a_h^{(k)}\right)\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\right),
\end{align*}
where $I_d\in\mathbb{R}^{d\times d}$ is the identity matrix. 
\begin{assumption}[Boundedness Conditions]\label{Boundedness_Conditions_homo}
	Assume $\Sigma$ is invertible. There exist absolute constants $C_1, G$ such that for any $(s,a)\in \mathcal{S}\times\mathcal{A},j\in[m]$, we have
	\begin{align*}
		\phi(s,a)^\top\Sigma^{-1}\phi(s,a)\leq C_1 d,\quad\left\vert\nabla_\theta^j\log\pi_\theta(a\vert s)\right\vert\leq G.
	\end{align*}
\end{assumption}
Define $\widehat{w}_r\in\mathbb{R}^{d},\widehat{M_\theta}\in\mathbb{R}^{d\times d},\widehat{\nabla_\theta M_\theta}\in\mathbb{R}^{d\times d},j\in[m]$ by
\begin{align*}
	\widehat{w}_r&:=\widehat{\Sigma}^{-1}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)r_h^{(k)},\\
	\widehat{M_\theta}&:=\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)}, a_h^{(k)}\right)\int_{\mathcal{A}}\pi_\theta\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\phi\left(s_{h+1}^{(k)}, a^\prime\right)^\top\mathrm{d}a^\prime,\\
	\widehat{\nabla_\theta^j M_\theta}&:=\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\int_{\mathcal{A}}\phi\left(s_{h+1}^{(k)}, a^\prime\right)^\top\nabla_\theta\pi_\theta\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\mathrm{d}a^\prime,\quad j\in[m].
\end{align*}
In this way, one can compute 
\begin{align*}
\widehat{Q}^\theta(\cdot,\cdot)=\phi(\cdot,\cdot)^\top\widehat{w}^\theta,\quad\widehat{\nabla_\theta^j Q^\theta}(\cdot,\cdot)=\phi(\cdot,\cdot)^\top\widehat{\nabla_\theta^j w^\theta},
\end{align*}
where 
\begin{align*}
\widehat{w}^\theta=\left(I_d-\gamma\widehat{M}_\theta\right)^{-1}\widehat{w}_r,\quad\widehat{\nabla_\theta^j w^\theta}=\left(I_d-\gamma\widehat{M}_\theta\right)^{-1}\widehat{\nabla_\theta^j M_\theta}\widehat{w}^\theta.
\end{align*}
Then the estimator is derived from
\begin{align*}
    &\widehat{\nabla_\theta v_\theta}=\int_{\mathcal{S}\times\mathcal{A}}\xi(s)\pi_\theta(a\vert s)\left(\widehat{\nabla_\theta Q^\theta}(s,a)+\left(\nabla_\theta\log\pi_\theta(a\vert s)\right)\widehat{Q}^\theta(s,a)\right)\mathrm{d}s\mathrm{d}a.
\end{align*}

\subsection{Results}
Define $\nu^\theta_h:=\mathbb{E}^{\pi_\theta}\left[\phi(s_h,a_h)\vert s_1\sim\xi\right], \nu^\theta=\sum_{h=1}^\infty\gamma^{h-1}\nu^\theta_h$ and $\Sigma_\theta:=\mathbb{E}^{\pi_\theta}\left[\left.\phi(s,a)\phi(s,a)^\top\right\vert s\sim\xi_\theta, a\sim\pi_\theta(\cdot\vert s)\right]$ where $\xi_\theta$ is the stationary distribution under $\pi_\theta$. Define
\begin{align*}
\phi_\theta(s)=\int_{\mathcal{A}}\pi_\theta(a^\prime\vert s)\phi(s,a^\prime)\mathrm{d}a^\prime,\quad\varepsilon_{h,k}^\theta=Q^\theta\left(s_h^{(k)},a_h^{(k)}\right)-r_h^{(k)}-\gamma\int_{\mathcal{A}}\pi_\theta\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)Q^\theta\left(s_{h+1}^{(k)},a^\prime\right)\mathrm{d}a^\prime,     
\end{align*}
and 
\begin{align*}
   &\Lambda_\theta=\mathbb{E}\left[\frac{1}{H}\sum_{h=1}^H\left(\nabla_\theta\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu^\theta\right)\right)^\top\nabla_\theta\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu^\theta\right)\right].
\end{align*}
We first give the finite sample guarantee. 
\begin{theorem}[Finite Sample Guarantee] 
\label{thm2_var_homo}
For any $t\in\mathbb{R}^m$, when $K\geq 36\kappa_1(4+\kappa_2+\kappa_3)^2C_1d(1-\gamma)^{-2}\log\frac{16dmH}{\delta}$ and $\lambda\leq\log\frac{8dmH}{\delta}C_1d\sigma_{\min}(\Sigma)$, with probability $1-\delta$, we have
\begin{align*}
    &\vert\langle t, \widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\rangle \vert\leq  \sqrt{\frac{2t^\top\Lambda_\theta t}{HK}\cdot \log\frac{8}{\delta}}+\frac{C_\theta\Vert t\Vert\log\frac{32mdH}{\delta}}{HK},
\end{align*}
where $C_\theta=240C_1dm^{0.5}(1-\gamma)^{-3}\kappa_1(5+\kappa_2+\kappa_3)\left(\max_{j\in[m]}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+\frac{G}{1-\gamma}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)$ and
\begin{align*}
    \kappa_1&=\frac{\sigma_{\max}\left(\Sigma^{-\frac{1}{2}}\Sigma_\theta\Sigma^{-\frac{1}{2}}\right)}{\sigma_{\min}\left(\Sigma^{-\frac{1}{2}}\Sigma_\theta\Sigma^{-\frac{1}{2}}\right)\wedge 1},\quad\kappa_2=\left\Vert\Sigma^{-\frac{1}{2}}\mathbb{E}\left[\phi_\theta\left(s_{h+1}^{(1)}\right)\phi_\theta\left(s_{h+1}^{(1)}\right)^\top\right]\Sigma^{-\frac{1}{2}}\right\Vert^{\frac{1}{2}},\\
    \kappa_3&=\frac{1}{G}\max_{j\in[m]}\left\Vert\Sigma^{-\frac{1}{2}}\mathbb{E}\left[\left(\nabla_\theta^j\phi_\theta\left(s_{h+1}^{(1)}\right)\right)\left(\nabla_\theta^j\phi_\theta\left(s_{h+1}^{(1)}\right)\right)^\top\right]\Sigma^{-\frac{1}{2}}\right\Vert^{\frac{1}{2}}.
\end{align*}
\end{theorem}
\begin{theorem}[Finite Sample Guarantee - Reward Free]
\label{thm2_homo}
Let the conditions in Theorem \ref{thm2_var_homo} hold, with probability $1-\delta$, we have for any reward function $r$,
\begin{align*}
&\left\vert\widehat{\nabla_\theta^j v_\theta}-\nabla_\theta^j v_\theta\right\vert\leq 4b_\theta\sqrt{\frac{\log\frac{8m}{\delta}}{HK}}+\frac{2C_\theta\log\frac{32mdH}{\delta}}{HK}, \quad\forall j\in[m],
\end{align*}
where $b_\theta=\frac{G}{(1-\gamma)^2}\left\Vert\Sigma^{-\frac{1}{2}}\nu^\theta\right\Vert+\frac{1}{1-\gamma}\left\Vert\Sigma^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta\right\Vert$ and $C_\theta$ is the same as that in Theorem \ref{thm2_var_homo}. If we in addition have $\phi(s^\prime,a^\prime)^\top\Sigma^{-1}\phi(s,a)\geq 0,\forall (s,a),(s^\prime,a^\prime)\in\mathcal{S}\times\mathcal{A}$, we have
\begin{align*}
\left\vert\widehat{\nabla_\theta^j v_\theta}-\nabla_\theta^j v_\theta\right\vert\leq \frac{16G}{(1-\gamma)^2}\sqrt{\frac{\log\frac{8m}{\delta}}{HK}}\left\Vert\Sigma^{-\frac{1}{2}}\nu^\theta\right\Vert\log(C_1d)+\frac{2C_\theta\sqrt{m}\log\frac{32mdH}{\delta}}{HK},\quad\forall j\in[m].
\end{align*}
\end{theorem}
The complete proofs of Theorem \ref{thm2_var_homo} and Theorem \ref{thm2_homo} are deferred to Appendix \ref{pfthm2_var_homo} and \ref{pfthm2_homo}. Next we show that FPG is an asymptotically normal and efficient estimator. 
\begin{theorem}[Asymptotic Normality]
\label{thm1_homo}
The FPG estimator is asymptotically normal:
\begin{align*}
    \sqrt{HK}\left(\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\right)\stackrel{d}{\rightarrow}\mathcal{N}(0,\Lambda_\theta).
\end{align*}
\end{theorem}
The proof of Theorem \ref{thm1_homo} is deferred to Appendix \ref{pfthm1_homo}. An obvious corollary of Theorem \ref{thm1_homo} is that for any vector $t \in \mathbb{R}^m,$ 
\begin{equation*}
    \sqrt{HK}\left\langle t,  \widehat{\nabla_{\theta} v_{\theta}} -\nabla_{\theta} v_{\theta} \right\rangle \stackrel{d}{\rightarrow} \mathcal{N}\left(0, t^{\top} \Lambda_{\theta} t\right).
\end{equation*}
The following theorem states the Cramer Rao bound for FPG estimation. 
\begin{theorem}
\label{thm4_homo}
Let Assumption \ref{fclass_homo} hold. For any vector $t\in\mathbb{R}^m$, the variance of any unbiased estimator for $t^{\top}\nabla_{\theta}v_{\theta}  \in \mathbb{R}$ is lower bounded by $\frac{1}{\sqrt{HK}}t^{\top} \Lambda_{\theta} t$.
\end{theorem}
The proof of Theorem \ref{thm4_homo} is deferred to Appendix \ref{pfthm4_homo}. 

\subsection{Additional Notations}
Define
\begin{align*}
\widehat{r}(\cdot,\cdot)&:= \phi(\cdot,\cdot)^\top \widehat{w}_r,\\
U^\theta &:=\gamma\mathcal{P}_\theta\left(\nabla_\theta\log\Pi_\theta\right) Q^\theta,\\
\widehat{U}^\theta&:=\gamma\widehat{\mathcal{P}}_\theta\left(\nabla_\theta\log\Pi_\theta\right)Q^\theta,\\
\tilde{U}^\theta&:=\gamma\widehat{\mathcal{P}}_\theta\left(\nabla_\theta\log\Pi_\theta\right)\widehat{Q}^\theta\\
\Delta Y_\theta&:=\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\phi_\theta\left(s_{h+1}^{(k)}\right)^\top-\Sigma M_\theta\\
\Delta\Sigma^{-1}&:=\widehat{\Sigma}^{-1}-\Sigma.
\end{align*}
When $\mathcal{F}$ is the class of the linear functions, there exists matrix $M_\theta$ such that the transition probability satisfies
\begin{align*}
    \mathbb{E}^{\pi_\theta}\left[\phi(s^\prime,a^\prime)^\top\vert s,a\right] = \phi(s,a)^\top M_\theta. 
\end{align*}

\subsection{Technical Lemmas}
\begin{lemma}
\label{Q_decomp_base_homo}
We have
\begin{align*}
    Q^\theta=\sum_{h=1}^\infty\gamma^{h-1}\left(\mathcal{P}_\theta\right)^{h-1}r,\quad\nabla_\theta Q^\theta=\sum_{h=1}^\infty\gamma^{h-1}\left(\mathcal{P}_\theta\right)^{h-1}U^\theta.
\end{align*}
\end{lemma}
\begin{proof}
By Bellman's equation, we have $Q^\theta=r+\gamma\mathcal{P}_\theta Q^\theta$, which implies
\begin{align*}
Q^\theta = r+\gamma\mathcal{P}_\theta Q^\theta = r+\gamma\mathcal{P}_\theta r + \gamma^2\left(\mathcal{P}_\theta\right)^2 Q^\theta = \ldots = \sum_{h=1}^\infty\gamma^{h-1}\left(\mathcal{P}_\theta\right)^{h-1}r,
\end{align*}
which proves the first equation. Differentiating on both sides of the Bellman's equation w.r.t. $\theta$, we have
\begin{align*}
    \nabla_\theta Q^\theta(s,a)=\gamma\mathbb{E}^{\pi_\theta}\left[\left(\nabla_\theta\log\pi_\theta(a^\prime\vert s^\prime)\right)Q^\theta(s^\prime,a^\prime)\vert s,a\right]+\gamma\mathbb{E}^{\pi_\theta}\left[\nabla_\theta Q^\theta(s^\prime,a^\prime)\vert s,a\right],
\end{align*}
i.e., $\nabla_\theta Q^\theta=U^\theta+\gamma\mathcal{P}_\theta\nabla_\theta Q^\theta$. By induction, we have proved the second equation. 
\end{proof}

The decomposition leads to the following boundedness result: 
\begin{lemma}
\label{upbd_homo}
We have $\vert Q^\theta(s,a)\vert\leq \frac{1}{1-\gamma},\ \left\Vert\nabla_\theta Q^\theta(s,a)\right\Vert_\infty\leq\frac{G}{(1-\gamma)^2},\ \forall s\in\mathcal{S},a\in\mathcal{A}.$
\end{lemma}

\begin{lemma}
\label{decomp_homo}
For any series of matrices $A_1,A_2,\ldots,A_n$ and $\Delta A_1,\Delta A_2,\ldots\Delta A_n$, we have
\begin{align*}
    \left\Vert\prod_{i=1}^n(A_i+\Delta A_i)-\prod_{i=1}^n A_i\right\Vert\leq \prod_{i=1}^n\left(\Vert A_i\Vert+\Vert\Delta A_i \Vert\right)-\prod_{i=1}^n\Vert A_i\Vert.
\end{align*}
\end{lemma}
\begin{proof}
We have
\begin{align*}
    \left\Vert\prod_{i=1}^n(A_i+\Delta A_i)-\prod_{i=1}^n A_i\right\Vert&= \left\Vert\sum_{\delta\in\{0,1\}^n\setminus\{(1,1,\ldots,1)\}}\prod_{i=1}^n A_i^{\delta_i}(\Delta A_i)^{1-\delta_i}\right\Vert\leq\sum_{\delta\in \{0,1\}^n\setminus\{(1,1,\ldots,1)\}}\prod_{i=1}^n\Vert A_i\Vert^{\delta_i}\Vert\Delta A_i\Vert^{1-\delta_i}\\
    &=\prod_{i=1}^n\left(\Vert A_i\Vert+\Vert\Delta A_i\Vert\right)-\prod_{i=1}^n\Vert A_i\Vert.
\end{align*}
\end{proof}
The following lemma gives an upper bound on the 2-norm of $M_\theta$ and its derivatives. 
\begin{lemma}
\label{ineq_homo}
We have $\left\Vert\Sigma_\theta^{\frac{1}{2}}M_\theta\Sigma_\theta^{-\frac{1}{2}}\right\Vert \leq 1$ and $\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\nabla_\theta^j M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\leq G,\ \forall j\in[m]$.
\end{lemma}
\begin{proof}
Note that for any $f:\mathcal{S}\times\mathcal{A}\rightarrow \mathbb{R},\ f(s,a):=\mu^\top\phi(s, a)$, and any fixed $h\in\mathbb{N}_+$, we have
\begin{align*}
    \mathbb{E}^{\pi_\theta}\left[f^2(s_{h+1},a_{h+1})\vert s_1\sim\xi_\theta\right]&=\mathbb{E}^{\pi_\theta}\left[\mathbb{E}^{\pi_\theta}\left[f^2(s_{h+1},a_{h+1})\vert s_h,a_h\right]\vert s_1\sim\xi_\theta\right]\\
    &\geq\mathbb{E}^{\pi_\theta}[\mathbb{E}^{\pi_\theta}[f(s_{h+1},a_{h+1})\vert s_h,a_h]^2\vert  s_1\sim\xi_\theta].
\end{align*}
The LHS satisfies
\begin{align*}
    \mathbb{E}^{\pi_\theta}[f^2(s_{h+1},a_{h+1})\vert s_1\sim\xi_\theta]&=\mu^\top\Sigma_\theta\mu,
\end{align*}
and the RHS satisfies
\begin{align*}
    \mathbb{E}^{\pi_\theta}[\mathbb{E}^{\pi_\theta}[f(s_{h+1},a_{h+1})\vert s_h,a_h]^2\vert s_1\sim\xi_\theta]=\mathbb{E}^{\pi_\theta}[\mu^\top M_\theta^\top\phi(s_h,a_h)\phi(s_h,a_h)^\top M_\theta\mu\vert s_1\sim\xi_\theta]=\mu^\top M_\theta^\top\Sigma_\theta M_\theta\mu.
\end{align*}
Therefore, we have $\mu^\top\Sigma_\theta\mu\geq\mu^\top M_\theta^\top\Sigma_\theta M_\theta \mu,\ \forall\mu$, which implies $\Vert\Sigma_\theta^{\frac{1}{2}}M_\theta \Sigma_\theta^{-\frac{1}{2}}\Vert\leq 1$. Similarly, let $g:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R},\ g(s, a):=\left(\nabla_\theta^j\log\pi_\theta(s,a)\right)\mu^\top\phi(s,a)$, we have
\begin{align*}
    \mathbb{E}^{\pi_\theta}[g^2(s_{h+1},a_{h+1})\vert s_1\sim\xi_\theta]=&\mathbb{E}^{\pi_\theta}[\mathbb{E}^{\pi_\theta}[g^2(s_{h+1},a_{h+1})\vert s_h,a_h]\vert s_1\sim\xi_\theta]\\
    \geq&\mathbb{E}^{\pi_\theta}[\mathbb{E}^{\pi_\theta}[g(s_{h+1},a_{h+1})\vert s_h,a_h]^2\vert s_1\sim\xi_\theta].
\end{align*}
The LHS satisfies
\begin{align*}
    \mathbb{E}^{\pi_\theta}[g^2(s_{h+1},a_{h+1})\vert s_1\sim\xi_\theta]&=\mu^\top\mathbb{E}^{\pi_\theta}\left[\left(\nabla_\theta^j\log\pi_\theta(a\vert s)\right)^2\phi(s_{h+1},a_{h+1})\phi(s_{h+1},a_{h+1})^\top\vert s_1\sim\xi_\theta\right]\mu\\
    &\leq G^2\mu^\top\mathbb{E}^{\pi_\theta}\left[\phi(s_{h+1},a_{h+1})\phi(s_{h+1},a_{h+1})^\top\vert s_1\sim\xi_\theta\right]\mu=G^2\mu^\top\Sigma_\theta\mu,
\end{align*}
and the RHS satisfies 
\begin{align*}
    &\mathbb{E}^{\pi_\theta}[\mathbb{E}^{\pi_\theta}[g(s_{h+1},a_{h+1})\vert s_h, a_h]^2\vert s_1\sim\xi_\theta]\\
    =&\mathbb{E}^{\pi_\theta}[\mu^\top\left(\nabla_\theta^j M_\theta\right)^\top\phi(s_h,a_h)\phi(s_h,a_h)^\top\left(\nabla_\theta^j M_\theta\right)\mu\vert s_1\sim\xi_\theta]\\
    =&\mu^\top\left(\nabla_\theta^j M_\theta\right)^\top\Sigma_\theta\left(\nabla_\theta^j M_\theta\right)\mu.
\end{align*}
Therefore, we get $G^2\mu^\top\Sigma_\theta\mu\geq\mu^\top\left(\nabla_\theta^j M_\theta\right)^\top\Sigma_\theta\left(\nabla_\theta^j M_\theta\right)\mu,\ \forall\mu$, which implies $\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\nabla_\theta^j M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\leq G$.
\end{proof}

\subsection{Probabilistic Events}
We define the following probabilistic events:
{\small
\begin{align*}
\mathcal{E}_{\Sigma} &:= \left\{\left\Vert\Sigma^{-\frac{1}{2}}\left(\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)}, a_h^{(k)}\right)\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\right)\Sigma^{-\frac{1}{2}}-I_d\right\Vert\leq \sqrt{\frac{2C_1d\log\frac{8dH}{\delta}}{K}} + \frac{2C_1d\log\frac{8dH}{\delta}}{3K}\right\},\\
\mathcal{E}_{Y,0}&:=\left\{\left\Vert\Sigma^{-\frac{1}{2}}\left(\Delta Y_\theta\right)\Sigma^{-\frac{1}{2}}\right\Vert\leq\left(\kappa_2\vee 1\right)\sqrt{\frac{2C_1d\log\frac{16dH}{\delta}}{K}} + \frac{4C_1d\log\frac{16dH}{\delta}}{3K}\right\},\\
\mathcal{E}_{Y,j}&:=\left\{\left\Vert\Sigma^{-\frac{1}{2}}\left(\nabla_\theta^j\left(\Delta Y_\theta\right)\right)\Sigma^{-\frac{1}{2}}\right\Vert\leq\left(\kappa_3\vee 1\right)G\sqrt{\frac{2C_1d\log\frac{16mdH}{\delta}}{K}}+\frac{4C_1dG\log\frac{16mdH}{\delta}}{3K}\right\}, j\in[m],\\
\mathcal{E}_{Y}&:=\bigcap_{j=0}^m\mathcal{E}_{Y,j}\\
    \mathcal{E}_{\varepsilon,0}&:=\left\{\left\Vert\Sigma^{-\frac{1}{2}}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right\Vert\leq\frac{\sqrt{d}}{1-\gamma}\left(\sqrt{\frac{2\log\frac{32dH}{\delta}}{KH}}+\frac{2\sqrt{C_1d}\log\frac{32dH}{\delta}}{K\sqrt{H}}+\frac{2C_1d\left(\log\frac{32dH}{\delta}\right)^{\frac{3}{2}}}{3K^{\frac{3}{2}}\sqrt{H}}\right)\right\}\\
    \mathcal{E}_{\varepsilon,j}&:=\left\{\left\Vert\Sigma^{-\frac{1}{2}}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta\right\Vert\leq \frac{2\sqrt{d}G}{(1-\gamma)^2}\left(\sqrt{\frac{2\log\frac{32mdH}{\delta}}{KH}}+\frac{2\sqrt{C_1d}\log\frac{32mdH}{\delta}}{K\sqrt{H}}+\frac{2C_1d\left(\log\frac{32mdH}{\delta}\right)^{\frac{3}{2}}}{3K^{\frac{3}{2}}\sqrt{H}}\right)\right\}, j\in[m],\\
    \mathcal{E}_{\varepsilon} &:= \bigcap_{j=0}^m\mathcal{E}_{\varepsilon,j},\\
    \mathcal{E} &:= \mathcal{E}_\Sigma\cap\mathcal{E}_Y\cap\mathcal{E}_\varepsilon.
\end{align*}}
We have the following guarantees on the above high probability events: 
\begin{lemma}
\label{dsig1_homo}
$\mathbb{P}\left(\mathcal{E}_\Sigma\right) \geq 1-\frac{\delta}{4}$.
\end{lemma}
\begin{proof}
Define
\begin{align*}
	X^{(k)}=\frac{1}{H}\sum_{h=1}^H\Sigma^{-\frac{1}{2}}\phi\left(s^{(k)}_h,a^{(k)}_h\right) \phi\left(s^{(k)}_h,a^{(k)}_h\right)^\top\Sigma^{-\frac{1}{2}}\in\mathbb{R}^{d\times d}.
\end{align*}
It's easy to see that $X^{(1)},X^{(2)},\ldots,X^{(K)}$ are independent and $\mathbb{E}\left[X^{(k)}\right]=I_d$. In the remaining part of the proof, we will apply the matrix Bernstein's inequality to analyze the concentration of $\frac{1}{K}\sum_{k=1}^K X^{(k)}$. We first consider the matrix-valued variance $\textrm{Var}\left(X^{(k)}\right)=\mathbb{E}\left[\left(X^{(k)}-I_d\right)^2\right]=\mathbb{E}\left[\left(X^{(k)}\right)^2\right]-I_d$. Let
\begin{align*}
\Phi^{(k)}:=\left[\phi\left(s^{(k)}_1,a^{(k)}_1\right),\phi\left(s^{(k)}_2,a^{(k)}_2\right),\ldots,\phi\left(s^{(k)}_H,a^{(k)}_H\right)\right]\in\mathbb{R}^{d\times H},
\end{align*}
Then $X^{(k)}=\frac{1}{H}\Sigma^{-\frac{1}{2}}\Phi^{(k)}\left(\Phi^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}$. For any vector $\mu\in\mathbb{R}^d$,
\begin{align*} 
	\mu^\top\mathbb{E}\left[\left(X^{(k)}\right)^2\right]\mu=&\mathbb{E}\left[\left\Vert X^{(k)}\mu\right\Vert^2\right]=\frac{1}{H^2}\mathbb{E}\left[\left\Vert\Sigma^{-\frac{1}{2}}\Phi^{(k)}\left(\Phi^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}\mu\right\Vert^2\right]\\
	\leq&\frac{1}{H^2}\mathbb{E}\left[\left\Vert\Sigma^{-\frac{1}{2}}\Phi^{(k)}\right\Vert^2\left\Vert\left(\Phi^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}\mu\right\Vert^2\right]\leq \frac{C_1d}{H}\mathbb{E}\left[\left\Vert\left(\Phi^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}\mu\right\Vert^2\right]\\
    =&C_1d\mu^\top\mathbb{E}\left[X^{(k)}\right]\mu = C_1d \Vert\mu\Vert^2,
\end{align*}
where we used the identity $\left\Vert\left(\Phi^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}\mu\right\Vert^2=\mu^\top X^{(k)}\mu$ and $\mathbb{E}\left[X^{(k)}\right]=I_d$. We have
\begin{align*}
    \textrm{Var}(X^{(k)})\preceq\mathbb{E}\left[\left(X^{(k)}\right)^2\right]\preceq C_1dI_d. 
\end{align*}
Additionally,
\begin{align*}
	-I_d\preceq X^{(k)}-I_d=\frac{1}{H}\sum_{h=1}^H\Sigma^{-\frac{1}{2}}\phi\left(s^{(k)}_h,a^{(k)}_h\right)\phi\left(s^{(k)}_h,a^{(k)}_h\right)^\top\Sigma^{-\frac{1}{2}}-I_d \preceq C_1dI_d-I_d. 
\end{align*}
Therefore, $\Vert X^{(k)}-I_d\Vert\leq C_1d$. Since $X^{(1)},X^{(2)},\ldots,X^{(K)}$ are \textit{i.i.d.}, by the matrix-form Bernstein inequality, we have
\begin{align*}
	\mathbb{P}\left(\left\Vert\sum_{k=1}^K X^{(k)}-I_d\right\Vert\geq\varepsilon\right)\leq 2d\cdot\exp\left(-\frac{\varepsilon^2/2}{C_1dK+C_1d \varepsilon/3}\right),\quad \forall \varepsilon>0, 
\end{align*}
i.e., with probability at least $1-\frac{\delta}{4}$,
\begin{align*}
    \left\Vert\frac{1}{K}\sum_{k=1}^K\left(X^{(k)}-I_d\right)\right\Vert\leq \sqrt{\frac{2C_1d\log\frac{8d}{\delta}}{K}}+\frac{2C_1d\log\frac{8d}{\delta}}{3K}, 
\end{align*}
which has finished the proof. 
\end{proof}
 
\begin{lemma}
\label{dy_homo}
$\mathbb{P}\left(\mathcal{E}_{Y}\right)\geq 1-\frac{\delta}{4}$. 
\end{lemma}
\begin{proof}
Take
\begin{align*}
	Y_\theta^{(k)}:=\frac{1}{H}\sum_{h=1}^H\Sigma^{-\frac{1}{2}}\phi\left(s^{(k)}_h,a^{(k)}_h\right)\phi_\theta\left(s^{(k)}_{h+1}\right)^\top\Sigma^{-\frac{1}{2}},\quad\forall k\in[K]. 
\end{align*}
Then, $\Sigma^{-\frac{1}{2}}\left(\Delta Y_\theta\right)\Sigma^{-\frac{1}{2}}=\frac{1}{K}\sum_{k=1}^K\left(Y_\theta^{(k)}-\Sigma^{\frac{1}{2}}M_\theta\Sigma^{-\frac{1}{2}}\right)$. Note that
\begin{align}
    \label{SMS0_homo} 
    \begin{aligned} 
        \mathbb{E}\left[Y_\theta^{(k)}\right]=&\frac{1}{H}\sum_{h=1}^H\mathbb{E}\left[\Sigma^{-\frac{1}{2}}\phi\left(s^{(k)}_h,a^{(k)}_h\right)\phi_\theta\left(s^{(k)}_{h+1}\right)^\top\Sigma^{-\frac{1}{2}}\right]\\
        =&\frac{1}{H}\sum_{h=1}^H\mathbb{E}\left[\Sigma^{-\frac{1}{2}}\phi\left(s^{(k)}_h,a^{(k)}_h\right)\mathbb{E}^{\pi_\theta}\left[\phi\left(s^\prime,a^\prime\right)^\top\vert s^{(k)}_h,a^{(k)}_h\right]\Sigma^{-\frac{1}{2}}\right]\\
        =&\frac{1}{H}\sum_{h=1}^H\mathbb{E}\left[\Sigma^{-\frac{1}{2}}\phi\left(s^{(k)}_h,a^{(k)}_h\right)\phi\left(s^{(k)}_h,a^{(k)}_h\right)^\top M_\theta\Sigma^{-\frac{1}{2}}\right]=\Sigma^{\frac{1}{2}}M_\theta\Sigma^{-\frac{1}{2}}, 
    \end{aligned}
\end{align}
To this end, $\Sigma^{-\frac{1}{2}}\left(\Delta Y_\theta\right)\Sigma^{-\frac{1}{2}}=\frac{1}{K}\sum_{k=1}^K\left(Y_\theta^{(k)}-\mathbb{E}\left[Y_\theta^{(k)}\right]\right)$. Since the trajectories are \textrm{i.i.d.}, we use the matrix-form Bernstein inequality to estimate $\left\Vert\Sigma^{-\frac{1}{2}}(\Delta Y_\theta)\Sigma^{-\frac{1}{2}}\right\Vert$. Let
\begin{align*}
\Phi^{(k)}&:=\left[\phi\left(s^{(k)}_1,a^{(k)}_1\right),\phi\left(s^{(k)}_2,a^{(k)}_2\right),\ldots,\phi\left(s^{(k)}_H,a^{(k)}_H\right)\right]\in\mathbb{R}^{d\times H},\\
\Phi^{(k)}_\theta&:=\left[\phi_\theta\left(s^{(k)}_2\right),\phi_\theta\left(s^{(k)}_3\right),\ldots,\phi_\theta\left(s^{(k)}_{H+1}\right)\right]\in\mathbb{R}^{d\times H},
\end{align*}
We have $Y^{(k)}_\theta=\frac{1}{H}\Sigma^{-\frac{1}{2}}\Phi^{(k)}\left(\Phi^{(k)}_\theta\right)^\top\Sigma^{-\frac{1}{2}}$. For any $\mu\in\mathbb{R}^d$, we have
\begin{align*}
    \mu^\top\mathbb{E}\left[Y_\theta^{(k)}\left(Y_\theta^{(k)}\right)^\top\right]\mu=&\mathbb{E}\left[\left\Vert\left(Y_\theta^{(k)}\right)^\top\mu\right\Vert^2\right]=\frac{1}{H^2}\mathbb{E}\left[\left\Vert\Sigma^{-\frac{1}{2}}\Phi_\theta^{(k)}\left(\Phi^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}\mu\right\Vert^2\right]\\
    \leq&\frac{1}{H^2}\mathbb{E}\left[\left\Vert\Sigma^{-\frac{1}{2}}\Phi_\theta^{(k)}\right\Vert^2\left\Vert\left(\Phi^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}\mu\right\Vert^2\right]\\
    \leq&\frac{C_1d}{H}\mathbb{E}\left[\left\Vert\left(\Phi^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}\mu\right\Vert^2\right]\\
    =&\frac{C_1d}{H}\mu^\top\Sigma^{-\frac{1}{2}}\mathbb{E}\left[\Phi^{(k)}\left(\Phi^{(k)}\right)^\top\right]\Sigma^{-\frac{1}{2}}\mu\\
    =&C_1d\Vert\mu\Vert^2,
\end{align*}
where we have used the fact $\Sigma=\frac{1}{H}\mathbb{E}\left[\Phi^{(k)}\left(\Phi^{(k)}\right)^\top\right]$. It follows that
\begin{align*} 
    \textrm{Var}_1\left(Y_\theta^{(k)}\right):=&\mathbb{E}\left[\left(Y_\theta^{(k)}-\mathbb{E}\left[Y_\theta^{(k)}\right]\right)\left(Y_\theta^{(k)}-\mathbb{E}\left[Y_\theta^{(k)}\right]\right)^\top\right]\preceq\mathbb{E}\left[Y_\theta^{(k)}\left(Y_\theta^{(k)}\right)^\top\right]\preceq C_1dI_d. 
\end{align*} 
Analogously,
\begin{align*} 
	\textrm{Var}_2\left(Y_\theta^{(k)}\right):=&\mathbb{E}\left[\left(Y_\theta^{(k)}-\mathbb{E}\left[Y_\theta^{(k)}\right]\right)^\top\left(Y_\theta^{(k)}-\mathbb{E}\left[Y_\theta^{(k)}\right]\right)\right]\preceq\mathbb{E}\left[\left(Y_\theta^{(k)}\right)^\top Y_\theta^{(k)}\right]\\
	\preceq& \frac{C_1d}{H}\Sigma^{-\frac{1}{2}}\mathbb{E}\left[\Phi^{(k)}_\theta\left(\Phi^{(k)}_\theta\right)^\top\right]\Sigma^{-\frac{1}{2}}. 
\end{align*}
Therefore, $\max\left\{\left\Vert\textrm{Var}_1\left(Y_\theta^{(k)}\right)\right\Vert, \left\Vert\textrm{Var}_2\left(Y_\theta^{(k)}\right)\right\Vert\right\}\leq C_1d\left(\kappa_2^2\vee 1\right)$. It also holds that $\Vert Y_\theta^{(k)}\Vert\leq C_1d$. Hence,
\begin{align*}
    \left\Vert Y_\theta^{(k)}-\Sigma^{\frac{1}{2}}M_\theta\Sigma^{-\frac{1}{2}}\right\Vert\leq 2C_1d. 
\end{align*}
Applying Matrix Bernstein's inequality, we derive for any $\varepsilon>0$,
\begin{align*}
    \mathbb{P}\left(\left\Vert\sum_{k=1}^K\left(Y_\theta^{(k)}-\Sigma^{\frac{1}{2}}M_\theta\Sigma^{-\frac{1}{2}}\right)\right\Vert>\varepsilon\right)\leq 2d\exp\left(-\frac{\varepsilon^2/2}{C_1dK\left(\kappa_2^2\vee 1\right)+2C_1d\varepsilon/3}\right), 
\end{align*}
which implies $\mathcal{E}_{Y,0}$ holds with probability $1-\frac{\delta}{8}$. For $\mathcal{E}_{Y,j},j\in[m]$, notice that for any $j\in[m]$, we have $\Sigma^{-\frac{1}{2}}(\nabla_\theta^j(\Delta Y_\theta))\Sigma^{-\frac{1}{2}}=\frac{1}{K}\sum_{k=1}^K\left(\nabla_\theta^j Y_\theta^{(k)}-\mathbb{E}\left[\nabla_\theta^j Y_\theta^{(k)}\right]\right)$, and $\nabla_\theta^j Y_\theta^{(k)}=\frac{1}{H}\Sigma^{-\frac{1}{2}}\Phi^{(k)}\left(\nabla_\theta^j\Phi_\theta^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}$. For any $\mu\in\mathbb{R}^d$, we have
\begin{align*}
    \mu^\top\mathbb{E}\left[\left(\nabla_\theta^j Y_\theta^{(k)}\right)\left(\nabla_\theta^j Y_\theta^{(k)}\right)^\top\right]\mu=&\frac{1}{H^2}\mathbb{E}\left[\left\Vert\Sigma^{-\frac{1}{2}}\left(\nabla_\theta^j\Phi_\theta^{(k)}\right)\left(\Phi^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}\mu\right\Vert^2\right]\\
    \leq&\frac{1}{H^2}\mathbb{E}\left[\left\Vert\Sigma^{-\frac{1}{2}}\nabla_\theta^j\Phi_\theta^{(k)}\right\Vert^2\left\Vert\left(\Phi^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}\mu\right\Vert^2\right]. 
\end{align*}
Since we have 
\begin{align*}
    &\left(\nabla_\theta^j\phi_\theta\left(s_{h+1}^{(k)}\right)\right)^\top\Sigma^{-1}\nabla_\theta^j\phi_\theta\left(s_{h+1}^{(k)}\right)\\
    =&\int_{\mathcal{A}\times\mathcal{A}}\pi_\theta\left(a\left\vert s_{h+1}^{(k)}\right.\right)\pi_\theta\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\left(\nabla_\theta^j\log\pi_\theta\left(a\left\vert s_{h+1}^{(k)}\right.\right)\right)\left(\nabla_\theta^j\log\pi_\theta\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\right)\\
    &\cdot\phi\left(s_{h+1}^{(k)},a\right)^\top\Sigma^{-1}\phi\left(s_{h+1}^{(k)},a^\prime\right)\mathrm{d}a\mathrm{d}a^\prime\\
    \leq&G^2\int_{\mathcal{A}\times\mathcal{A}}\pi_\theta\left(a\vert s_{h+1}^{(k)}\right)\pi_\theta\left(a^\prime\vert s_{h+1}^{(k)}\right)\left\Vert\Sigma^{-\frac{1}{2}}\phi\left(s_{h+1}^{(k)},a\right)\right\Vert\left\Vert\Sigma^{-\frac{1}{2}}\phi\left(s_{h+1}^{(k)},a^\prime\right)\right\Vert\mathrm{d}a\mathrm{d}a^\prime\leq G^2C_1d,
\end{align*}
which implies 
\begin{align*}
    \mu^\top\mathbb{E}\left[\left(\nabla_\theta^j Y_\theta^{(k)}\right)\left(\nabla_\theta^j Y_\theta^{(k)}\right)^\top\right]\mu\leq \frac{G^2C_1d}{H}\mathbb{E}\left[\left\Vert\left(\Phi^{(k)}\right)^\top\Sigma^{-\frac{1}{2}}\mu\right\Vert^2\right]=G^2C_1d\Vert\mu\Vert^2. 
\end{align*}
Therefore, 
\begin{align*}
    \textrm{Var}_1\left(\nabla_\theta^j Y_\theta^{(k)}\right):=&\mathbb{E}\left[\left(\nabla_\theta^j Y_\theta^{(k)}-\mathbb{E}\left[\nabla_\theta^j Y_\theta^{(k)}\right]\right)\left(\nabla_\theta^j Y_\theta^{(k)}-\mathbb{E}\left[\nabla_\theta^j Y_\theta^{(k)}\right]\right)^\top\right]\preceq\mathbb{E}\left[\left(\nabla_\theta^j Y_\theta^{(k)}\right)\left(\nabla_\theta^j Y_\theta^{(k)}\right)^\top\right]\preceq G^2C_1dI_d.
\end{align*}
Meanwhile, we have
\begin{align*} 
	\textrm{Var}_2\left(\nabla_\theta^j Y_\theta^{(k)}\right)\preceq\mathbb{E}\left[\left(\nabla_\theta^j Y_\theta^{(k)}\right)^\top\nabla_\theta^j Y_\theta^{(k)}\right]\preceq \frac{C_1d}{H}\Sigma^{-\frac{1}{2}}\mathbb{E}\left[\left(\nabla_\theta^j \Phi_\theta^{(k)}\right)\left(\nabla_\theta^j\Phi_\theta^{(k)}\right)^\top\right]\Sigma^{-\frac{1}{2}}. 
\end{align*}
In conclusion, we get
\begin{align*}
    \max\left\{\left\Vert\textrm{Var}_1\left(\nabla_\theta^j Y_\theta^{(k)}\right)\right\Vert,\left\Vert\textrm{Var}_2\left(\nabla_\theta^j Y_\theta^{(k)}\right)\right\Vert\right\}\leq G^2C_1d\left(\kappa_3^2\vee 1\right),
\end{align*}
Note that $\left\Vert\nabla_\theta^j Y_\theta^{(k)}\right\Vert\leq C_1dG$, we know $\left\Vert\nabla_\theta^j Y_\theta^{(k)}-\mathbb{E}\left[\nabla_\theta^j Y_\theta^{(k)}\right]\right\Vert\leq 2C_1dG$. By Matrix Bernstein's inequality, we get for any $\varepsilon>0$,
\begin{align*}
    \mathbb{P}\left(\left\Vert\sum_{k=1}^K\left(\nabla_\theta^j Y_k^\theta-\Sigma^{\frac{1}{2}}\left(\nabla_\theta^j M_\theta\right) \Sigma^{-\frac{1}{2}}\right)\right\Vert\geq\varepsilon\right)\leq 2d\exp\left(-\frac{\varepsilon^2/2}{G^2C_1dK\left(\kappa_3^2\vee 1\right)+2C_1dG \varepsilon/3}\right), 
\end{align*}
taking a union bound over all $j\in[m]$ proves that $\bigcap_{j=1}^m\mathcal{E}_{Y,j}$ holds with probability $1-\frac{\delta}{8}$. Using a union bound argument again, we know with probability $1-\frac{\delta}{4}$, $\mathcal{E}_Y$ holds, which has finished the proof.
\end{proof}

\begin{lemma}
\label{eps_homo}
$\mathbb{P}(\mathcal{E}_{\varepsilon}) \geq 1-\frac{\delta}{4}$. 
\end{lemma}
\begin{proof}
Let $X_{\theta,h}^{(k)}:=\Sigma^{-\frac{1}{2}}\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\in\mathbb{R}^d$ and let $\mathcal{F}_{h,k}$ be $\sigma$-algebra generated by the history up to step $h$ at episode $k$, we have $\mathbb{E}\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]=0$. We apply matrix-form Freedman's inequality to analyze the concentration property. Consider conditional variances $\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]:=\mathbb{E}\left[\left.X_{\theta,h}^{(k)} \left(X_{\theta,h}^{(k)}\right)^\top\right\vert\mathcal{F}_{h,k}\right]\in\mathbb{R}^{d\times d}$ and $\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]:=\mathbb{E} \left[\left(X_{\theta,h}^{(k)}\right)^\top X_{\theta,h}^{(k)}\vert\mathcal{F}_{h,k}\right]\in\mathbb{R}$. It holds that
\begin{align*} 
    \left\Vert\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert=&\left\Vert\mathbb{E}\left[\left.X_{\theta,h}^{(k)}\left(X_{\theta,h}^{(k)}\right)^\top\right\vert\mathcal{F}_{h,k}\right]\right\Vert\leq\mathbb{E}\left[\left.\left\Vert X_{\theta,h}^{(k)}\left(X_{\theta,h}^{(k)}\right)^\top\right\Vert\right\vert\mathcal{F}_{h,k}\right]\\
    =&\mathbb{E}\left[\left.\left\Vert X_{\theta,h}^{(k)}\right\Vert^2\right\vert\mathcal{F}_{h,k}\right]= \textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right] 
\end{align*}
and
\begin{align*} 
    \textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]=&\mathbb{E}\left[\left.\left\Vert X_{\theta,h}^{(k)}\right\Vert^2\right\vert\mathcal{F}_{h,k}\right]=\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right)\textrm{Var}\left[\left.\varepsilon_{h,k}^\theta\right\vert s_h^{(k)},a_h^{(k)}\right]\\
    \leq&\frac{1}{(1-\gamma)^2}\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right), 
\end{align*}
where we have used $\textrm{Var}\left[\left.\varepsilon_{h,k}^\theta\right\vert s_h^{(k)},a_h^{(k)} \right]\leq\frac{1}{(1-\gamma)^2}$. Note that
\begin{align*} 
    \sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right)=&KHd+KH\textrm{Tr}\left(\Sigma^{-\frac{1}{2}}\left(\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\right)\Sigma^{-\frac{1}{2}}-I_d\right)\\
    \leq&KHd+KHd\left\Vert\Sigma^{-\frac{1}{2}}\left(\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\right)\Sigma^{-\frac{1}{2}}-I_d\right\Vert. 
\end{align*}
We take
\begin{align} 
    \label{sigma2_homo} 
    \sigma^2:=\frac{KHd}{(1-\gamma)^2}\left(1+\sqrt{\frac{2C_1d\log\frac{32dH}{\delta}}{K}}+\frac{2C_1d\log\frac{32dH}{\delta}}{3K}\right). 
\end{align}
The result of Lemma \ref{dsig1_homo} implies that
\begin{align}
    \label{Var2_homo} 
    \mathbb{P}\left(\left\Vert\sum_{k=1}^K\sum_{h=1}^H\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert\leq\sum_{k=1}^K\sum_{h=1}^H\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\leq\sigma^2\right)\geq 1-\frac{\delta}{16}. 
\end{align}
Additionally, we have $\left\Vert X_{\theta,h}^{(k)}\right\Vert\leq\frac{\sqrt{C_1d}}{1-\gamma}$. The Freedman's inequality therefore implies that for any $\varepsilon>0$,
\begin{align} 
    \label{Freedman2_homo} 
    \begin{aligned}
        &\mathbb{P}\left(\left\vert\sum_{k=1}^K\sum_{h=1}^H X_{\theta,h}^{(k)}\right\vert\geq \varepsilon,\ \max\left\{\left\Vert\sum_{k=1}^K\sum_{h=1}^H\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert,\sum_{k=1}^K\sum_{h=1}^H\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\}\leq\sigma^2\right)\\
        \leq &2d\exp\left(-\frac{\varepsilon^2/2}{\sigma^2+\sqrt{C_1d}\varepsilon/(3(1-\gamma))} \right),
    \end{aligned} 
\end{align}
where $\sigma^2$ is defined in \eqref{sigma2_homo}. We take
\begin{align*}
    \varepsilon:=\sigma\sqrt{2\log\frac{32d}{\delta}}+\frac{2\sqrt{C_1d}}{3(1-\gamma)}\log\frac{32d}{\delta}.
\end{align*}
Then we get
\begin{align*}
    \mathbb{P}\left(\left\vert\sum_{k=1}^K\sum_{h=1}^H X_{\theta,h}^{(k)}\right\vert\geq\varepsilon,\ \max\left\{\left\Vert\sum_{k=1}^K\sum_{h=1}^H\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert,\sum_{k=1}^K\sum_{h=1}^H\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\}\leq\sigma^2\right)\leq\frac{\delta}{16},
\end{align*}
which implies
\begin{align*}
    \mathbb{P}\left(\left\vert\sum_{k=1}^K\sum_{h=1}^H X_{\theta,h}^{(k)}\right\vert\geq \varepsilon\right)\leq&\mathbb{P}\left(\left\vert\sum_{k=1}^K\sum_{h=1}^H X_{\theta,h}^{(k)}\right\vert\geq \varepsilon,\ \max\left\{\left\Vert\sum_{k=1}^K\sum_{h=1}^H\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert,\sum_{k=1}^K\sum_{h=1}^H\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\}\leq\sigma^2\right)\\
    &+\mathbb{P}\left(\max\left\{\left\Vert\sum_{k=1}^K\sum_{h=1}^H\textrm{Var}_1\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert,\sum_{k=1}^K\sum_{h=1}^H\textrm{Var}_2\left[\left.X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\}>\sigma^2\right)\leq\frac{\delta}{8}.
\end{align*}
which has proved $\mathbb{P}\left(\mathcal{E}_{\varepsilon,0}\right)\geq 1-\frac{\delta}{8}$. For any fixed $j\in[m]$, we use Freedman's inequality again to prove $\mathbb{P}\left(\mathcal{E}_{\varepsilon,j}\right)\geq 1-\frac{\delta}{8m}$. We have
\begin{align*} 
    \left\Vert\textrm{Var}_1\left[\left.\nabla_\theta^j X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]\right\Vert=&\left\Vert\mathbb{E}\left[\left.\left(\nabla_\theta^j X_{\theta,h}^{(k)}\right)\left(\nabla_\theta^j X_{\theta,h}^{(k)}\right)^\top\right\vert\mathcal{F}_{h,k}\right]\right\Vert\leq\mathbb{E}\left[\left.\left\Vert\left(\nabla_\theta^j X_{\theta,h}^{(k)}\right)\left(\nabla_\theta^j X_{\theta,h}^{(k)}\right)^\top\right\Vert\right\vert\mathcal{F}_{h,k}\right]\\
    =&\mathbb{E}\left[\left.\left\Vert\nabla_\theta^j X_{\theta,h}^{(k)}\right\Vert^2\right\vert\mathcal{F}_{h,k}\right]= \textrm{Var}_2\left[\left.\nabla_\theta^j X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right],
\end{align*}
and
\begin{align*} 
    \textrm{Var}_2\left[\left.\nabla_\theta^j X_{\theta,h}^{(k)}\right\vert\mathcal{F}_{h,k}\right]=&\mathbb{E}\left[\left.\left\Vert\nabla_\theta^j X_{\theta,h}^{(k)}\right\Vert^2\right\vert\mathcal{F}_{h,k}\right]=\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right)\textrm{Var}\left[\left.\nabla_\theta^j\varepsilon_{h,k}^\theta\right\vert s_h^{(k)},a_h^{(k)}\right]\\
    \leq&\frac{4G^2}{(1-\gamma)^2}\phi\left(s_h^{(k)},a_h^{(k)}\right)^\top\Sigma^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right),  
\end{align*}
where we have used $\textrm{Var}\left[\left.\nabla_\theta^j\varepsilon_{h,k}^\theta\right\vert s_h^{(k)},a_h^{(k)}\right]\leq \frac{4G^2}{(1-\gamma)^2}$. Furthermore, notice that $\left\Vert\nabla_\theta^j X_{\theta,h}^{(k)}\right\Vert\leq\frac{2G\sqrt{C_1d}}{1-\gamma}$, the remaining steps will be exactly the same as those in the proof of the case $\mathcal{E}_{\varepsilon,0}$. Taking a union bound over $j\in[m]$ and $\mathcal{E}_{\varepsilon,0}$, we have proved $\mathbb{P}\left(\bigcap_{j=0}^m\mathcal{E}_{\varepsilon, j}\right)\geq 1-\frac{\delta}{4}$, which has finished the proof. 
\end{proof}
Combining the results of Lemma \ref{dsig1_homo}, Lemma \ref{dy_homo}, Lemma \ref{eps_homo} and take a union bound, we conclude 
\begin{align*}
\mathbb{P}\left(\mathcal{E}\right) \geq 1-\frac{3}{4}\delta. 
\end{align*}

Next, we prove some immediate results when the event $\mathcal{E}$ holds. 
\begin{lemma}
\label{dsig2_homo}
When $\mathcal{E}_\Sigma$ holds and 
\begin{align*}
    K\geq C_1d\log\frac{8dmH}{\delta},\quad\lambda\leq C_1d\sigma_{\textrm{min}}(\Sigma)\cdot\log\frac{8dmH}{\delta},
\end{align*}
we have
\begin{align*}
        \left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\leq 4\sqrt{\frac{C_1d\log\frac{8dmH}{\delta}}{K}}. 
\end{align*}
\end{lemma}
\begin{proof}
Note that
\begin{align}
    \label{2_1_homo}
    \left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert=\left\Vert\Sigma^{\frac{1}{2}}\left(\widehat{\Sigma}^{-1}-\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\leq\left\Vert\Sigma^{\frac{1}{2}}\widehat{\Sigma}^{-1}\Sigma^{\frac{1}{2}}\right\Vert\left\Vert\Sigma^{-\frac{1}{2}}\widehat{\Sigma}\Sigma^{-\frac{1}{2}}-I_d\right\Vert.  
\end{align}
When $\mathcal{E}_\Sigma$ holds, with the condition
\begin{align*}
    K\geq C_1d\log\frac{8dmH}{\delta},\quad\lambda\leq C_1d\sigma_{\textrm{min}}(\Sigma)\cdot\log\frac{8dmH}{\delta},
\end{align*}
we have
\begin{align*}
\left\Vert\Sigma^{-\frac{1}{2}}\widehat{\Sigma}\Sigma^{-\frac{1}{2}}-I_d\right\Vert\leq \sqrt{\frac{2C_1d\log\frac{8dH}{\delta}}{K}}+\frac{2C_1d\log\frac{8dH}{\delta}}{3K}+\frac{\lambda\Vert\Sigma^{-1}\Vert}{K}\leq 2\sqrt{\frac{C_1d\log\frac{8dmH}{\delta}}{K}}\leq \frac{1}{2},
\end{align*}
which further implies $\sigma_{\textrm{min}}\left(\Sigma^{-\frac{1}{2}}\widehat{\Sigma}\Sigma^{-\frac{1}{2}}\right)\geq\frac{1}{2}$, and $\left\Vert\Sigma^{\frac{1}{2}}\widehat{\Sigma}^{-1}\Sigma^{\frac{1}{2}}\right\Vert\leq 2$. Combining this result with \eqref{2_1_homo}, we get 
\begin{align*}
    \left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\leq 4\sqrt{\frac{C_1d\log\frac{8dmH}{\delta}}{K}}.
\end{align*}
which has finished the proof. 
\end{proof}

\begin{lemma}
\label{dm2_homo}
When $\mathcal{E}_\Sigma$ and $\mathcal{E}_Y$ hold, and
\begin{align*}
    K\geq 36\kappa_1(4+\kappa_2+\kappa_3)^2\frac{C_1d}{(1-\gamma)^2}\log\frac{16dmH}{\delta},\quad\lambda\leq C_1d\sigma_{\textrm{min}}(\Sigma)\cdot\log\frac{8dmH}{\delta},
\end{align*}
we have
\begin{align*}
    \left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\leq 6\sqrt{\kappa_1}(4+\kappa_2+\kappa_3)\sqrt{\frac{C_1d\log\frac{16dmH}{\delta}}{K}},
\end{align*}
and
\begin{align*}
    \left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta M_\theta\right)}{G}\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\leq 6\sqrt{\kappa_1}(4+\kappa_2+\kappa_3)\sqrt{\frac{C_1d\log\frac{16dmH}{\delta}}{K}},\quad\forall j\in[m].
\end{align*}
\end{lemma}
\begin{proof}
We have
\begin{align*}
    \left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert&\leq\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma^{\frac{1}{2}}\Sigma_\theta^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma^{-\frac{1}{2}}\right\Vert=\sqrt{\kappa_1}\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma^{-\frac{1}{2}}\right\Vert\\
    &=\sqrt{\kappa_1}\left\Vert\Sigma^{\frac{1}{2}}\left(\widehat{\Sigma}^{-1}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\int_{\mathcal{A}}\phi\left(s_{h+1}^{(k)},a^\prime\right)\pi_\theta\left(a^\prime\left\vert s_{h+1}^{(k)}\right.\right)\mathrm{d}a^\prime- M_\theta\right)\Sigma^{-\frac{1}{2}}\right\Vert\\
    &\leq\sqrt{\kappa_1}\left(\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\right)\left(1+\left\Vert\Sigma^{-\frac{1}{2}}\left(\Delta Y_\theta\right)\Sigma^{-\frac{1}{2}}\right\Vert\right)-1\right),
\end{align*}
where $\Delta Y_\theta=\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\phi_\theta\left(s_{h+1}^{(k)}\right)^\top-\Sigma M_\theta$ and the last inequality uses Lemma \ref{decomp_homo}. Similarly, we have
\begin{align*}
    \left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta M_\theta\right)}{G}\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\leq\sqrt{\kappa_1}\left(\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma\right)\Sigma^{\frac{1}{2}}\right\Vert\right)\left(1+\left\Vert\Sigma^{-\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta Y_\theta\right)}{G}\right)\Sigma^{-\frac{1}{2}}\right\Vert\right)-1\right).
\end{align*}
Using the result of Lemma \ref{dsig2_homo} the event $\mathcal{E}_Y$, we get 
\begin{align*}
    \left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\leq 4\sqrt{\frac{C_1d\log\frac{8dmH}{\delta}}{K}}\leq 1, 
\end{align*}
and $\forall j\in[m]$, 
\begin{align*}
    \left\Vert\Sigma^{\frac{1}{2}}\left(\Delta Y_\theta\right)\Sigma^{-\frac{1}{2}}\right\Vert\leq 2(\kappa_2+1)\sqrt{\frac{C_1d\log\frac{16dmH}{\delta}}{K}}\leq 1,\quad\left\Vert\Sigma^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta Y_\theta\right)}{G}\right)\Sigma^{-\frac{1}{2}}\right\Vert\leq 2(\kappa_3+1)\sqrt{\frac{C_1d\log\frac{16dmH}{\delta}}{K}}\leq 1,
\end{align*}
which implies
\begin{align*}
    \left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\leq 2\sqrt{\kappa_1}\left(\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta Y_\theta\right)\Sigma^{-\frac{1}{2}}\right\Vert\right)\leq 6\sqrt{\kappa_1}(4+\kappa_2+\kappa_3)\sqrt{\frac{C_1d\log\frac{16dmH}{\delta}}{K}},
\end{align*}
where we use the fact $(1+x_1)(1+x_2)-1\leq 2(x_1+x_2)$ whenever $x_1,x_2\in[0,1]$. Similarly, we get
\begin{align*}
    \left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta M_\theta\right)}{G}\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\leq 6\sqrt{\kappa_1}(4+\kappa_2+\kappa_3)\sqrt{\frac{C_1d\log\frac{16dmH}{\delta}}{K}},\quad\forall j\in[m],
\end{align*}
which has finished the proof. 
\end{proof}
\begin{lemma}
\label{Q_hat_decomp_homo}
When $\mathcal{E}_\Sigma$ and $\mathcal{E}_Y$ hold, and 
\begin{align*}
    K\geq 36\kappa_1(4+\kappa_2+\kappa_3)^2\frac{C_1d}{(1-\gamma)^2}\log\frac{16dmH}{\delta},\quad\lambda\leq C_1d\sigma_{\textrm{min}}(\Sigma)\cdot\log\frac{8dmH}{\delta},
\end{align*}
we have
\begin{align*}
    \widehat{Q}^\theta=\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\widehat{r},\quad\widehat{\nabla_\theta Q^\theta}=\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\tilde{U}^\theta.
\end{align*}
\begin{proof}
Firstly, note that given the conditions, the result of Lemma \ref{dm2_homo} implies
\begin{align*}
    \left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\leq 6\sqrt{\kappa_1}(4+\kappa_2+\kappa_3)\sqrt{\frac{C_1d\log\frac{16dmH}{\delta}}{K}}\leq 1-\gamma.
\end{align*}
Therefore, 
\begin{align*}
\Vert\gamma\Sigma_\theta^{\frac{1}{2}}\widehat{M}_\theta\Sigma_\theta^{-\frac{1}{2}}\Vert\leq \gamma\left(\left\Vert\Sigma_\theta^{\frac{1}{2}}\widehat{M}_\theta\Sigma_\theta^{-\frac{1}{2}}\right\Vert + \left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta\widehat{M}_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\right)\leq \gamma(2-\gamma) < 1. 
\end{align*}
where we use the result of Lemma \ref{ineq_homo} to get $\left\Vert\Sigma_\theta^{\frac{1}{2}}\widehat{M}_\theta\Sigma_\theta^{-\frac{1}{2}}\right\Vert\leq 1$. Therefore, we have 
\begin{align*}
\left(I_d - \gamma\widehat{M}_\theta\right)^{-1} = \Sigma_\theta^{-\frac{1}{2}}\left(I_d - \gamma\Sigma_\theta^{\frac{1}{2}}\widehat{M}_\theta\Sigma_\theta^{-\frac{1}{2}}\right)^{-1}\Sigma_\theta^{\frac{1}{2}}=\Sigma_\theta^{-\frac{1}{2}}\sum_{h=1}^\infty\gamma^{h-1}\left(\Sigma_\theta^{\frac{1}{2}}\widehat{M}_\theta\Sigma_\theta^{-\frac{1}{2}}\right)^{h-1}\Sigma_\theta^{\frac{1}{2}} = \sum_{h=1}^\infty\gamma^{h-1}\widehat{M}_\theta^{h-1}. 
\end{align*}
Based on this result, we prove the main result by definition:
\begin{align*}
\widehat{Q}^\theta(\cdot,\cdot) &= \phi(\cdot,\cdot)^\top\left(I_d-\gamma\widehat{M}_\theta\right)^{-1}\widehat{w}_r=\phi(\cdot,\cdot)^\top\sum_{h=1}^\infty\gamma^{h-1}\widehat{M}_\theta^{h-1}\widehat{w}_r=\left(\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\widehat{r}\right)(\cdot,\cdot),\\
\widehat{\nabla_\theta Q^\theta}(\cdot,\cdot) &= \phi(\cdot,\cdot)^\top\left(I_d-\gamma\widehat{M}_\theta\right)^{-1}\widehat{\nabla_\theta^j M_\theta}\widehat{w}^\theta=\phi(\cdot,\cdot)^\top\sum_{h=1}^\infty\gamma^{h-1}\widehat{M}_\theta^{h-1}\widehat{\nabla_\theta^j M_\theta}\widehat{w}^\theta=\left(\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\tilde{U}^\theta\right)(\cdot,\cdot),
\end{align*}
which has finished the proof. 
\end{proof}
\end{lemma}
Now we consider the decomposition of $Q^\theta-\widehat{Q}^\theta$: 
\begin{lemma}
\label{Q_decomp_homo}
Under the same condition of Lemma \ref{Q_hat_decomp_homo}, we have 
\begin{align*}
    Q^\theta-\widehat{Q}^\theta=\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(Q^\theta-\widehat{r}-\gamma\widehat{\mathcal{P}}_\theta Q^\theta\right).
\end{align*}
\end{lemma}
\begin{proof}
Simply note that 
\begin{align*}
    Q^\theta-\widehat{Q}^\theta&=\sum_{h=1}^\infty\gamma^{h-1}\left(\mathcal{P}_\theta\right)^{h-1}r-\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\widehat{r}\\
    &=\sum_{h=1}^\infty\gamma^{h-1}\left(\left(\mathcal{P}_\theta\right)^{h-1}-\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\right)r+\sum_{h^\prime=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(r-\widehat{r}\right)\\
    &=\sum_{h=1}^\infty\gamma^{h-1}\sum_{h^{\prime}=1}^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h^\prime-1}\left(\mathcal{P}_\theta-\widehat{\mathcal{P}}_\theta\right)\left(\mathcal{P}_\theta\right)^{h-h^\prime-1}r+\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(r-\widehat{r}\right)\\
    &=\sum_{h^\prime=1}^\infty\left(\widehat{\mathcal{P}}_\theta\right)^{h^\prime-1}\left(\mathcal{P}_\theta-\widehat{\mathcal{P}}_\theta\right)\sum_{h=h^\prime+1}^\infty\gamma^{h-1}\left(\mathcal{P}_\theta\right)^{h-h^\prime-1}r+\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(r-\widehat{r}\right)\\
    &=\sum_{h=1}^\infty\gamma^h\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(\mathcal{P}_\theta-\widehat{\mathcal{P}}_\theta\right)Q^\theta+\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(r-\widehat{r}\right)\\
    &=\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(Q^\theta-\widehat{r}-\gamma\widehat{\mathcal{P}}_\theta Q^\theta\right), 
\end{align*}
which is the desired result. 
\end{proof}


\subsection{Proofs of Main Theorems}
Define $\widehat{\nu}^\theta_h:=\left(\widehat{M}_\theta^\top\right)^{h-1}\nu_1^\theta$ and $\widehat{\nu}^\theta = \sum_{h=1}^\infty \gamma^{h-1}\widehat{\nu}_h^\theta$. We may prove the following decomposition of $\nabla_\theta v_\theta-\widehat{\nabla_\theta v_\theta}$:
\begin{lemma}
\label{error_decomp_homo}
Given the same condition of Lemma \ref{Q_hat_decomp_homo}, we have $\nabla_\theta v_\theta-\widehat{\nabla_\theta v_\theta}=E_1+E_2+E_3$, where 
\begin{align*}
    E_1=&\nabla_\theta\left[\left(\nu^\theta\right)^\top\Sigma^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right]\\
    E_2=&\nabla_\theta\left[\left(\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}-\left(\nu^\theta\right)^\top\Sigma^{-1}\right)\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right]\\
    E_3=&\frac{\lambda}{KH}\sum_{h=1}^T\nabla_\theta\left[\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}w^\theta\right].
\end{align*}
\end{lemma}
The proof of Lemma \ref{error_decomp_homo} is deferred to appendix \ref{missing_proof_homo}. Based on this observation, here we show the proofs of our main theorems. 

\subsubsection{Proof of Theorem \ref{thm2_var_homo}}
\label{pfthm2_var_homo}
\begin{proof}
We use Lemma \ref{error_decomp_homo} to decompose $\langle\nabla_\theta v_\theta - \widehat{\nabla_\theta v_\theta}, t\rangle=\langle E_1, t\rangle+\langle E_2,t\rangle+\langle E_3,t\rangle$. To bound each term individually, we introduce the following lemmas, whose proofs are deferred to appendix \ref{missing_proof_homo}. 
\begin{lemma}
\label{e1_finite_product_homo}
For any $t\in\mathbb{R}^m$, with probability $1-\frac{\delta}{4}$, we have
\begin{align*}
    \vert\langle E_1, t\rangle\vert\leq\sqrt{\frac{2t^\top\Lambda_\theta t\log(8/\delta)}{HK}}+\frac{2\log(8/\delta)\sqrt{C_1md}\Vert t\Vert B}{3HK}. 
\end{align*}
where $B=\frac{1}{1-\gamma}\max_{j\in[m]}\sqrt{\left(\nabla^j_\theta\nu^\theta\right)^\top\Sigma^{-1}\nabla^j_\theta\nu^\theta}+\frac{2G}{(1-\gamma)^2}\sqrt{\left(\nu^\theta\right)^\top\Sigma^{-1}\nu^\theta}$.
\end{lemma}
\begin{lemma}
\label{e2_homo}
Let $E_2^j$ be the $j$th entry of $E_2$, suppose $\mathcal{E}$ holds and $K\geq 36\kappa_1(4+\kappa_2+\kappa_3)^2C_1d(1-\gamma)^{-2}\log\frac{16dmH}{\delta}$ and $\lambda\leq C_1d\sigma_{\min}(\Sigma)\log\frac{8dmH}{\delta}$, then we have 
\begin{align*}
    \vert E_2^j\vert\leq \frac{240\sqrt{\kappa_1}(2+\kappa_2+\kappa_3)\sqrt{C_1}d}{(1-\gamma)^3}\left(\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+\frac{G}{1-\gamma}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\frac{\log\frac{32dmH}{\delta}}{KH},\quad\forall j\in[m].
\end{align*}
\end{lemma}
\begin{lemma}
\label{e3_homo}
Let $E_3^j$ be the $j$th entry of $E_3$, suppose $\mathcal{E}$ holds and $K\geq 36\kappa_1(4+\kappa_2+\kappa_3)^2C_1d(1-\gamma)^{-2}\log\frac{16dmH}{\delta}$ and $\lambda\leq C_1d\sigma_{\min}(\Sigma)\log\frac{8dmH}{\delta}$, we have 
\begin{align*}
    \vert E_3^j\vert\leq&\frac{6C_1d}{(1-\gamma)^2}\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+\frac{G}{1-\gamma}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\frac{\log\frac{8dmH}{\delta}}{KH},\quad\forall j\in[m]. 
\end{align*}
\end{lemma}
Let $B_1^j=\frac{1}{1-\gamma}\sqrt{\left(\nabla^j_\theta\nu^\theta\right)^\top\Sigma^{-1}\nabla^j_\theta\nu^\theta},\ B_2=\frac{G}{(1-\gamma)^2}\sqrt{\left(\nu^\theta\right)^\top\Sigma^{-1}\nu^\theta}$, then we have the relation $B=\max_{j\in[m]}B_1^j+2B_2$. For any $j\in[m]$, note that
\begin{align*}
    B_1^j=&\frac{1}{1-\gamma}\left\Vert\Sigma^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta\right\Vert\leq \sum_{h=1}^\infty \frac{\gamma^{h-1}}{1-\gamma}\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\left(\left(M_\theta^{h-1}\right)^\top\nu^\theta_1\right)\right\Vert\\
    =&\sum_{h=1}^\infty\frac{\gamma^{h-1}}{1-\gamma}\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\\
    \cdot&\left(\left\Vert\Sigma_\theta^{\frac{1}{2}}M_\theta\Sigma_\theta^{-\frac{1}{2}}\right\Vert^{h-1}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+(h-1)\left\Vert\Sigma_\theta^{\frac{1}{2}}M_\theta\Sigma_\theta^{-\frac{1}{2}}\right\Vert^{h-2}\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\nabla_\theta^j M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\\
    \leq&\frac{1}{(1-\gamma)^2}\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+\frac{G}{1-\gamma}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right),
\end{align*}
where we use the result of Lemma \ref{ineq_homo}. Similarly, 
\begin{align*}
    B_2&\leq\sum_{h=1}^\infty\frac{G\gamma^{h-1}}{(1-\gamma)^2}\left\Vert\Sigma^{-\frac{1}{2}}\nu^\theta_h\right\Vert\leq\sum_{h=1}^\infty\frac{G\gamma^{h-1}}{(1-\gamma)^2}\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}M_\theta\Sigma_\theta^{-\frac{1}{2}}\right\Vert^{h-1}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\\
    &\leq\sum_{h=1}^\infty\frac{G\gamma^{h-1}}{(1-\gamma)^2}\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\leq \frac{G}{(1-\gamma)^3}\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert.
\end{align*}
We conclude that when $K\geq 36C_1d(1-\gamma)^{-2}\kappa_1(4+\kappa_2+\kappa_3)^2\log\frac{16dmH}{\delta}$, we have
\begin{align*}
    \left(\max_{j\in[m]}B_1^j+2B_2\right)\frac{2\log\frac{2}{\delta}\sqrt{C_1md}\Vert t\Vert}{KH}\leq \frac{1}{(1-\gamma)^2}\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\max_{j\in[m]}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+2HG\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\frac{2\log\frac{2}{\delta}\sqrt{C_1md}\Vert t\Vert}{KH},
\end{align*}
Now, take a union bound on $\mathcal{E}$ and the event in Lemma \ref{e1_finite_product_homo}, we get with probability $1-\delta$, we have
\begin{align*}
    &\vert\langle E_1,t\rangle\vert+\vert\langle E_2,t\rangle\vert+\vert \langle E_3, t\rangle\vert\leq\sqrt{\frac{2t^\top\Lambda_\theta t\log(8/\delta)}{HK}}\\
    &+\sqrt{\kappa_1}(5+\kappa_2+\kappa_3)\left(\max_{j\in[m]}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+\frac{G}{1-\gamma}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\frac{240C_1d\log\frac{32dmH}{\delta}}{(1-\gamma)^3KH}\\
    \leq&\sqrt{\frac{2t^\top\Lambda_\theta t\log(8/\delta)}{KH}}+\kappa_1(5+\kappa_2+\kappa_3)\left(\max_{j\in[m]}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+\frac{G}{1-\gamma}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\frac{240C_1d\sqrt{m}\Vert t\Vert\log\frac{32dmH}{\delta}}{(1-\gamma)^3HK}.
\end{align*}
we have finished the proof. 
\end{proof}

\subsubsection{Proof of Theorem \ref{thm2_homo}}
\label{pfthm2_homo}
\begin{proof}
According to the result of Theorem \ref{thm2_var_homo}, we know
\begin{align*}
    &\vert\langle t,\widehat{\nabla_\theta v_\theta}-\nabla_\theta v_\theta\rangle\vert\leq\sqrt{\frac{2t^\top\Lambda_\theta t}{HK}\cdot\log\frac{8}{\delta}}+\frac{C_\theta\Vert t\Vert\log\frac{32mdH}{\delta}}{HK},
\end{align*}
Pick $t=e_j, j\in[m]$, we have
\begin{align*}
    t^\top \Lambda_\theta t=&\mathbb{E}\left[\frac{1}{H}\sum_{h=1}^H\left(\nabla_\theta^j\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu^\theta\right)\right)^2\right]\\
    \leq&2\mathbb{E}\left[\frac{1}{H}\sum_{h=1}^H\left(\nabla_\theta^j\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu^\theta\right)^2\right]+2\mathbb{E}\left[\frac{1}{H}\sum_{h=1}^H\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nabla_\theta^j\nu^\theta\right)^2\right]\\
    \leq&2\mathbb{E}\left[\sum_{h=1}^H\frac{G^2}{H(1-\gamma)^4}\left(\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^H\frac{1}{H(1-\gamma)^2}\left(\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nabla_\theta^j\nu^\theta\right)^2\right]\\
     \leq&2\left(\frac{G^2}{(1-\gamma)^4}\left\Vert\Sigma^{-\frac{1}{2}}\nu^\theta\right\Vert^2+\frac{1}{(1-\gamma)^2}\left\Vert\Sigma^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta\right\Vert^2\right).
\end{align*}
Therefore, take a union bound over $j\in[m]$, we get
\begin{align*}
\left\vert\widehat{\nabla_\theta^j v_\theta}-\nabla_\theta^j v_\theta\right\vert\leq 4b_\theta\sqrt{\frac{\log\frac{8m}{\delta}}{HK}}+\frac{2C_\theta\sqrt{m}\log\frac{32mdH}{\delta}}{HK},\quad\forall j\in[m],
\end{align*}
where 
\begin{align*}
b_\theta=\frac{G}{(1-\gamma)^2}\left\Vert\Sigma^{-\frac{1}{2}}\nu^\theta\right\Vert+\frac{1}{1-\gamma}\left\Vert\Sigma^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta\right\Vert.
\end{align*}
When $\phi(s^\prime,a^\prime)^\top\Sigma^{-1}\phi(s,a)\geq 0,\ \forall(s,a),(s^\prime,a^\prime)\in\mathcal{S}\times\mathcal{A}$, for any $h\in\mathbb{N}_+$ and $(s,a)\in\mathcal{S}\times\mathcal{A}$, we have
\begin{align*}
    \left\vert\left(\nabla^j_\theta\nu^\theta_{h}\right)^\top\Sigma^{-1}\phi(s,a)\right\vert=& \left\vert\mathbb{E}^{\pi_\theta}\left[\phi(s_h,a_h)\Sigma^{-1}\phi(s,a)\sum_{h^\prime=1}^h\nabla_\theta^j\log\pi_{\theta,h^\prime}(a_{h^\prime}\vert s_{h^\prime})\right]\right\vert\\
    \leq&\mathbb{E}^{\pi_\theta}\left[\phi(s_h, a_h)\Sigma^{-1}\phi(s, a)\sum_{h^\prime=1}^h\left\vert\nabla_\theta^j\log\pi_{\theta,h^\prime}(a_{h^\prime}\vert s_{h^\prime})\right\vert\right]\\
    \leq&Gh\mathbb{E}^{\pi_\theta}\left[\phi(s_h,a_h)\Sigma^{-1}\phi(s,a)\right]\\
    =&Gh\left(\nu^\theta_{h}\right)^\top\Sigma^{-1}\phi(s,a).
\end{align*}
Meanwhile, for any positive integer $\tilde{H}$, we have
\begin{align*}
\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nabla_\theta^j\nu^\theta &= \sum_{h=1}^\infty\gamma^{h-1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nabla_\theta^j\nu_h^\theta \\
&= \sum_{h=1}^{\tilde{H}}\gamma^{h-1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nabla_\theta^j\nu_h^\theta + \sum_{h=\tilde{H}+1}^\infty\gamma^{h-1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nabla_\theta^j\nu_h^\theta.
\end{align*}
For the second term, we have
\begin{align*}
\sum_{h=\tilde{H}+1}^\infty\gamma^{h-1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nabla_\theta^j\nu_h^\theta&\leq \sum_{h=\tilde{H}+1}^\infty Gh\gamma^{h-1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu_h^\theta\\
&\leq GC_1d\sum_{h=\tilde{H}+1}^\infty h\gamma^{h-1}\\
&=GC_1d\left(\tilde{H} + \frac{1}{1-\gamma}\right)\gamma^{\tilde{H}}.
\end{align*}
For the first term, we have
\begin{align*}
\sum_{h=1}^{\tilde{H}}\gamma^{h-1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nabla_\theta^j\nu_h^\theta &\leq G\tilde{H}\sum_{h=1}^{\tilde{H}}\gamma^{h-1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu_h^\theta\leq G\tilde{H}\sum_{h=1}^\infty\gamma^{h-1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu_h^\theta\\
&=G\tilde{H}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu^\theta.
\end{align*}
Therefore,
\begin{align*}
t^\top \Lambda_\theta t=&2\mathbb{E}\left[\sum_{h=1}^H\frac{G^2}{H(1-\gamma)^4}\left(\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu^\theta\right)^2\right]+2\mathbb{E}\left[\sum_{h=1}^H\frac{1}{H(1-\gamma)^2}\left(\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nabla_\theta^j\nu^\theta\right)^2\right]\\
\leq&4\mathbb{E}\left[\sum_{h=1}^H\frac{G^2}{H(1-\gamma)^2}\left(\tilde{H}^2+\frac{1}{(1-\gamma)^2}\right)\left(\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu^\theta\right)^2\right]+\frac{4G^2C_1^2d^2}{(1-\gamma)^2}\left(\tilde{H}+\frac{1}{1-\gamma}\right)^2\gamma^{2\tilde{H}}\\
\leq&\frac{4G^2}{(1-\gamma)^2}\left(\tilde{H}+\frac{1}{1-\gamma}\right)^2\left(\left\Vert\Sigma^{-\frac{1}{2}}\nu^\theta\right\Vert^2 + C_1^2d^2\gamma^{2\tilde{H}}\right).
\end{align*}
In particular, pick 
\begin{align*}
\tilde{H} = \frac{2\log(C_1d)}{1-\gamma}\geq\frac{\log\frac{C_1d}{\left\Vert\Sigma^{-\frac{1}{2}}\nu^\theta\right\Vert}}{\log\frac{1}{\gamma}},
\end{align*}
where we use the fact $\log\frac{1}{\gamma}\geq \frac{1-\gamma}{2\gamma}$ whenever $\gamma\in(\frac{1}{2},1)$, and
\begin{align*}
\left(\nu^\theta\right)^\top\Sigma^{-1}\nu^\theta = \sup_{w\in\mathbb{R}^d}\frac{\left(w^\top \nu^\theta\right)^2}{w^\top\Sigma w} \geq \frac{1}{(1-\gamma)^2} \geq 1,
\end{align*}
where the inequality is due to the fact that $\mathcal{F}$ includes the constant functions. We have
\begin{align*}
t^\top \Lambda_\theta t\leq \frac{8G^2}{(1-\gamma)^2}\left(\tilde{H}+\frac{1}{(1-\gamma)}\right)^2\left\Vert\Sigma^{-\frac{1}{2}}\nu^\theta\right\Vert^2\leq \frac{128G^2\left(\log(C_1d)\right)^2}{(1-\gamma)^4}\left\Vert\Sigma^{-\frac{1}{2}}\nu^\theta\right\Vert^2.
\end{align*}
Taking a union bound w.r.t. $m$, we get
\begin{align*}
\left\vert\widehat{\nabla_\theta^j v_\theta}-\nabla_\theta^j v_\theta\right\vert\leq\frac{16G\log(C_1d)}{(1-\gamma)^2}\left\Vert\Sigma^{-\frac{1}{2}}\nu^\theta\right\Vert\sqrt{\frac{\log\frac{8m}{\delta}}{HK}}+\frac{2C_\theta\sqrt{m}\log\frac{32mdH}{\delta}}{HK}.
\end{align*}
\end{proof}

\subsubsection{Proof of Theorem \ref{thm1_homo}}
\label{pfthm1_homo}
\begin{proof}
We use the same decomposition as in Theorem \ref{thm2_var_homo}. Define a martingale difference sequence $\{e_k^\theta\}_{k=1}^K$ by
\begin{align*}
    e_{h,k}^\theta&=\frac{1}{\sqrt{HK}}\nabla_\theta\left(\left(\nu^\theta\right)^\top\Sigma^{-1}\phi(s_h^{(k)},a_h^{(k)})\varepsilon_{h,k}^{\theta}\right)\\
    &=\frac{1}{\sqrt{HK}}\left(\nabla_\theta\nu^\theta\right)^\top\Sigma^{-1}\phi(s_h^{(k)},a_h^{(k)})\varepsilon_{h,k}^\theta+\frac{1}{\sqrt{HK}}\sum_{h=1}^H\left(\nu^\theta\right)^\top\Sigma^{-1}\phi(s_h^{(k)},a_h^{(k)})\nabla_\theta\varepsilon_{h,k}^\theta,
\end{align*}
we have 
\begin{align*}
    \Vert e_{h,k}^\theta\Vert_\infty\leq\frac{1}{\sqrt{HK}(1-\gamma)}\max_{j\in[m]}\Vert\Sigma^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta\Vert\sqrt{C_1d}+\frac{2}{\sqrt{HK}(1-\gamma)^2}\Vert\Sigma^{-\frac{1}{2}}\nu^\theta\Vert\sqrt{C_1d}G\rightarrow 0, 
\end{align*}
where we use the result of Lemma \ref{upbd_homo}. Furthermore, 
\begin{align*}
    \sum_{h=1}^H\mathbb{E}\left[e_{h,k}^\theta\left(e_{h,k}^\theta\right)^\top\right]_{ij}=&\frac{1}{HK}\mathbb{E}\left[\sum_{h=1}^H\left[\nabla_{\theta_1}^i\left(\left(\nu^{\theta_1}\right)^\top\Sigma^{-1}\phi(s_h^{(k)},a_h^{(k)})\varepsilon_{h,k}^{\theta_1}\right)\right]\left[\nabla_{\theta_2}^j\left(\left(\nu^{\theta_2}\right)^\top\Sigma^{-1}\phi(s_h^{(k)},a_h^{(k)})\varepsilon_{h,k}^{\theta_2}\right)\right]^\top\right]\Bigg\vert_{\theta_1=\theta_2=\theta}\\
    =&\frac{[\Lambda_\theta]_{ij}}{K}.
\end{align*}
Therefore,by WLLN, we have
\begin{align*}
    \sum_{k=1}^K\sum_{h=1}^H\left[e_{h,k}^\theta\left(e_{h,k}^\theta\right)^\top\right]_{ij}\rightarrow_p\sum_{k=1}^K\sum_{h=1}^H\mathbb{E}\left[e_{h,k}^\theta\left(e_{h,k}^\theta\right)^\top\right]_{ij}=\left[\Lambda_\theta\right]_{ij},
\end{align*}
To finish the rest of the proof, we introduce the following lemmas, 
\begin{lemma}[Martingale CLT, Corollary 2.8 in (McLeish et al., 1974)] \label{CLT_homo}
Let $\left\{X_{mn},n=1,\ldots,k_m\right\}$ be a martingale difference array (row-wise) on the probability triple $(\Omega, \mathcal{F}, P)$.Suppose $X_{mn}$ satisfy the following two conditions:
\begin{align*}
    \max _{1\leq n\leq k_m}\left\vert X_{mn}\right\vert\stackrel{p}{\rightarrow}0,\textrm{ and } \sum_{n=1}^{k_m}X_{mn}^2\stackrel{p}{\rightarrow}\sigma^2
\end{align*}
for $k_m\rightarrow\infty$. Then $\sum_{n=1}^{k_m}X_{mn}\stackrel{d}{\rightarrow}\mathcal{N}\left(0,\sigma^2\right)$.
\end{lemma}
\begin{lemma}[CramÃ©râ€“Wold Theorem] 
\label{CW_thm_homo}
Let $X_n=(X_n^1,X_n^2,\ldots,X_n^k)^\top$ be a $k$-dimensional random vector series and $X=(X^1,X^2,\ldots,X^k)^\top$ be a random vector of same dimension. Then $X_n$ converges in distribution to $X$ if and only if for any constant vector $t=(t_1,t_2,\ldots,t_k)^\top$, $t^\top X_n$ converges to $t^\top X$ in distribution.
\end{lemma}
Lemma \ref{CLT_homo} implies $\sum_{k=1}^K\sum_{h=1}^H t^\top e_{h,k}\rightarrow_d\mathcal{N}(0,t^\top\Lambda_\theta t)$ for any $t$, and Lemma \ref{CW_thm_homo} implies
\begin{align*}
    \sqrt{HK}E_1=\sum_{k=1}^K\sum_{h=1}^H e_{h,k}\rightarrow_p\mathcal{N}(0,\Lambda_\theta). 
\end{align*}
Furthermore, notice that the results of Lemma \ref{e2_homo} and Lemma \ref{e3_homo} imply $\sqrt{HK}E_2\rightarrow_p 0, \sqrt{HK}E_3\rightarrow_p 0$. Combining the above results, we have finished the proof. 
\end{proof}

\subsubsection{Proof of Theorem \ref{thm4_homo}}
\label{pfthm4_homo}
\begin{proof}
We first derive the influence function of policy gradient estimator for sake of completeness. We denote each of the $K$ sampled trajectories as
$$
\boldsymbol{\tau}:=\left(s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, \ldots, s_{H}, a_{H}, r_{H}, s_{H+1}\right)
$$
We denote $\bar{\pi}(a \mid s)$ as the behavior policy. The distribution of trajectory is then given by
$$
\mathcal{P}(d \boldsymbol{\tau})= \bar{\xi}\left(d s_{1}, d a_{1}\right) p\left(d s_{2} \mid s_{1}, a_{1}\right) \bar{\pi}\left(d a_{2} \mid s_{2}\right) \ldots \bar{\pi}\left(d a_{H} \mid s_{H}\right) p\left(d s_{H+1} \mid s_{H}, a_{H}\right)
$$
Define $p_{\eta} = p + \eta\Delta p$ as a new transition probability function and $\mathcal{P}_{\eta}:=\mathcal{P}+\eta \Delta\mathcal{P}$ where $\Delta \mathcal{P}$ satisfies
$$(\Delta \mathcal{P}) \mathcal{F} \subseteq \mathcal{F}.$$ 
Define $g_\eta\left(s^{\prime} \mid s, a\right):=\frac{\partial}{\partial \eta} \log p_\eta\left( s^{\prime} \mid s, a\right)$ and the score function as 
$$
g_{\eta}(\boldsymbol{\tau}):=\frac{\partial}{\partial \eta} \log \mathcal{P}_{\eta}(d \boldsymbol{\tau})=\sum_{h=1}^{H} g_{\eta}\left(s_{h+1} \mid s_{h}, a_{h}\right).
$$
Without loss of generality, we assume $p_{\eta}$ is continuously derivative with respect to $\eta.$ This guarantees that we can change the order of taking derivatives with respect to $\eta$ and $\theta.$ When the subscript $\eta$ vanishes, it means $\eta = 0$ and the underlying transition probability is $p(s^{\prime}|s,a),$ i.e. $p_0(s^{\prime} |s,a) = p(s^{\prime} |s,a).$ Then we denote $g(s^{\prime}|s,a) := \left.\frac{\partial}{\partial \eta}\log p_\eta(s^{\prime}|s,a)\right|_{\eta = 0},$ and $g(\boldsymbol{\tau}) = \sum_{h=1}^H g(s_{h+1}|s_h,a_h).$ We define the policy value under new transition kernel is
\begin{equation*}
    v_{\theta,\eta} := \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{h=1}^\infty\gamma^{h-1} r(s_h,a_h) \right| s_1 \sim \xi, \mathcal{P}_{\eta}\right]
\end{equation*}
Then, our objective function is
$$
\psi_{\eta} := \nabla_{\theta} v_{\theta,\eta} =\mathbb{E}^{\pi_{\theta}}\left[\left.\sum_{h=1}^\infty \nabla_{\theta} \log \pi_{\theta}\left(a_{h} \mid s_{h}\right) \cdot\left(\sum_{h^{\prime}=h}^{\infty} \gamma^{h^\prime-1}r\left(s_{h^{\prime}}, a_{h^{\prime}}\right)\right) \right| s_{1} \sim \xi, \mathcal{P}_{\eta}\right].
$$
We are going to compute the influence function with respect to the above objective function. We denote this influence function as $\mathcal{I}(\boldsymbol{\tau}).$ By definition, it satisfies that
\begin{equation*}
    \left.\frac{\partial}{\partial \eta} \psi_{\eta}\right|_{\eta = 0} = \mathbb{E}\left[g(\boldsymbol{\tau}) \mathcal{I}(\boldsymbol{\tau})\right].
\end{equation*}
By exchanging the order of derivatives, we find that
\begin{equation*}
    \left.\frac{\partial}{\partial \eta} \psi_{\eta}\right|_{\eta = 0} = \nabla_{\theta} \left[\left.\frac{\partial}{\partial \eta} v_{\theta,\eta}\right|_{\eta = 0}\right].
\end{equation*}
Therefore, we calculate the derivatives.
\begin{align*}
    \frac{\partial}{\partial\eta}v_{\theta,\eta}&=\frac{\partial}{\partial\eta}\left[\sum_{h=1}^\infty\gamma^{h-1}\int_{(\mathcal{S}\times\mathcal{A})^h}r\left(s_h,a_h\right)\xi(s_1)\prod_{j=1}^{h-1}p_{\eta}\left(s_{j+1}\mid s_j,a_j\right)\prod_{j=1}^h\pi_\theta\left(a_j\mid s_j\right)d\boldsymbol{\tau}_h\right]\\
    &=\sum_{h=1}^{\infty}\gamma^{h-1} \int_{(\mathcal{S} \times \mathcal{A})^{h}} r\left(s_{h}, a_{h}\right)\left(\sum_{j=1}^{h-1} g_{\eta}\left(s_{j+1} \mid s_{j}, a_{j}\right)\right) \xi(s_1) \prod_{j=1}^{h-1} p_{\eta}\left(s_{j+1} \mid s_{j}, a_{j}\right) \prod_{j=1}^{h} \pi_{\theta}\left(a_{j} \mid s_{j}\right) d\boldsymbol{\tau}_h\\
    &= \int\sum_{h=1}^\infty\gamma^{h-1}r\left(s_{h}, a_{h}\right)\left(\sum_{j=1}^{h-1} g_\eta\left(s_{j+1}\mid s_j,a_j\right)\right) \left[\xi(s_1) \prod_{j=1}^h p_\eta\left(s_{j+1} \mid s_j,a_j\right)\prod_{j=1}^h\pi_\theta\left(a_{j} \mid s_{j}\right)\right] d\boldsymbol{\tau}.
\end{align*}
We denote $Q_{\eta}^{\theta}$ and  $\nabla_{\theta}Q_{\eta}^{\theta}$ as the state-action function and its gradient with underlying transition probability being $p_{\eta}.$ For sake of simplicity, we define the state value function as
\begin{equation*}
    V^{\theta}(s) := \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{h = 1}^\infty \gamma^{h-1}r(s_h,a_h) \right| s_1 = s, \mathcal{P}\right].
\end{equation*}
We denote $V_{\eta}^{\theta}(s)$ as the same function except for transition probability substituted by $p_{\eta}.$ Therefore,
\begin{align*}
    \frac{\partial}{\partial \eta} v_{\theta, \eta}
    &= \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{h=1}^{\infty}\gamma^{h-1}r\left(s_{h}, a_{h}\right)\left(\sum_{j=1}^{h-1} g_{\eta}\left(s_{j+1} \mid s_{j}, a_{j}\right)\right) \right| s_1 \sim \xi, \mathcal{P}_{\eta} \right] \\
    &= \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{j=1}^{\infty} g_{\eta}\left(s_{j+1} \mid s_{j}, a_{j}\right) \sum_{h=j+1}^\infty \gamma^{h-1}r\left(s_{h}, a_{h}\right) \right| s_1 \sim \xi, \mathcal{P}_{\eta} \right] \\
    &= \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{j=1}^{\infty} g_{\eta}\left(s_{j+1} \mid s_{j}, a_{j}\right) \cdot \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{h=j+1}^\infty\gamma^{h-1} r\left(s_{h}, a_{h}\right) \right| s_{j+1}\right]\right| s_1 \sim \xi, \mathcal{P}_{\eta} \right] \\
    &= \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{j=1}^{\infty} \mathbb{E}\left[\left. \gamma^{j}g_{\eta}\left(s_{j+1} \mid s_{j}, a_{j}\right) V_{\eta}^{\theta}(s_{j+1}) \right| s_j,a_j \right]\right| s_1 \sim \xi, \mathcal{P}_{\eta} \right].
\end{align*}
Therefore,
\begin{equation}\label{influence_function1_homo}
    \left.\frac{\partial}{\partial \eta} v_{\theta, \eta}\right|_{\eta=0} = \mathbb{E}^{\pi_{\theta}} \left[\left.\sum_{h=1}^{\infty} \mathbb{E}\left[\left. \gamma^h g\left(s_{h+1} \mid s_{h}, a_{h}\right) V^{\theta}(s_{h+1}) \right| s_h,a_h \right]\right| s_1 \sim \xi, \mathcal{P}_{\eta} \right].
\end{equation}
We notice that $\Sigma = \mathbb{E}\left[\frac{1}{H}\sum_{h=1}^H\phi\left(s_h^{(1)},a_h^{(1)}\right)\phi\left(s_h^{(1)},a_h^{(1)}\right)^{\top}\right]$. We denote $w_h(s,a) := \phi^{\top}(s,a)\Sigma^{-1} \nu_h^{\theta} = \phi^{\top}(s,a)\Sigma^{-1} \mathbb{E}^{\pi_{\theta}} \left[\phi(s_h,a_h) \mid s_1 \sim \xi\right].$ We leverage the following fact to rewrite \eqref{influence_function1_homo}: for any $f(s,a) = w_f^{\top} \phi(s,a) \in \mathcal{F}$ where $w_f \in \mathbb{R}^d,$ we have
\begin{align*}
\mathbb{E}^{\pi_{\theta}}\left[f(s_h,a_h)\right]
&= \mathbb{E}^{\pi_{\theta}} \left[ w_f^{\top} \phi(s_h,a_h)\right] \\
&= \mathbb{E}^{\pi_{\theta}} \left[ w_f^{\top} \mathbb{E}\left[\frac{1}{H}\sum_{h^\prime=1}^H\phi\left(s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right)\phi^{\top}\left(s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right)\right] \Sigma^{-1} \phi(s_h,a_h) \right] \\
&= \mathbb{E} \left[\frac{1}{H}\sum_{h^\prime=1}^Hw_f^{\top}\phi\left(s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right)\phi^{\top}\left(s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right)\Sigma^{-1}\mathbb{E}^{\pi_{\theta}} \left[\phi(s_h,a_h)\right]\right]\\
&= \mathbb{E}\left[\frac{1}{H}\sum_{h^\prime=1}^H f\left(s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right) w_h\left(s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right)\right]
\end{align*}
Since 
\begin{equation*}
    \mathbb{E} \left[ g\left(s^{\prime} \mid s, a\right) V^{\theta}(s^{\prime}) | s, a \right] = \left.\frac{\partial}{\partial \eta} \left(Q_{\eta}^{\theta}(s,a) - r_{\eta}(s,a)\right)\right|_{\eta = 0} \in \mathcal{F},
\end{equation*}
we have
\begin{align*}
    \left.\frac{\partial}{\partial\eta} v_{\theta,\eta}\right|_{\eta = 0}
    &= \mathbb{E}\left[\sum_{h=1}^{\infty}\gamma^h \frac{1}{H}\sum_{h^\prime=1}^H w_{h}\left(s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right) \mathbb{E}\left[g\left(s^{\prime} \mid s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right) \cdot  V^{\theta}\left(s^{\prime}\right) \mid s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right]\right] \\
    &=\mathbb{E}\left[\frac{1}{H}\sum_{h^\prime=1}^H\mathbb{E}_{s^{\prime} \sim p(\cdot \mid s_{h^\prime}^{(1)},a_{h^\prime}^{(1)})}\left[\sum_{h=1}^{\infty}\gamma^h w_{h}\left(s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right) g\left(s^{\prime} \mid s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right) \cdot  V^{\theta}\left(s^{\prime}\right)\right]\right] \\
    &=\mathbb{E}\left[\frac{1}{H}\sum_{h^\prime=1}^H\mathbb{E}_{s^{\prime} \sim p(\cdot \mid s_{h^\prime}^{(1)},a_{h^\prime}^{(1)})}\left[\sum_{h=1}^{\infty}\gamma^h w_{h}\left(s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right) g\left(s^{\prime} \mid s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right)\left( V^{\theta}\left(s^{\prime}\right)-\mathbb{E}\left[V^{\theta}\left(s^{\prime}\right) \mid s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right]\right)\right]\right]\\
    &=\mathbb{E}\left[\frac{1}{H}\sum_{h^\prime=1}^H\sum_{h=1}^{\infty}\gamma^h w_{h}\left(s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right) g\left(s_{h^\prime+1}^{(1)} \mid s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right)\left( V^{\theta}\left(s_{h^\prime+1}^{(1)}\right)-\mathbb{E}\left[V^{\theta}\left(s_{h^\prime+1}^{(1)}\right) \mid s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right]\right)\right]\\
    &= \mathbb{E} \left[g\left(\boldsymbol{\tau}\right)\frac{1}{H}\sum_{h^\prime=1}^H\sum_{h=1}^\infty\gamma^h w_{h}\left(s_{h^\prime}^{(1)}, a_{h^\prime}^{(1)}\right)\left( V^{\theta}\left(s_{h^\prime+1}^{(1)}\right)-\mathbb{E}\left[V^{\theta}\left(s_{h^\prime+1}^{(1)}\right)\mid s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right]\right)\right].
\end{align*}
Taking gradient in both sides and we have
\begin{equation*}
    \nabla_{\theta}\left(\left.\frac{\partial}{\partial\eta} v_{\theta,\eta}\right|_{\eta = 0}\right) = \mathbb{E}\left\{g\left(\boldsymbol{\tau}\right) \cdot \nabla_{\theta} \left[\frac{1}{H}\sum_{h^\prime=1}^H\sum_{h=1}^H \gamma^h w_{h}\left(s_{h^\prime}^{(1)}, a_{h^\prime}^{(1)}\right) \left( V^{\theta}\left(s_{h^\prime+1}^{(1)}\right)-\mathbb{E}\left[V^{\theta}\left(s_{h^\prime+1}^{(1)}\right) \mid s_{h^\prime}^{(1)}, a_{h^\prime}^{(1)}\right]\right)\right]\right\}.
\end{equation*}
The implies that the influence function we want is
\begin{equation*}
    \mathcal{I}(\boldsymbol{\tau}) = \nabla_{\theta} \left[\frac{1}{H}\sum_{h^\prime=1}^H\sum_{h=1}^\infty \gamma^h w_{h}\left(s_{h^\prime}^{(1)}, a_{h^\prime}^{(1)}\right) \left( V^{\theta}\left(s_{h^\prime+1}^{(1)}\right)-\mathbb{E}\left[V^{\theta}\left(s_{h^\prime+1}^{(1)}\right) \mid s_{h^\prime}^{(1)}, a_{h^\prime}^{(1)}\right]\right)\right].
\end{equation*}
Insert the expression of $w_h(s,a)$ and exploit $\varepsilon_{h,k}^{\theta}=Q^{\theta}(s_h^{(k)}, a_h^{(k)})-r_h^{(k)}-\gamma\int_{\mathcal{A}} \pi_{\theta}\left(a^{\prime} \mid s_{h+1}^{(k)}\right) Q^{\theta}\left(s_{h+1}^{(k)}, a^{\prime}\right) \mathrm{d} a^{\prime},$ we can rewrite the influence function as
\begin{equation*}
    \mathcal{I}(\boldsymbol{\tau})=-\nabla_\theta\left[\frac{1}{H}\sum_{h^\prime=1}^H\sum_{h=1}^\infty\gamma^{h-1}\phi\left(s_{h^\prime}^{(1)},a_{h^\prime}^{(1)}\right)^\top\Sigma^{-1}\varepsilon_{h^\prime,1}^\theta\nu_h^\theta\right]=-\nabla_\theta\left[\frac{1}{H}\sum_{h=1}^H\phi\left(s_{h}^{(1)},a_{h}^{(1)}\right)^\top\Sigma^{-1}\varepsilon_{h,1}^\theta\nu^\theta\right]
\end{equation*}
Therefore, since the cross terms vanish by taking conditional expectation, we have
\begin{align*}
    &\mathbb{E}\left[\mathcal{I}(\boldsymbol{\tau})^{\top} \mathcal{I}(\boldsymbol{\tau})\right]=\mathbb{E}\Bigg[\frac{1}{H^2}\sum_{h=1}^H\left(\nabla_\theta\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu^\theta\right)\right)^\top\nabla_\theta\left(\varepsilon^\theta_{h,1}\phi\left(s_h^{(1)},a_h^{(1)}\right)^\top\Sigma^{-1}\nu^\theta \right)\Bigg]=\frac{1}{H}\Lambda_{\theta}.
\end{align*}
For any vector $t \in \mathbb{R}^m,$ when it comes to $\left\langle t,\psi_{\eta}\right\rangle,$ by linearity we have
\begin{equation*}
    \left.\frac{\partial}{\partial \eta} \left\langle t,\psi_{\eta}\right\rangle\right|_{\eta=0}=\mathbb{E}[g(\boldsymbol{\tau}) \left\langle t,\mathcal{I}(\boldsymbol{\tau})\right\rangle].
\end{equation*}
Then the influence function of $\left\langle t,\nabla_{\theta}v_{\theta}\right\rangle$ is $\left\langle t,\mathcal{I}(\boldsymbol{\tau})\right\rangle.$ The Cramer-Rao lower bound for $\left\langle t,\nabla_{\theta}v_{\theta}\right\rangle$ is
\begin{equation*}
    \mathbb{E}\left[\left\langle t,\mathcal{I}(\boldsymbol{\tau})\right\rangle^2\right] = t^{\top} \mathbb{E}\left[\mathcal{I}(\boldsymbol{\tau})^{\top}\mathcal{I}(\boldsymbol{\tau})\right] t = \frac{1}{H}t^{\top} \Lambda_{\theta} t.
\end{equation*}
By continuous mapping theorem, a trivial corollary of Theorem \ref{thm4_homo} is that for any $t \in \mathbb{R}^m,$
\begin{equation*}
    \sqrt{HK}\left(\left\langle t,\widehat{\nabla_{\theta} v_{\theta}}-\nabla_{\theta} v_{\theta}\right\rangle\right) \stackrel{d}{\rightarrow} \mathcal{N}\left(0, t^{\top} \Lambda_{\theta} t\right).
\end{equation*} 
This implies that the variance of any unbiased estimator for $\left\langle t, \nabla_{\theta} v_{\theta} \right\rangle \in \mathbb{R}$ is lower bounded by $\frac{1}{\sqrt{HK}}t^{\top} \Lambda_{\theta} t.$
\end{proof}

\subsection{Missing Proofs}
\label{missing_proof_homo}
\subsubsection{Proof of Lemma \ref{error_decomp_homo}}
\begin{proof}
Note that
\begin{align*}
    \nabla_\theta Q^\theta-\widehat{\nabla_\theta Q^\theta}&=\sum_{h=1}^\infty\gamma^{h-1}\left(\mathcal{P}_\theta\right)^{h-1}U^\theta-\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\tilde{U}^\theta\\
    &=\sum_{h=1}^\infty\gamma^{h-1}\left(\mathcal{P}_\theta\right)^{h-1}U^\theta-\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\widehat{U}^\theta+\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(\widehat{U}^\theta-\tilde{U}^\theta\right)\\
    &=\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(\nabla_\theta Q^\theta-\widehat{U}^\theta-\widehat{\mathcal{P}}_\theta\nabla_\theta Q^\theta\right)+\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(\widehat{U}^\theta-\tilde{U}^\theta\right).
\end{align*}
For the first term, we have
\begin{align*}
    &\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(\nabla_\theta Q^\theta-\widehat{U}^\theta-\gamma\widehat{\mathcal{P}}_\theta\nabla_\theta Q^\theta\right)\\
    =&\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\phi^\top\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h^\prime=1}^H\phi\left(s_{h^\prime}^{(k)},a_{h^\prime}^{(k)}\right)\\
    &\cdot\left(\nabla_\theta Q^\theta\left(s_{h^\prime}^{(k)},a_{h^\prime}^{(k)}\right)-\gamma\int_{\mathcal{A}}\left(\left(\nabla_\theta\pi_\theta\left(a^\prime\left\vert s_{h^\prime+1}^{(k)}\right.\right)\right)Q^{\theta}\left(s_{h^\prime+1}^{(k)},a^\prime\right)+\pi_\theta\left(a^\prime\left\vert s_{h^\prime+1}^{(k)}\right.\right)\nabla_\theta Q^\theta\left(s_{h^\prime+1}^{(k)},a^\prime\right)\right)\mathrm{d}a^\prime\right)\\
    &+\frac{\lambda}{KH}\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\phi^\top\widehat{\Sigma}^{-1}\nabla_\theta w^\theta\\
    =&\sum_{h=1}^\infty\gamma^{h-1}\phi^\top\left(\widehat{M}_\theta\right)^{h-1}\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h^\prime=1}^H\phi\left(s_{h^\prime}^{(k)},a_{h^\prime}^{(k)}\right)\nabla_\theta\varepsilon_{h^\prime,k}^\theta+\frac{\lambda}{KH}\sum_{h=1}^\infty\gamma^{h-1}\phi^\top\left(\widehat{M}_\theta\right)^{h-1}\widehat{\Sigma}^{-1}\nabla_\theta w^\theta.
\end{align*}
Using the definition of $\widehat{\nu}^\theta$, we get
\begin{align}
    \label{p1_homo}
    \begin{aligned}
        &\int_{\mathcal{S}\times\mathcal{A}}\xi(s)\pi_\theta(a\vert s)\left(\sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(\nabla_\theta Q^\theta-\widehat{U}^\theta-\gamma\widehat{\mathcal{P}}_\theta\nabla_\theta Q^\theta\right)\right)(s,a)\mathrm{d}s\mathrm{d}a\\
        =&\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta\varepsilon_{h,k}^\theta+\frac{\lambda}{KH}\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}\nabla_\theta w^\theta.
    \end{aligned}
\end{align}
For the second term,  by Lemma \ref{Q_decomp_homo}, we have
\begin{align*}
    \sum_{h=1}^\infty\gamma^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(\widehat{U}^\theta-\tilde{U}^\theta\right)&=\sum_{h=1}^\infty\gamma^h\left(\widehat{\mathcal{P}}_\theta\right)^h\left(\nabla_\theta\log\Pi_\theta\right)\left(Q^\theta-\widehat{Q}^\theta\right)\\
    &=\sum_{h=1}^\infty\gamma^h\left(\widehat{\mathcal{P}}_\theta\right)^h\left(\nabla_\theta\log\Pi_\theta\right)\sum_{h^\prime=h}^\infty\gamma^{h^\prime-h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h^\prime-h-1}\left(Q^\theta-\widehat{r}-\gamma\widehat{\mathcal{P}}_\theta Q^\theta\right)\\
    &=\sum_{h=1}^\infty\sum_{h^\prime=1}^{h-1}\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h^\prime}\left(\nabla_\theta\log\Pi_\theta\right)\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h-h^\prime-1}\left(Q^\theta-\widehat{r}-\gamma\widehat{\mathcal{P}}_\theta Q^\theta\right).
\end{align*}
Meanwhile, again by Lemma \ref{Q_decomp_homo}, we have
\begin{align*}
    \left(\nabla_\theta\log\Pi_\theta\right)(Q^\theta-\widehat{Q}^\theta)=\left(\nabla_\theta\log\Pi_\theta\right)\sum_{h=1}^\infty\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(Q^\theta-\widehat{r}-\gamma\widehat{\mathcal{P}}_\theta Q^\theta\right),
\end{align*}
which implies
\begin{align*}
    &\sum_{h=1}^\infty\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(\widehat{U}^\theta-\tilde{U}^\theta\right)+\left(\nabla_\theta\log\Pi_\theta\right)(Q^\theta-\widehat{Q}^\theta)\\
    =&\sum_{h=1}^\infty\left(\left(\nabla_\theta\log\Pi_\theta\right)\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h-1}+\sum_{h^\prime=1}^{h-1}\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h^\prime}\left(\nabla_\theta\log\Pi_\theta\right)\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h-h^\prime-1}\right)\left(Q^\theta-\widehat{r}-\gamma\widehat{\mathcal{P}}_\theta Q^\theta\right)\\
    =&\sum_{h=1}^\infty\sum_{h^\prime=0}^{h-1}\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h^\prime}\left(\nabla_\theta\log\Pi_\theta\right)\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h-h^\prime-1}\left(Q^\theta-\widehat{r}-\gamma\widehat{\mathcal{P}}_\theta Q^\theta\right)\\
    =&\sum_{h=1}^\infty\sum_{h^\prime=0}^{h-1}\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h^\prime}\left(\nabla_\theta\log\Pi_\theta\right)\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h-h^\prime-1}\phi^\top\widehat{\Sigma}^{-1}\\
    &\cdot\frac{1}{KH}\sum_{k=1}^K\sum_{h^\prime=1}^H\phi\left(s_{h^\prime}^{(k)},a_{h^\prime}^{(k)}\right)\left(Q^\theta\left(s_{h^\prime}^{(k)},a_{h^\prime}^{(k)}\right)-r_{h^\prime}^{(k)}-\gamma\int_{\mathcal{A}}\pi_\theta\left(a^\prime\left\vert s_{h^\prime+1}^{(k)}\right.\right)Q^\theta\left(s_{h^\prime+1}^{(k)},a^\prime\right)\mathrm{d}a^\prime\right)\\
    &+\frac{\lambda}{KH}\sum_{h=1}^\infty\sum_{h^\prime=0}^{h-1}\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h^\prime}\left(\nabla_\theta\log\Pi_\theta\right)\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h-h^\prime-1}\phi^\top\widehat{\Sigma}^{-1}w^\theta.
\end{align*}
For each $j\in[m]$, notice the relation
\begin{align*}
    \left(\nabla_\theta^j\widehat{\nu}_h^\theta\right)^\top&=\left(\nabla_\theta^j\nu_1^\theta\right)^\top\left(\widehat{M}_\theta\right)^{h-1}+\sum_{h^\prime=1}^{h-1}\left(\nu_1^\theta\right)^\top\left(\widehat{M}_\theta\right)^{h^\prime-1}\left(\widehat{\nabla_\theta^j M_\theta}\right)\left(\widehat{M}_\theta\right)^{h-h^\prime-1}\\
    &=\int_{\mathcal{S}\times\mathcal{A}}\xi(s)\pi_\theta(a\vert s)\left(\sum_{h^\prime=0}^{h-1}\left(\widehat{\mathcal{P}}_\theta\right)^{h^\prime}\left(\nabla_\theta\log\Pi_\theta\right)\left(\widehat{\mathcal{P}}_\theta\right)^{h-h^\prime-1}\phi^\top\right)(s,a)\mathrm{d}s\mathrm{d}a.
\end{align*} 
Therefore, we have
\begin{align}
    \label{p2_homo}
    \begin{aligned}
        &\left[\int\xi(s)\pi_\theta(a\vert s)\left(\sum_{h=1}^\infty\left(\gamma\widehat{\mathcal{P}}_\theta\right)^{h-1}\left(\widehat{U}^\theta-\tilde{U}^\theta\right)+\left(\nabla_\theta\log\Pi_\theta\right)(Q^\theta-\widehat{Q}^\theta)\right)(s,a)\mathrm{d}s\mathrm{d}a\right]_j\\
        =&\sum_{h=1}^\infty\gamma^{h-1}\left(\nabla_\theta^j\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h^\prime=1}^H\phi\left(s_{h^\prime}^{(k)},a_{h^\prime}^{(k)}\right)\left(Q^\theta\left(s_{h^\prime}^{(k)},a_{h^\prime}^{(k)}\right)-r_{h^\prime}^{(k)}-\int_{\mathcal{A}}\pi_\theta\left(a^\prime\left\vert s_{h^\prime+1}^{(k)}\right.\right)Q^\theta\left(s_{h^\prime+1}^{(k)},a^\prime\right)\mathrm{d}a^\prime\right)\\
        &+\frac{\lambda}{KH}\sum_{h=1}^\infty\gamma^{h-1}\left(\nabla_\theta^j\widehat{\nu}_h^\theta\right)^\top\widehat{\Sigma}^{-1}w^\theta\\
        =&\left(\nabla_\theta^j\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta+\frac{\lambda}{KH}\left(\nabla_\theta^j\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}w^\theta.
    \end{aligned}
\end{align}
Combing the results of \eqref{p1_homo} and \eqref{p2_homo}, we get for each $j\in[m]$, 
\begin{align*}
    &\nabla_\theta^j v_\theta-\widehat{\nabla_\theta^j v_\theta}=\int_{\mathcal{S}\times\mathcal{A}}\xi(s)\pi_\theta(a\vert s)\left(\nabla_\theta^j Q^\theta-\widehat{\nabla_\theta^j Q^\theta}+\left(\nabla_\theta^j\log\Pi_\theta\right)(Q^\theta-\widehat{Q}^\theta)\right)(s,a)\textrm{d}s\textrm{d}a\\
    =&\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta+\frac{\lambda}{KH}\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}\nabla_\theta^j w^\theta+\left(\nabla_\theta^j\nu^\theta\right)^\top\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\\
    &+\frac{\lambda}{KH}\left(\nabla_\theta^j\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}w^\theta\\
    =&\nabla_\theta^j\left(\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta+\frac{\lambda}{KH}\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}w^\theta\right)\\
    =&\nabla_\theta^j\left(\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta+\frac{\lambda}{KH}\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}w^\theta+\left(\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}-\left(\nu^\theta\right)^\top\Sigma^{-1}\right)\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right).
\end{align*}
Rewriting the above decomposition in a vector form, we get
\begin{align*}
    \nabla_\theta v_\theta-\widehat{\nabla_\theta v_\theta}=&\nabla_\theta\Bigg(\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\\
    &+\frac{\lambda}{KH}\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}w^\theta+\left(\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}-\left(\nu^\theta\right)^\top\Sigma^{-1}\right)\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\Bigg),
\end{align*}
which is the desired result. 
\end{proof}

\subsubsection{Proof of Lemma \ref{e1_finite_product_homo}}
\begin{proof}
Note that, 
\begin{align*}
    \langle E_1,t\rangle=&\left\langle\nabla_\theta\left(\left(\nu^\theta\right)^\top\Sigma^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right),t\right\rangle\\
    =&\left\langle\left(\nabla_\theta\nu^\theta\right)^\top\Sigma^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta,t\right\rangle+\left\langle\left(\nu^\theta\right)^\top\Sigma^{-1}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta\varepsilon_{h,k}^\theta,t\right\rangle.
\end{align*}
Let $e_{h,k}=\left\langle\nabla_\theta\left(\left(\nu^\theta\right)^\top\Sigma^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right),t\right\rangle$, we have 
\begin{align*}
    \vert e_{h,k}\vert&\leq\sqrt{C_1dm}\Vert t\Vert\frac{1}{1-\gamma}\max_{j\in[m]}\sqrt{\left(\nabla^j_\theta\nu^\theta\right)^\top\Sigma^{-1}\nabla^j_\theta\nu^\theta}+2G\sqrt{C_1dm}\Vert t\Vert\frac{1}{(1-\gamma)^2}\sqrt{\left(\nu^\theta\right)^\top\Sigma^{-1}\nu^\theta}=B\sqrt{C_1dm}\Vert t\Vert.
\end{align*}
We have
\begin{align*}
    &\sum_{k=1}^K\sum_{h=1}^H\textrm{Var}[e_{h,k}\vert\mathcal{F}_{h,k}]=\sum_{k=1}^K\sum_{h=1}^H\mathbb{E}\left[\left.\left\langle\nabla_\theta\left(\left(\nu^\theta\right)^\top\Sigma^{-1}\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right),t\right\rangle^2\right\vert\mathcal{F}_{h,k}\right]=HKt^\top\Lambda_\theta t.
\end{align*}
We pick $\sigma^2=HKt^\top\Lambda_\theta t$, the Bernsteinâ€™s inequality implies that for any $\varepsilon\in\mathbb{R}$, 
\begin{align*}
    \mathbb{P}\left(\left\vert\sum_{k=1}^K\sum_{h=1}^He_{h,k}\right\vert\geq\varepsilon\right)\leq 2\exp\left(-\frac{\varepsilon^2/2}{\sigma^2+\sqrt{C_1dm}\Vert t\Vert B\varepsilon/3}\right).
\end{align*}
Therefore, if we pick $\varepsilon=\sigma\sqrt{2\log(2/\delta)}+2\log(2/\delta)\sqrt{C_1dm}\Vert t\Vert B/3$, we get
\begin{align*}
    \mathbb{P}\left(\left\vert\sum_{k=1}^K\sum_{h=1}^He_{h,k}\right\vert\geq\varepsilon\right)\leq\delta,
\end{align*}
i.e., we have with probability $1-\frac{\delta}{4}$, 
\begin{align*}
    \left\vert\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H e_{h,k}\right\vert\leq\sqrt{\frac{2t^\top\Lambda_\theta t\log(8/\delta)}{HK}}+\frac{2\log(8/\delta)\sqrt{C_1dm}\Vert t\Vert B}{3HK}
\end{align*}
\end{proof}

\subsubsection{Proof of Lemma \ref{e2_homo}}
\begin{proof}
For an arbitrarily given $\theta_0$, let $\Sigma_{\theta_0}=\mathbb{E}^{\pi_{\theta_0}}[\phi(s,a)\phi(s,a)^\top\vert s\sim\xi_{\theta_0},a\sim\pi_{\theta_0}(\cdot\vert s)]$, we have
\begin{align*}
    &\left(\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}-\left(\nu^\theta\right)^\top\Sigma^{-1}\right)\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\\
    =&\sum_{h^\prime=1}^\infty\gamma^{h^\prime-1}\left(\nu^\theta_1\right)^\top\left(\left(\widehat{M}_\theta\right)^{h^\prime-1}\widehat{\Sigma}^{-1}-\left(M_\theta\right)^{h^\prime-1}\Sigma^{-1}\right)\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\\
    =&\sum_{h^\prime=1}^\infty\gamma^{h^\prime-1}\left(\Sigma_{\theta_0}^{-\frac{1}{2}}\nu^\theta_1\right)^\top\left(\left(\Sigma_{\theta_0}^{\frac{1}{2}}\widehat{M}_\theta\Sigma_{\theta_0}^{-\frac{1}{2}}\right)^{h^\prime-1}\Sigma_{\theta_0}^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\Sigma^{\frac{1}{2}}\widehat{\Sigma}^{-1}\Sigma^{\frac{1}{2}}-\left(\Sigma_{\theta_0}^{\frac{1}{2}}M_\theta\Sigma_{\theta_0}^{-\frac{1}{2}}\right)^{h^\prime-1}\Sigma_{\theta_0}^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right)\\
    &\cdot\Sigma^{-\frac{1}{2}}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta.
\end{align*}
Taking derivatives on both sides, and let $\theta_0=\theta$, we get
\begin{align*}
    &\nabla_\theta^j E_2=\nabla_\theta^j\left(\left(\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}-\left(\nu^\theta\right)^\top\Sigma^{-1}\right)\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right)=E_{21}^j+E_{22}^j+E_{23}^j,
\end{align*}
where 
\begin{align*}
    E_{21}^j=&\sum_{h^\prime=1}^\infty\gamma^{h^\prime-1}\left(\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right)^\top\left(\left(\Sigma_\theta^{\frac{1}{2}}\widehat{M}_\theta\Sigma_\theta^{-\frac{1}{2}}\right)^{h^\prime-1}\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\Sigma^{\frac{1}{2}}\widehat{\Sigma}^{-1}\Sigma^{\frac{1}{2}}-\left(\Sigma_\theta^{\frac{1}{2}}M_\theta\Sigma_\theta^{-\frac{1}{2}}\right)^{h^\prime-1}\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right)\\
    &\cdot\Sigma^{-\frac{1}{2}}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta\\
    E_{22}^j=&\sum_{h^\prime=1}^\infty\gamma^{h^\prime-1}\left(\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right)^\top\left(\left(\Sigma_\theta^{\frac{1}{2}}\widehat{M}_\theta\Sigma_\theta^{-\frac{1}{2}}\right)^{h^\prime-1}\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\Sigma^{\frac{1}{2}}\widehat{\Sigma}^{-1}\Sigma^{\frac{1}{2}}-\left(\Sigma_\theta^{\frac{1}{2}}M_\theta\Sigma_\theta^{-\frac{1}{2}}\right)^{h^\prime-1}\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right)\\
    &\cdot\Sigma^{-\frac{1}{2}}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\\
    E_{23}^j=&\sum_{h^\prime=1}^\infty\gamma^{h^\prime-1}\left(\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right)^\top\\
    &\cdot\left(\left.\nabla_\theta^j\left(\Sigma_{\theta_0}^{\frac{1}{2}}\widehat{M}_{\theta}\Sigma_{\theta_0}^{-\frac{1}{2}}\right)^{h^\prime-1}\right\vert_{\theta_0=\theta}\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\Sigma^{\frac{1}{2}}\widehat{\Sigma}^{-1}\Sigma^{\frac{1}{2}}-\left.\nabla_\theta^j\left(\Sigma_{\theta_0}^{\frac{1}{2}}M_\theta\Sigma_{\theta_0}^{-\frac{1}{2}}\right)^{h^\prime-1}\right\vert_{\theta_0=\theta}\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right)\\
    &\cdot\Sigma^{-\frac{1}{2}}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta.
\end{align*}
Therefore, using the result of Lemma \ref{decomp_homo}, we get
\begin{align*}
    \vert E_{21}^j\vert\leq&\sum_{h^\prime=1}^\infty\gamma^{h^\prime-1}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\left(1+\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\right)^{h^\prime-1}\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\right)-1\right)\\
    &\cdot\left\Vert\Sigma^{-\frac{1}{2}}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta\right\Vert\\
    =&\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\left(1-\gamma-\gamma\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\right)^{-1}\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\right)-\left(1-\gamma\right)^{-1}\right)\\
    &\cdot\left\Vert\Sigma^{-\frac{1}{2}}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta\right\Vert\\
    \leq&\frac{1}{1-\gamma}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\left(1-\frac{\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert}{1-\gamma}\right)^{-1}\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\right)-1\right)\\
    &\cdot\left\Vert\Sigma^{-\frac{1}{2}}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta\right\Vert\\
    \vert E_{22}^j\vert\leq&\sum_{h^\prime=1}^\infty\gamma^{h^\prime-1}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\left(1+\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\right)^{h^\prime-1}\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\right)-1\right)\\
    &\cdot\left\Vert\Sigma^{-\frac{1}{2}}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right\Vert\\
    \leq&\frac{1}{1-\gamma}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\left(1-\frac{\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert}{1-\gamma}\right)^{-1}\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\right)-1\right)\\
    &\cdot\left\Vert\Sigma^{-\frac{1}{2}}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right\Vert\\
    \vert E_{23}^j\vert\leq&\sum_{h^\prime=2}^\infty(h^\prime-1)\gamma^{h^\prime-1}G\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\\
    &\cdot\left(\left(1+\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\right)^{h^\prime-2}\left(1+\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta M_\theta\right)}{G}\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\right)\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\right)-1\right)\\
    &\cdot\left\Vert\Sigma^{-\frac{1}{2}}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right\Vert\\
    \leq&\frac{G}{(1-\gamma)^2}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\\
    &\cdot\left(\left(1-\frac{\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert}{1-\gamma}\right)^{-2}\left(1+\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta M_\theta\right)}{G}\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\right)\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\right)-1\right)\\
    &\cdot\left\Vert\Sigma^{-\frac{1}{2}}\frac{1}{HK}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right\Vert,
\end{align*}
where $\Delta\Sigma^{-1}=\widehat{\Sigma}^{-1}-\Sigma$ and we use the fact $\left\Vert\Sigma_\theta^{\frac{1}{2}}M_\theta\Sigma_\theta^{-\frac{1}{2}}\right\Vert\leq 1$ and $\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\nabla_\theta^j M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\leq G$ from Lemma \ref{ineq_homo}. Now, define $\alpha=6\sqrt{\kappa_1}(4+\kappa_2+\kappa_3)\sqrt{\frac{C_1d\log\frac{16dmH}{\delta}}{K}}$ and pick
\begin{align*}
    K\geq 36\kappa_1(4+\kappa_2+\kappa_3)^2\frac{C_1d}{(1-\gamma)^2}\log\frac{16dmH}{\delta},\quad\lambda\leq C_1d\sigma_{\textrm{min}}(\Sigma)\cdot\log\frac{8dmH}{\delta},
\end{align*}
we get $\alpha\leq \frac{1-\gamma}{2}$. Using the results of Lemma \ref{dsig2_homo} and Lemma \ref{dm2_homo}, we get
\begin{align}
    \label{sig_homo}
    \left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\leq 4\sqrt{\frac{C_1d\log\frac{8dmH}{\delta}}{K}}\leq\alpha\leq 1, 
\end{align}
and 
\begin{align}
    \label{dm_homo}
    \left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert&\leq\alpha,\\
    \label{ddm_homo}
    \left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta M_\theta\right)}{G}\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert&\leq\alpha,\quad\forall j\in[m].
\end{align}
Meanwhile, the event $\mathcal{E}_\varepsilon$ implies
\begin{align}
    \label{ep1_homo}
    \left\Vert\Sigma^{-\frac{1}{2}}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\varepsilon_{h,k}^\theta\right\Vert&\leq \frac{4\sqrt{d}}{1-\gamma}\sqrt{\frac{\log\frac{32dmH}{\delta}}{KH}}\\
    \label{ep2_homo}
    \left\Vert\Sigma^{-\frac{1}{2}}\frac{1}{KH}\sum_{k=1}^K\sum_{h=1}^H\phi\left(s_h^{(k)},a_h^{(k)}\right)\nabla_\theta^j\varepsilon_{h,k}^\theta\right\Vert&\leq \frac{8\sqrt{d}G}{(1-\gamma)^2}\sqrt{\frac{\log\frac{32dmH}{\delta}}{KH}}.
\end{align}
Combining the results of \eqref{sig_homo}, \eqref{dm_homo}, \eqref{ddm_homo}, \eqref{ep1_homo}, \eqref{ep2_homo} and use a union bound, we have with probability $1-3\delta$, 
\begin{align*}
    \vert E_{21}^j\vert\leq&\frac{16\alpha\sqrt{d}G}{(1-\gamma)^4}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert \sqrt{\frac{\log\frac{32dmH}{\delta}}{KH}}\\
    \vert E_{22}^j\vert\leq&\frac{8\alpha\sqrt{d}}{(1-\gamma)^3}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\sqrt{\frac{\log\frac{32dmH}{\delta}}{KH}}\\
    \vert E_{23}^j\vert\leq&\frac{16G\alpha\sqrt{d}}{(1-\gamma)^4}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\sqrt{\frac{\log\frac{32dmH}{\delta}}{KH}}
\end{align*}
where we use the fact $(1-\frac{\alpha}{1-\gamma})^{-1}(1+\alpha)\leq 1+\frac{3\alpha}{1-\gamma}$ whenever $\alpha \leq \frac{1}{2(1-\gamma)}$. Summing up the above terms and using the definition of $\alpha$, we get
\begin{align*}
    \vert E_2^j\vert\leq \frac{240\sqrt{\kappa_1}(2+\kappa_2+\kappa_3)\sqrt{C_1}d}{(1-\gamma)^3}\left(\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert+\frac{G}{1-\gamma}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\right)\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\frac{\log\frac{32dmH}{\delta}}{KH},\quad\forall j\in[m],
\end{align*}
which finished the proof. 
\end{proof}

\subsubsection{Proof of Lemma \ref{e3_homo}}
\begin{proof}
Similar to the decomposition in the proof of Lemma \ref{e2_homo}, we have
\begin{align*}
    \left\vert E_3^j\right\vert=&\frac{\lambda}{KH}\left\vert\nabla_\theta^j\left(\left(\widehat{\nu}^\theta\right)^\top\widehat{\Sigma}^{-1}w^\theta\right)\right\vert\\
    \leq&\frac{\lambda}{KH}\sum_{h=1}^\infty\gamma^{h-1}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(1+\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\right)^{h-1}\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\right)\left\Vert\Sigma^{-1}\right\Vert\left\Vert\Sigma^{\frac{1}{2}}\nabla_\theta^j w^\theta\right\Vert\\
    +&\frac{\lambda}{KH}\sum_{h=1}^\infty\gamma^{h-1}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(1+\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\right)^{h-1}\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\right)\left\Vert\Sigma^{-1}\right\Vert\left\Vert\Sigma^{\frac{1}{2}}w^\theta\right\Vert\\
    +&\frac{\lambda}{KH}\sum_{h=1}^\infty\gamma^{h-1}(h-1)G\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\\
    &\cdot\left(1+\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\Delta M_\theta\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\right)^{h-2}\left(1+\left\Vert\Sigma_\theta^{\frac{1}{2}}\left(\frac{\nabla_\theta^j\left(\Delta M_\theta\right)}{G}\right)\Sigma_\theta^{-\frac{1}{2}}\right\Vert\right)\left(1+\left\Vert\Sigma^{\frac{1}{2}}\left(\Delta\Sigma^{-1}\right)\Sigma^{\frac{1}{2}}\right\Vert\right)\left\Vert\Sigma^{-1}\right\Vert\left\Vert\Sigma^{\frac{1}{2}}w^\theta\right\Vert\\
    \leq&\frac{\lambda}{KH}\sum_{h=1}^\infty\gamma^{h-1}\left(1+\alpha\right)^h\left\Vert\Sigma^{-1}\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma^{\frac{1}{2}}\nabla_\theta^j w_h^\theta\right\Vert+\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\left\Vert\Sigma^{\frac{1}{2}}w^\theta\right\Vert+G(h-1)\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma^{\frac{1}{2}}w^\theta\right\Vert\right)\\
    \leq&\frac{2\lambda}{KH(1-\gamma)}\left\Vert\Sigma^{-1}\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma^{\frac{1}{2}}\nabla_\theta^j w^\theta\right\Vert+\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\left\Vert\Sigma^{\frac{1}{2}}w^\theta\right\Vert+\frac{2G}{1-\gamma}\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\left\Vert\Sigma^{\frac{1}{2}}w^\theta\right\Vert\right),
\end{align*}
where $\alpha$ is defined in the same way as that in the proof of Lemma \ref{e2_homo}. Similarly, we have $\alpha\leq\frac{1-\gamma}{2}$ and we have
\begin{align*}
    \left\Vert\Sigma^{\frac{1}{2}}\nabla_\theta^j w^\theta\right\Vert^2&=\mathbb{E}\left[\frac{1}{H}\sum_{h=1}^H\left(\nabla_\theta^j Q^\theta\left(s_h^{(1)},a_h^{(1)}\right)\right)^2\right]\leq \frac{G^2}{(1-\gamma)^4}\\
    \left\Vert\Sigma^{\frac{1}{2}}w^\theta\right\Vert^2&=\mathbb{E}\left[\frac{1}{H}\sum_{h=1}^H\left(Q^\theta\left(s_h^{(1)},a_h^{(1)}\right)\right)^2\right]\leq\frac{1}{(1-\gamma)^2}.
\end{align*}
We conclude
\begin{align*}
    \vert E_3^j\vert\leq&\frac{2\lambda}{KH(1-\gamma)^2}\left\Vert\Sigma^{-1}\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\frac{3G}{1-\gamma}+\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\right)\\
    \leq&\frac{6C_1d\sigma_{\textrm{min}}(\Sigma)\cdot\log\frac{8dmH}{\delta}
}{KH(1-\gamma)^2}\left\Vert\Sigma^{-1}\right\Vert\left\Vert\Sigma_\theta^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\right\Vert\left(\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nu^\theta_1\right\Vert\frac{G}{1-\gamma}+\left\Vert\Sigma_\theta^{-\frac{1}{2}}\nabla_\theta^j\nu^\theta_1\right\Vert\right).
\end{align*}
which has finished the proof. 
\end{proof}

\section{Supplement for Experiments}
\label{exp_appendix}

\subsection{Off-line policy optimization with FPG}
To test FPG's applicability as a policy gradient estimator, we conduct an offline policy optimization experiment, in addition to the online experiment in Section \ref{sec:exp-optimization}. We create a dataset of $K=500$, generated with a behavior policy chosen as the $0.3$ $\epsilon$-greedy policy of the target policy. At each iteration, we only use the offline dataset and do not sample for fresh data. Thus, we replace the online policy gradient estimator in REINFORCE with an off-policy one using FPG and importance sampling. \textbf{Figure \ref{fig:FrozenLake_off}} shows that FPG-REINFORCE converges reasonably fast and approaches the optimal value. However, IS-REINFORCE appears to converge to a highly biased solution, due to that all PGs are estimated using the same small batch dataset and suffer from dependent estimation errors.

\begin{figure}[!t]
 \centering
 \includegraphics[width=0.3\linewidth]{figure/fig2.3.pdf}
\caption{\textbf{FPG for offline policy optimization.} FPG is compared with IS as the PG estimator module in offline REINFORCE. }
\label{fig:FrozenLake_off}
\end{figure}

\subsection{Experiments with deep policy networks}
Further, we evaluate the efficiency of FPG  using the OpenAI gym CliffWalking environment with neural network policy parameterization and $H=100$. Specifically, the environment is modified by adding artificial randomness for stochastic transitions, that is, at each transition, a random action is taken with probability $0.1$, and the target policy is softmax and parameterized with a neural network with one ReLU hidden layer. 

\def\grad{\nabla}
\def\hat{\widehat}
\begin{figure}[!t]
 \centering
 \includegraphics[width=0.3\linewidth]{figure/fig2.2.pdf}
  \includegraphics[width=0.3\linewidth]{figure/fig2.2_norm.pdf}
\caption{\textbf{Sample efficiency of FPG with deep policy networks on off-policy data.} The off-policy PG estimation accuracy is evaluated using two metrics: $\cos\angle (\hat{\grad v_{\theta}}, \grad v_{\theta})$ and the relative error norm $\frac{\| \hat{\grad v_{\theta}} -  \grad v_{\theta}\| }{\|\grad v_{\theta}\|}$. }
\label{fig:CliffWalking_1}
\end{figure}

As before, we test the performance of FPG against the size of off-policy data and the degree of distribution shift, using the same cosine and relative norm error metrics. We test FPG varying the size of the dataset. \textbf{Figure \ref{fig:CliffWalking_1}} shows that FPG still gives accurate estimates with moderate variance, in contrast to IS's inaccuracy and high variance. Both methods become more accurate asymptotically as the dataset size increases.

\begin{figure}[!t]
 \centering
  \includegraphics[width=0.3\linewidth]{figure/fig2.1.pdf}
  \includegraphics[width=0.3\linewidth]{figure/fig2.1_norm.pdf}
\caption{\textbf{Tolerance to off-policy distribution shift with deep policy networks.} The distributional mismatch is measured by  $\hbox{cond}(\Sigma^{1/2}\overline{\Sigma}^{-1}\Sigma^{1/2})$, where $\overline{\Sigma}$ is the data covariance and $\Sigma$ is the target policy's occupancy measure.}
\label{fig:CliffWalking_2}
\end{figure}

We also test FPG on datasets with different amount of mismatch from the target policy. \textbf{Figure \ref{fig:CliffWalking_2}} shows that FPG's estimation error is much lower and less affected by enlarging distributional mismatch than IS's. Please note that the distribution mismatch is large because there are state-action pairs that are almost never visited by the target policy and seldom visited by the behavior policy. Irrelevant as they are to the estimation, such state-action pairs cause $\Sigma^{1/2}\overline{\Sigma}^{-1}\Sigma^{1/2}$ to be nearly singular. In conclusion, the general trends in these two experiments are the same as those in Section \ref{sec:exp-K} and \ref{sec:exp-mismatch}.

\section{More Related Work}

\paragraph{PG-type Methods} In order to achieve the optimal policy, policy gradient methods update parameters along the direction of estimated gradient. Vanilla Policy Gradient(PG) method REINFORCE and its variant GPOMDP using Natural Policy Gradient(NPG) methods were proposed and developed by \cite{williams1992simple, sutton2000policy, kakade2001natural}. Early policy gradient methods suffered from its huge variance, hence a lot of variance-reduced modifications of REINFORCE and GPOMDP were proposed. 
\cite{papini2018stochastic} introduced stochastic gradient technique into gradient estimation and proposed Stochastic Variance-Reduced Policy Gradient(SVRPG) algorithm to effectively reduce variance and achieved $O(1/\varepsilon^2)$ sample complexity to reach an $\varepsilon$ stationary policy, i.e, $\left\|\mathbb{E} [\nabla_{\theta} v_{\theta}]\right\|_2^2 \leq \varepsilon.$ This complexity upper bound was soon improved to $O(1/\varepsilon^{\frac{5}{3}})$ by \cite{xu2020improved}, and further to $O(1/\varepsilon^{\frac{3}{2}})$ by \cite{xu2019sample}. Another variant, Hessian Aid Policy Gradient(HAPG) in \cite{shen2019hessian}, replaces calculating gradient correction in stochastic gradient estimation with constructing an unbiased estimate of policy Hessian and also needs $O(1/\varepsilon^{\frac{3}{2}})$ sampled trajectories to reach $\varepsilon$ stationary policy. For a long time, analysis of convergent behavior has been limited within local optimality. Recently, \cite{agarwal2021theory} prove the convergence to the global optimal solution for the first time. They prove that in tabular case, several PG methods converge with $O(1/\varepsilon^2)$ sample complexity, while NPG method with softmax paramiterization converges with $O(1/\varepsilon)$ samples to global optimum. 

\paragraph{On Policy Actor-Critic Methods} Actor-Critic(AC) method \cite{konda2000actor} \cite{peters2008natural}, as an extension of Policy Gradient method via combining with value-based method, has long gained great success empirically in variant tasks. It composes two stages: the actor updates the parameters, and the critic learns policy value function or other functions with respect to current policy, which serves as input for actor to further updates. AC method was first applied in on policy learning, with multiple variants showing huge success such as DPG \cite{silver2014deterministic} and ACER \cite{wang2016sample}. \cite{yang2019global} exploits the simple structure of Linear Quadratic Regulator(LQR), and restricts the policy to be linear-Gaussian and the value function to be quadratic. To our knowledge, this is the first rigorous analysis of actor critic methods and it shows that the on policy LQR actor-critic algorithm can get an $\varepsilon-$optimal policy with $O(1/\varepsilon^5)$ state-action pairs sampled. Since then, many papers have carried out further analysis on non-asymptotic behavior of on policy actor critic methods. \cite{wang2019neural} and \cite{khodadadian2021finite} analyze two variants of actor-critic and prove their sample complexity to get an $\varepsilon-$optimal policy to be $O(1/\varepsilon^{14})$ and $O(1/\varepsilon^4)$ respectively. The latest developments in this regard come from \cite{xu2020improving}. It considers linear value function class and general policy class. The critic uses mini-batch Temporal Difference to update, while actor updates parameters using Markovian mini-batch sampling. It shows that AC and NAC methods in this setting have sample complexity of order $O(1/\varepsilon^2 \log(1/\varepsilon))$ and $O(1/\varepsilon^3 \log(1/\varepsilon))$ respectively.

\paragraph{Off Policy Actor-Critic Methods} When it comes to offline settings, \cite{degris2012off} presented a off-policy policy gradient theorem and proposed the first off-policy actor-critic algorithm, with critic updated via Gradient Temporal Difference(GTD) method\cite{sutton2009fast} and actor updated incrementally with eligibility trace. Though converging in the tabular case, it can result in asymptotically non-vanishing bias in more subtle settings. Theoretically, \cite{xu2021doubly} invented a doubly robust actor-critic algorithm and established the first sample complexity analysis for off-policy AC algorithm. Its sample complexity to achieve $\varepsilon$-optimal policy is $O(1/\varepsilon^4).$ Another off-policy Natural AC algorithm proposed in \cite{khodadadian2021finite} improved this bound to $O(1/\varepsilon^3 \log(1/\varepsilon^2)),$ which beats a lot of on-policy AC algorithm. Recently, \cite{zanette2021provable}  design a pessimistic actor-critic algorithm that iteratively optimizes the lower bound of the optimal policy value using linear function approximation. The critic pessimistically provides an estimate of policy value by a suitably perturbed the result of least square Temporal Difference method, while the actor updates parameters by mirror descent algorithm. It is theoretically proven that regardless of the mis-specification error, the remaining part of error is less than $\varepsilon$ with a $O(1/\varepsilon^2)$ sample size and $O(1/\varepsilon^2)$ iterations. For other finite sample analysis of PG method with actor-critic scheme, we refer to readers \cite{kumar2019sample2,xiong2020non,qiu2021finite,khodadadian2021finite,maei2018convergent,zhang2020provably}. For a more comprehensive summation of sample complexity of (N)PG-AC algorithm and their variants, we refer to readers \cite{agarwal2021theory,xu2020improving}.

\end{document}
